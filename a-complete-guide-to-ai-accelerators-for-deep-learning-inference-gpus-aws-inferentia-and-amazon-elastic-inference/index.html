<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>A complete guide to AI accelerators for deep learning inference — GPUs, AWS Inferentia and Amazon Elastic Inference | Shashank Prasanna</title>
<meta name=description content="Learn about CPUs, GPUs, AWS Inferentia, and Amazon Elastic Inference and how to choose the right AI accelerator for inference deployment"><meta property="og:title" content="A complete guide to AI accelerators for deep learning inference — GPUs, AWS Inferentia and Amazon Elastic Inference"><meta property="og:description" content="Learn about CPUs, GPUs, AWS Inferentia, and Amazon Elastic Inference and how to choose the right AI accelerator for inference deployment"><meta property="og:type" content="article"><meta property="og:url" content="/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/"><meta property="og:image" content="/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/featured.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2020-10-21T00:00:00+00:00"><meta property="article:modified_time" content="2020-10-21T00:00:00+00:00"><meta itemprop=name content="A complete guide to AI accelerators for deep learning inference — GPUs, AWS Inferentia and Amazon Elastic Inference"><meta itemprop=description content="Learn about CPUs, GPUs, AWS Inferentia, and Amazon Elastic Inference and how to choose the right AI accelerator for inference deployment"><meta itemprop=datePublished content="2020-10-21T00:00:00+00:00"><meta itemprop=dateModified content="2020-10-21T00:00:00+00:00"><meta itemprop=wordCount content="305"><meta itemprop=image content="/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/featured.png"><meta itemprop=keywords content="ai accelerators,gpu,"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/featured.png"><meta name=twitter:title content="A complete guide to AI accelerators for deep learning inference — GPUs, AWS Inferentia and Amazon Elastic Inference"><meta name=twitter:description content="Learn about CPUs, GPUs, AWS Inferentia, and Amazon Elastic Inference and how to choose the right AI accelerator for inference deployment"><link rel=preload href=/scss/main.min.513f3f983974308c9847d2d1bc9bf12e1307bb0ccac7a4c4b4da85bda11c1885.css as=style><link href=/scss/main.min.513f3f983974308c9847d2d1bc9bf12e1307bb0ccac7a4c4b4da85bda11c1885.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.3.min.js integrity="sha512-STof4xm1wgkfm7heWqFJVn58Hm3EtS31XFaagaa8VMReCXAkQnJZ+jEy8PCC/iT18dFy95WcExNHFTqLyp72eQ==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-20Q10LSLZ7"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-20Q10LSLZ7")}</script></head><body class="td-page td-blog"><header><nav class="td-navbar navbar-dark js-navbar-scroll"><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"></span><span class=navbar-brand__name>Shashank Prasanna</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class="nav-link active" href=/><span>Home</span></a></li><li class=nav-item><a class="nav-link active" href=/blog/><span>Personal Blog</span></a></li><li class=nav-item><a class=nav-link href=https://medium.com/@shashankprasanna target=_blank rel=noopener><i class='fa-brands fa-medium'></i><span>Medium Blog </span><sup><i class='ps-1 fa-solid fa-up-right-from-square fa-xs' aria-hidden=true></i></sup></a></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.b49b446b7f99aba42f4fa141f8e989e0.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><a href=/><img style=vertical-align:middle src=/featured-background.png alt="shashank prasanna" width=200></a><div class="td-page-meta ml-2 pb-1 pt-2 mb-0"><div class=column><div class=column><a href=https://twitter.com/shshnkp target=_blank><font color=gray size=3><i class="fa fab fa-twitter"></i>&nbsp;&nbsp;@shshnkp</font></a></div><div class=column><a href=https://www.youtube.com/@shashank.prasanna target=_blank><font color=gray size=3><i class="fa fab fa-youtube"></i>&nbsp;&nbsp;@shashank.prasanna</font></a></div><div class=column><a href=https://medium.com/@shashankprasanna target=_blank><font color=gray size=3><i class="fa fab fa-medium"></i>&nbsp;@shashankprasanna</font></a></div><div class=column><a href=https://www.linkedin.com/in/shashankprasanna/ target=_blank><font color=gray size=3><i class="fa fab fa-linkedin"></i>&nbsp;&nbsp;shashankprasanna</font></a></div><div class=column><a href=https://github.com/shashankprasanna/ target=_blank><font color=gray size=3><i class="fa fab fa-github"></i>&nbsp;&nbsp;shashankprasanna</font></a></div><div class=column><a href=/shashank_prasanna_resume_2024.pdf target=_blank><font color=gray size=3><i class="fa-solid fa-file"></i>&nbsp;&nbsp;&nbsp;resume</font></a></div></div><a href=https://github.com/shashankprasanna/shashankprasanna.com/tree/main/content/en/blog/A%20complete%20guide%20to%20AI%20accelerators%20for%20deep%20learning%20inference%20%e2%80%94%20GPUs,%20AWS%20Inferentia%20and%20Amazon%20Elastic%20Inference/index.md class=td-page-meta--view target=_blank rel=noopener><i class="fa fa-file-alt fa-fw"></i> View page source</a>
<a href=https://github.com/shashankprasanna/shashankprasanna.com/edit/main/content/en/blog/A%20complete%20guide%20to%20AI%20accelerators%20for%20deep%20learning%20inference%20%e2%80%94%20GPUs,%20AWS%20Inferentia%20and%20Amazon%20Elastic%20Inference/index.md class=td-page-meta--edit target=_blank rel=noopener><i class="fa fa-edit fa-fw"></i> Edit this page</a>
<a href="https://github.com/shashankprasanna/shashankprasanna.com/new/main/content/en/blog/A%20complete%20guide%20to%20AI%20accelerators%20for%20deep%20learning%20inference%20%e2%80%94%20GPUs,%20AWS%20Inferentia%20and%20Amazon%20Elastic%20Inference/index.md?filename=change-me.md&amp;value=---%0Atitle%3A+%22Long+Page+Title%22%0AlinkTitle%3A+%22Short+Nav+Title%22%0Aweight%3A+100%0Adescription%3A+%3E-%0A+++++Page+description+for+heading+and+indexes.%0A---%0A%0A%23%23+Heading%0A%0AEdit+this+template+to+create+your+new+page.%0A%0A%2A+Give+it+a+good+name%2C+ending+in+%60.md%60+-+e.g.+%60getting-started.md%60%0A%2A+Edit+the+%22front+matter%22+section+at+the+top+of+the+page+%28weight+controls+how+its+ordered+amongst+other+pages+in+the+same+directory%3B+lowest+number+first%29.%0A%2A+Add+a+good+commit+message+at+the+bottom+of+the+page+%28%3C80+characters%3B+use+the+extended+description+field+for+more+detail%29.%0A%2A+Create+a+new+branch+so+you+can+preview+your+new+file+and+request+a+review+via+Pull+Request.%0A" class=td-page-meta--child target=_blank rel=noopener><i class="fa fa-edit fa-fw"></i> Create child page</a>
<a href="https://github.com/shashankprasanna/shashankprasanna.com/issues/new?title=A%20complete%20guide%20to%20AI%20accelerators%20for%20deep%20learning%20inference%20%e2%80%94%20GPUs,%20AWS%20Inferentia%20and%20Amazon%20Elastic%20Inference" class=td-page-meta--issue target=_blank rel=noopener><i class="fas fa-tasks fa-fw"></i> Create documentation issue</a>
<a href=https://github.com/shashankprasanna/shashankprasanna.com/issues/new class=td-page-meta--project-issue target=_blank rel=noopener><i class="fas fa-tasks fa-fw"></i> Create project issue</a></div><div class="taxonomy taxonomy-terms-cloud taxo-tags"><h5 class=taxonomy-title>Topics</h5><ul class=taxonomy-terms><li><a class=taxonomy-term href=/tags/ai-accelerators/ data-taxonomy-term=ai-accelerators><span class=taxonomy-label>ai accelerators</span><span class=taxonomy-count>3</span></a></li><li><a class=taxonomy-term href=/tags/aws/ data-taxonomy-term=aws><span class=taxonomy-label>aws</span><span class=taxonomy-count>3</span></a></li><li><a class=taxonomy-term href=/tags/career/ data-taxonomy-term=career><span class=taxonomy-label>career</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/docker/ data-taxonomy-term=docker><span class=taxonomy-label>docker</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/genai/ data-taxonomy-term=genai><span class=taxonomy-label>GenAI</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/gpu/ data-taxonomy-term=gpu><span class=taxonomy-label>gpu</span><span class=taxonomy-count>7</span></a></li><li><a class=taxonomy-term href=/tags/mandelbrot/ data-taxonomy-term=mandelbrot><span class=taxonomy-label>mandelbrot</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/modular/ data-taxonomy-term=modular><span class=taxonomy-label>modular</span><span class=taxonomy-count>2</span></a></li><li><a class=taxonomy-term href=/tags/mojo/ data-taxonomy-term=mojo><span class=taxonomy-label>mojo</span><span class=taxonomy-count>2</span></a></li><li><a class=taxonomy-term href=/tags/openai/ data-taxonomy-term=openai><span class=taxonomy-label>openai</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/pytorch/ data-taxonomy-term=pytorch><span class=taxonomy-label>pytorch</span><span class=taxonomy-count>2</span></a></li></ul></div></aside><aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none"><div class=td-toc><font color=gray size=6>Sections</font><nav id=TableOfContents><ul><li><ul><li><a href=#lets-start-by-answering-the-question-what-is-an-ai-accelerator>Let’s start by answering the question “What is an AI accelerator?”</a></li><li><a href=#do-i-need-an-ai-accelerator-for-machine-learning-ml-inference>Do I need an AI accelerator for machine learning (ML) inference?</a></li></ul></li></ul></nav></div></aside><main class="col-12 col-md-9 col-xl-8 ps-md-5 pe-md-4" role=main><font color=black size=5></font>
<a class=td-rss-button title=RSS href=/blog/index.xml target=_blank rel=noopener><i class="fa-solid fa-rss" aria-hidden=true></i></a><div class=td-content><h1>A complete guide to AI accelerators for deep learning inference — GPUs, AWS Inferentia and Amazon Elastic Inference</h1><div class=lead>Learn about CPUs, GPUs, AWS Inferentia, and Amazon Elastic Inference and how to choose the right AI accelerator for inference deployment</div><div class="td-byline mb-4">By <b>Shashank Prasanna</b> |
<time datetime=2020-10-21 class=text-muted>Wednesday, October 21, 2020</time></div><header class=article-meta><div class="taxonomy taxonomy-terms-article taxo-tags"><h5 class=taxonomy-title>Tags:</h5><ul class=taxonomy-terms><li><a class=taxonomy-term href=/tags/ai-accelerators/ data-taxonomy-term=ai-accelerators><span class=taxonomy-label>ai accelerators</span></a></li><li><a class=taxonomy-term href=/tags/gpu/ data-taxonomy-term=gpu><span class=taxonomy-label>gpu</span></a></li></ul></div></header><h3 id=lets-start-by-answering-the-question-what-is-an-ai-accelerator>Let’s start by answering the question “What is an AI accelerator?”</h3><p>An AI accelerator is a dedicated processor designed to accelerate machine learning computations. Machine learning, and particularly its subset, deep learning is primarily composed of a large number of linear algebra computations, (i.e. matrix-matrix, matrix-vector operations) and these operations can be easily parallelized. AI accelerators are specialized hardware designed to accelerate these basic machine learning computations and improve performance, reduce latency and reduce cost of deploying machine learning based applications.</p><h3 id=do-i-need-an-ai-accelerator-for-machine-learning-ml-inference>Do I need an AI accelerator for machine learning (ML) inference?</h3><p>Let’s say you have an ML model as part of your software application. The prediction step (or inference) is often the most time consuming part of your application that directly affects user experience. A model that takes several hundreds of milliseconds to generate text translations or apply filters to images or generate product recommendations, can drive users away from your “sluggish”, “slow”, “frustrating to use” app.
By speeding up inference, you can reduce the overall application latency and deliver an app experience that can be described as “smooth”, “snappy”, and “delightful to use”. And you can speed up inference by offloading ML model prediction computation to an AI accelerator.
With great market needs comes great many product alternatives, so naturally there is more than one way to accelerate your ML models in the cloud.
In this blog post, I’ll explore three popular options:</p><ol><li>GPUs: Particularly, the high-performance NVIDIA T4 and NVIDIA V100 GPUs</li><li>AWS Inferentia: A custom designed machine learning inference chip by AWS</li><li>Amazon Elastic Inference (EI): An accelerator that saves cost by giving you access to variable-size GPU acceleration, for models that don’t need a dedicated GPU</li></ol><div class="td-card card border me-4"><div class=card-body><h5 class=card-title>Read the full blog post here:<br><br><i class='fa-brands fa-medium'></i> <a href=https://medium.com/towards-data-science/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c>A complete guide to AI accelerators for deep learning inference — GPUs, AWS Inferentia and Amazon Elastic Inference</a></h5><p class=card-text></p></div><div class=card-footer><img src=featured.png alt></div></div><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><li><a href=/choosing-the-right-gpu-for-deep-learning-on-aws/ aria-label="Previous - Choosing the right GPU for deep learning on AWS" class="btn btn-primary"><span class=me-1>←</span>Previous</a></li><li><a href=/best-gpus-on-aws-for-deep-learning/ aria-label="Next - Best GPUs on AWS for Deep Learning" class="btn btn-primary">Next<span class=ms-1>→</span></a></li></ul></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="col-6 col-sm-4 text-xs-center order-sm-2"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Twitter aria-label=Twitter><a target=_blank rel=noopener href=https://twitter.com/shshnkp aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=LinkedIn aria-label=LinkedIn><a target=_blank rel=noopener href=https://www.linkedin.com/in/shashankprasanna/ aria-label=LinkedIn><i class="fab fa-linkedin"></i></a></li></ul></div><div class="col-6 col-sm-4 text-end text-xs-center order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/shashankprasanna/ aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=YouTube aria-label=YouTube><a target=_blank rel=noopener href=https://www.youtube.com/@shashank.prasanna aria-label=YouTube><i class="fab fa-youtube"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Medium aria-label=Medium><a target=_blank rel=noopener href=https://medium.com/@shashankprasanna aria-label=Medium><i class="fab fa-medium"></i></a></li></ul></div><div class="td-footer__copyright-etc col-12 col-sm-4 text-center py-2 order-sm-2"><span>&copy; 2024 Shashank Prasanna All Rights Reserved</span></div></div></div></footer></div><script src=/js/main.min.71ce084bcf13abbf1fcd942047313f25a70594ce26ec4a6a93ab9a359a8cbf66.js integrity="sha256-cc4IS88Tq78fzZQgRzE/JacFlM4m7Epqk6uaNZqMv2Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.f724d3de49218995223b7316aa2e53e2b34bf42026bf399ebb21bb02212402d1.js integrity="sha256-9yTT3kkhiZUiO3MWqi5T4rNL9CAmvzmeuyG7AiEkAtE=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>