<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Shashank Prasanna – gpu</title><link>/tags/gpu/</link><description>Recent content in gpu on Shashank Prasanna</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 11 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="/tags/gpu/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: What is an AI accelerator?</title><link>/what-is-an-ai-accelerator/</link><pubDate>Wed, 11 Jan 2023 00:00:00 +0000</pubDate><guid>/what-is-an-ai-accelerator/</guid><description>
&lt;img src="/what-is-an-ai-accelerator/ai-hardware-spectrum-featured_hu153888cb09e42ad7fc9ab1679dae3388_33262_640x0_resize_catmullrom_3.png" width="640" height="361"/>
&lt;!-- ![](ai-hardware-spectrum-featured.png) -->
&lt;img src="ai-hardware-spectrum-featured.png" width="800"/>
&lt;p>An AI accelerator is a dedicated processor designed to accelerate machine learning computations. Machine learning, and particularly its subset, deep learning is primarily composed of a large number of linear algebra computations, (i.e. matrix-matrix, matrix-vector operations) and these operations can be easily parallelized. AI accelerators are specialized hardware designed to accelerate these basic machine learning computations and improve performance, reduce latency and reduce cost of deploying machine learning based applications.&lt;/p>
&lt;h3 id="why-do-we-need-specialized-ai-accelerators">Why do we need specialized AI accelerators?&lt;/h3>
&lt;p>The two most important reasons for building dedicated processors for machine learning are:&lt;/p>
&lt;ol>
&lt;li>Energy efficiency&lt;/li>
&lt;li>Faster performance&lt;/li>
&lt;/ol>
&lt;p>Recent trends to improve model accuracy, have been to introduce larger models with more parameters and train them on larger data sets. As model sizes get larger, and current processors won’t be able to deliver the processing power needed to train or run inference on these models under tight time-to-train and inference latency requirements.&lt;/p>
&lt;div class="alert alert-primary" role="alert">
&lt;em>General purpose processors like CPUs trade-off energy efficiency for versatility and special purpose processors (AI accelerators) trade off versatility for energy efficiency.&lt;/em>
&lt;/div>
&lt;p>AI accelerators on the other hand can be designed with features to minimize memory access, offer larger on-chip cache and include dedicated hardware features to accelerate matrix-matrix computations. Since AI accelerators are purpose built devices it is “aware” of the algorithms that it runs on and its dedicated features will run it more efficiently than a general purpose processor.&lt;/p>
&lt;p>List of popular AI accelerators for &lt;strong>training&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>NVIDIA GPUs&lt;/strong>: Available on AWS, GCP, Azure and at your local computer store (See my recommendation list on the left menu)&lt;/li>
&lt;li>&lt;strong>AWS Tranium&lt;/strong>: Available on AWS&lt;/li>
&lt;li>&lt;strong>Intel Habana Gaudi&lt;/strong>: Available on AWS (v1) and Intel DevCloud (v1 and v2)&lt;/li>
&lt;li>&lt;strong>Google Cloud TPUs&lt;/strong>: Available on GCP and via Colab (v1-v4)&lt;/li>
&lt;/ul>
&lt;p>List of popular AI accelerators for &lt;strong>inference&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>NVIDIA GPUs&lt;/strong>: Available on AWS, GCP, Azure (See my recommendation list on the left menu)&lt;/li>
&lt;li>&lt;strong>AWS Inferentia&lt;/strong>: Available on AWS (See my recommend blog post below)&lt;/li>
&lt;li>&lt;strong>Intel Habana Gaudi&lt;/strong>: Available on AWS and Intel DevCloud (v1 and v2)&lt;/li>
&lt;li>&lt;strong>Google Cloud TPUs&lt;/strong>: Available on GCP and via Colab (v1-v4)&lt;/li>
&lt;/ul>
&lt;div class="alert alert-primary" role="alert">
Note: Modern GPUs have dedicated silicon (TensorCores) and precision types (TF32, BF16) designed for deep learning bringing them closer to dedicated AI accelerators vs. general purpose parallel processors
&lt;/div>
&lt;h3 id="recommended-blog-posts">Recommended blog posts&lt;/h3>
&lt;div class="td-card-group card-group p-0 mb-4">
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;img src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*SAgQSIEprO1looCxAdf52w.png" alt="">&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/ai-accelerators-machine-learning-algorithms-and-their-co-design-and-evolution-2676efd47179">AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution&lt;/a>&lt;/div>
&lt;/div>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*AGpm_2l-32AfXUAfOxwUKA.png" alt="">&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c">A complete guide to AI accelerators for deep learning inference — GPUs, AWS Inferentia and Amazon Elastic Inference&lt;/a>&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Blog: AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution</title><link>/ai-accelerators-and-machine-learning-algorithms-co-design-and-evolution/</link><pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate><guid>/ai-accelerators-and-machine-learning-algorithms-co-design-and-evolution/</guid><description>
&lt;img src="/ai-accelerators-and-machine-learning-algorithms-co-design-and-evolution/featured_hu153888cb09e42ad7fc9ab1679dae3388_33262_640x0_resize_catmullrom_3.png" width="640" height="361"/>
&lt;p>If you told me a few years ago that data scientists would be using Docker containers in their day to day work, I wouldn’t have believed you. As a member of the broader machine learning (ML) community I always considered Docker, Kubernetes, Swarm (remember that?) exotic infrastructure tools for IT/Ops experts. Today it’s a different story, rarely a day goes by when I don’t use a Docker container for training or hosting a model.
An attribute of machine learning development that makes it different from traditional software development is that it relies on specialized hardware such as GPUs, Habana Gaudi, AWS Inferentia to accelerate training and inference. This makes it challenging to have containerized deployments that are hardware-agnostic, which is one of the key benefits of containers. In this blog post I’ll discuss how Docker and container technologies have evolved to address this challenge. We’ll discuss:&lt;/p>
&lt;ul>
&lt;li>Why Docker has become an essential tool for machine learning today and how it addresses machine learning specific challenges&lt;/li>
&lt;li>How Docker accesses specialized hardware resources on heterogeneous systems that have more than one type of processor (CPU + AI accelerators).&lt;/li>
&lt;li>How different AI accelerators extend Docker for hardware access with examples of 1/ NVIDIA GPUs and NVIDIA Container Toolkit and 2/ AWS Inferentia and Neuron SDK support for containers&lt;/li>
&lt;li>How to scale Docker containers on Kubernetes with hardware accelerated nodes&lt;/li>
&lt;/ul>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
Read the full blog post here: &lt;br>&lt;br> &lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators-e076c6eb7802">How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators&lt;/a>&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
&lt;img src="featured.png" alt="">&lt;/div>
&lt;/div></description></item><item><title>Blog: How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators</title><link>/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators/</link><pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate><guid>/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators/</guid><description>
&lt;img src="/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators/featured_hu40258b408388e57d94159bf91c739a7c_102531_640x0_resize_catmullrom_3.png" width="640" height="285"/>
&lt;p>If you told me a few years ago that data scientists would be using Docker containers in their day to day work, I wouldn’t have believed you. As a member of the broader machine learning (ML) community I always considered Docker, Kubernetes, Swarm (remember that?) exotic infrastructure tools for IT/Ops experts. Today it’s a different story, rarely a day goes by when I don’t use a Docker container for training or hosting a model.
An attribute of machine learning development that makes it different from traditional software development is that it relies on specialized hardware such as GPUs, Habana Gaudi, AWS Inferentia to accelerate training and inference. This makes it challenging to have containerized deployments that are hardware-agnostic, which is one of the key benefits of containers. In this blog post I’ll discuss how Docker and container technologies have evolved to address this challenge. We’ll discuss:&lt;/p>
&lt;ul>
&lt;li>Why Docker has become an essential tool for machine learning today and how it addresses machine learning specific challenges&lt;/li>
&lt;li>How Docker accesses specialized hardware resources on heterogeneous systems that have more than one type of processor (CPU + AI accelerators).&lt;/li>
&lt;li>How different AI accelerators extend Docker for hardware access with examples of 1/ NVIDIA GPUs and NVIDIA Container Toolkit and 2/ AWS Inferentia and Neuron SDK support for containers&lt;/li>
&lt;li>How to scale Docker containers on Kubernetes with hardware accelerated nodes&lt;/li>
&lt;/ul>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
Read the full blog post here:&lt;br>&lt;br> &lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/ai-accelerators-machine-learning-algorithms-and-their-co-design-and-evolution-2676efd47179">How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators&lt;/a>&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
&lt;img src="featured.png" alt="">&lt;/div>
&lt;/div></description></item><item><title>Blog: AWS GPU instances complete list</title><link>/complete-list-of-all-aws-gpu-instances/</link><pubDate>Thu, 02 Jun 2022 00:00:00 +0000</pubDate><guid>/complete-list-of-all-aws-gpu-instances/</guid><description>
&lt;p>Here is a complete list of all Amazon EC2 GPU instance types on AWS that I&amp;rsquo;ve painstakenly compiled, because you can&amp;rsquo;t find this information anywhere on AWS.
In the tabular format below, you&amp;rsquo;ll find more detailed information about GPU type, interconnect, Thermal design power (TDP), precision types supported etc.&lt;/p>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;img src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*1qFKgyph2CT2Dxat3tSqmQ.png" alt="">&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
From my blog post: &lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86">Choosing the right GPU for deep learning on AWS&lt;/a>&lt;/div>
&lt;/div>
&lt;h2 id="tabular-format">Tabular format&lt;/h2>
&lt;p>&lt;em>With more information than you were probably looking for&lt;/em> 😊&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Architecture&lt;/th>
&lt;th>NVIDIA GPU&lt;/th>
&lt;th>Instance type&lt;/th>
&lt;th>Instance name&lt;/th>
&lt;th>Number of GPUs&lt;/th>
&lt;th>GPU Memory (per GPU)&lt;/th>
&lt;th>GPU Interconnect (NVLink / PCIe)&lt;/th>
&lt;th>Thermal&lt;br>Design Power (TDP) from nvidia-smi&lt;/th>
&lt;th>Tensor Cores (mixed-precision)&lt;/th>
&lt;th>Precision Support&lt;/th>
&lt;th>CPU Type&lt;/th>
&lt;th>Nitro based&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A100&lt;/td>
&lt;td>P4&lt;/td>
&lt;td>p4d.24xlarge&lt;/td>
&lt;td>8&lt;/td>
&lt;td>40 GB&lt;/td>
&lt;td>NVLink gen 3 (600 GB/s)&lt;/td>
&lt;td>400W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.2xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.4xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.8xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.16xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.12xlarge&lt;/td>
&lt;td>4&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.24xlarge&lt;/td>
&lt;td>4&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.48xlarge&lt;/td>
&lt;td>8&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5g.xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>AWS Graviton2&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5g.2xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>AWS Graviton2&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5g.4xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>AWS Graviton2&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5g.8xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>AWS Graviton2&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5g.16xlarge&lt;/td>
&lt;td>2&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>AWS Graviton2&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5g.metal&lt;/td>
&lt;td>2&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>AWS Graviton2&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.2xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.4xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.8xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.16xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.12xlarge&lt;/td>
&lt;td>4&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.metal&lt;/td>
&lt;td>8&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Volta&lt;/td>
&lt;td>V100&lt;/td>
&lt;td>P3&lt;/td>
&lt;td>p3.2xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 1)&lt;/td>
&lt;td>FP64, FP32, FP16&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Volta&lt;/td>
&lt;td>V100&lt;/td>
&lt;td>P3&lt;/td>
&lt;td>p3.8xlarge&lt;/td>
&lt;td>4&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NVLink gen 2 (300 GB/s)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 1)&lt;/td>
&lt;td>FP64, FP32, FP16&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Volta&lt;/td>
&lt;td>V100&lt;/td>
&lt;td>P3&lt;/td>
&lt;td>p3.16xlarge&lt;/td>
&lt;td>8&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NVLink gen 2 (300 GB/s)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 1)&lt;/td>
&lt;td>FP64, FP32, FP16&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Volta&lt;/td>
&lt;td>V100*&lt;/td>
&lt;td>P3&lt;/td>
&lt;td>p3dn.24xlarge&lt;/td>
&lt;td>8&lt;/td>
&lt;td>32 GB&lt;/td>
&lt;td>NVLink gen 2 (300 GB/s)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 1)&lt;/td>
&lt;td>FP64, FP32, FP16&lt;/td>
&lt;td>Intel Xeon (Skylake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kepler&lt;/td>
&lt;td>K80&lt;/td>
&lt;td>P2&lt;/td>
&lt;td>p2.xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>12 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>149W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP64, FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kepler&lt;/td>
&lt;td>K80&lt;/td>
&lt;td>P2&lt;/td>
&lt;td>p2.8xlarge&lt;/td>
&lt;td>8&lt;/td>
&lt;td>12 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>149W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP64, FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kepler&lt;/td>
&lt;td>K80&lt;/td>
&lt;td>P2&lt;/td>
&lt;td>p2.16xlarge&lt;/td>
&lt;td>16&lt;/td>
&lt;td>12 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>149W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP64, FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maxwell&lt;/td>
&lt;td>M60&lt;/td>
&lt;td>G3&lt;/td>
&lt;td>g3s.xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>8 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>150W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maxwell&lt;/td>
&lt;td>M60&lt;/td>
&lt;td>G3&lt;/td>
&lt;td>g3.4xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>8 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>150W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maxwell&lt;/td>
&lt;td>M60&lt;/td>
&lt;td>G3&lt;/td>
&lt;td>g3.8xlarge&lt;/td>
&lt;td>2&lt;/td>
&lt;td>8 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>150W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maxwell&lt;/td>
&lt;td>M60&lt;/td>
&lt;td>G3&lt;/td>
&lt;td>g3.16xlarge&lt;/td>
&lt;td>4&lt;/td>
&lt;td>8 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>150W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Blog: Best GPUs on AWS for Deep Learning</title><link>/best-gpus-on-aws-for-deep-learning/</link><pubDate>Wed, 11 May 2022 00:00:00 +0000</pubDate><guid>/best-gpus-on-aws-for-deep-learning/</guid><description>
&lt;img src="/best-gpus-on-aws-for-deep-learning/gpu4-featured_hu93531d3f19a1a68a0fc2d3bfd6de5052_4220_640x0_resize_catmullrom_3.png" width="640" height="142"/>
&lt;p>Here are 5 GPU instance recommendations on AWS that should serve majority of deep learning use-cases. For a complete deep dive into choosing the right GPU for deep learning on AWS, read my blog post:&lt;/p>
&lt;div class="alert alert-primary" role="alert">
&lt;h3 id="i-classfa-brands-fa-mediumi-choosing-the-right-gpu-for-deep-learning-on-awshttpsmediumcomtowards-data-sciencechoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86">&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86">Choosing the right GPU for deep learning on AWS&lt;/a>&lt;/h3>
&lt;/div>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
&lt;strong>Highest performing multi-GPU instance on AWS&lt;/strong>
&lt;/div>
&lt;div class="card-body">
&lt;p class="card-text">
&lt;p>&lt;img src="gpu1.png" alt="GPU">&lt;/p>
&lt;p>&lt;strong>Instance: p4d.24xlarge&lt;/strong>&lt;/p>
&lt;p>&lt;strong>When to use it:&lt;/strong> When you need all the performance you can get. Use it for distributed training on large models and datasets.&lt;/p>
&lt;p>&lt;strong>What you get:&lt;/strong> 8 x NVIDIA A100 GPUs with 40 GB GPU memory per GPU. Based on the latest NVIDIA Ampere architecture. Includes 3rd generation NVLink for fast multi-GPU training.&lt;/p>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;br/>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
&lt;strong>Highest performing single-GPU instance on AWS&lt;/strong>
&lt;/div>
&lt;div class="card-body">
&lt;p class="card-text">
&lt;p>&lt;img src="gpu2.png" alt="GPU">&lt;/p>
&lt;p>&lt;strong>Instance: p3.2xlarge&lt;/strong>&lt;/p>
&lt;p>&lt;strong>When to use it:&lt;/strong> When you want the highest performance Single GPU and you’re fine with 16 GB of GPU memory.&lt;/p>
&lt;p>&lt;strong>What you get:&lt;/strong> 1 x NVIDIA V100 GPU with 16 GB of GPU memory. Based on the older NVIDIA Volta architecture. The best performing single-GPU is still the NVIDIA A100 on P4 instance, but you can only get 8 x NVIDIA A100 GPUs on P4. This GPU has a slight performance edge over NVIDIA A10G on G5 instance discussed next, but G5 is far more cost-effective and has more GPU memory.&lt;/p>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;br/>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
&lt;strong>Best performance/cost, single-GPU instance on AWS&lt;/strong>
&lt;/div>
&lt;div class="card-body">
&lt;p class="card-text">
&lt;p>&lt;img src="gpu3.png" alt="GPU">&lt;/p>
&lt;p>&lt;strong>Instance: g5.xlarge&lt;/strong>&lt;/p>
&lt;p>&lt;strong>When to use it:&lt;/strong> When you want high-performance, more GPU memory at lower cost than P3 instance&lt;/p>
&lt;p>&lt;strong>What you get:&lt;/strong> 1 x NVIDIA A10G GPU with 24 GB of GPU memory, based on the latest Ampere architecture. NVIDIA A10G can be seen as a lower powered cousin of the A100 on the p4d.24xlarge so it’s easy to migrate and scale when you need more compute. Consider larger sizes withg5.(2/4/8/16)xlarge for the same single-GPU with more vCPUs and higher system memory if you have more pre or post processing steps.&lt;/p>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;br/>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
&lt;strong>Best performance/cost, multi-GPU instance on AWS&lt;/strong>
&lt;/div>
&lt;div class="card-body">
&lt;p class="card-text">
&lt;p>&lt;img src="gpu4-featured.png" alt="GPU">&lt;/p>
&lt;p>&lt;strong>Instance: p3.(8/16)xlarge&lt;/strong>&lt;/p>
&lt;p>&lt;strong>When to use it:&lt;/strong> Cost-effective multi-GPU model development and training.&lt;/p>
&lt;p>&lt;strong>What you get:&lt;/strong> p3.8xlarge has 4 x NVIDIA V100 GPUs and p3.16xlarge has 8 x NVIDIA V100 GPUs with 16 GB of GPU memory on each GPU, based on the older NVIDIA Volta architecture. For larger models, datasets and faster performance consider P4 instances.&lt;/p>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;br/>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
&lt;strong>High-performance GPU instance at a budget on AWS&lt;/strong>
&lt;/div>
&lt;div class="card-body">
&lt;p class="card-text">
&lt;p>&lt;img src="gpu5.png" alt="GPU">&lt;/p>
&lt;p>&lt;strong>Instance: g4dn.xlarge&lt;/strong>&lt;/p>
&lt;p>&lt;strong>When to use it:&lt;/strong> Lower performance than other options at lower cost for model development and training. Cost effective model inference deployment.&lt;/p>
&lt;p>&lt;strong>What you get:&lt;/strong> 1 x NVIDIA T4 GPU with 16 GB of GPU memory. Based on the previous generation NVIDIA Turing architecture. Consider g4dn.(2/4/8/16)xlarge for more vCPUs and higher system memory if you have more pre or post processing.&lt;/p>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;br/>
## Related blog posts
&lt;div class="td-card-group card-group p-0 mb-4">
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;img src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*SAgQSIEprO1looCxAdf52w.png" alt="">&lt;/h5>
&lt;/div>
&lt;div class="card-footer">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/ai-accelerators-machine-learning-algorithms-and-their-co-design-and-evolution-2676efd47179">AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution&lt;/a>&lt;/div>
&lt;/div>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;img src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*qAx5CnFbebRR_bOyQbSGYw.png" alt="">&lt;/h5>
&lt;/div>
&lt;div class="card-footer">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86">Choosing the right GPU for deep learning on AWS&lt;/a>&lt;/div>
&lt;/div>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;img src="https://miro.medium.com/v2/resize:fit:4800/0*Po5qTIX3N1ZJoJHd" alt="">&lt;/h5>
&lt;/div>
&lt;div class="card-footer">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators-e076c6eb7802">How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators&lt;/a>&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Blog: A complete guide to AI accelerators for deep learning inference — GPUs, AWS Inferentia and Amazon Elastic Inference</title><link>/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/</guid><description>
&lt;img src="/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/featured_hu153888cb09e42ad7fc9ab1679dae3388_33262_640x0_resize_catmullrom_3.png" width="640" height="361"/>
&lt;h3 id="lets-start-by-answering-the-question-what-is-an-ai-accelerator">Let’s start by answering the question “What is an AI accelerator?”&lt;/h3>
&lt;p>An AI accelerator is a dedicated processor designed to accelerate machine learning computations. Machine learning, and particularly its subset, deep learning is primarily composed of a large number of linear algebra computations, (i.e. matrix-matrix, matrix-vector operations) and these operations can be easily parallelized. AI accelerators are specialized hardware designed to accelerate these basic machine learning computations and improve performance, reduce latency and reduce cost of deploying machine learning based applications.&lt;/p>
&lt;h3 id="do-i-need-an-ai-accelerator-for-machine-learning-ml-inference">Do I need an AI accelerator for machine learning (ML) inference?&lt;/h3>
&lt;p>Let’s say you have an ML model as part of your software application. The prediction step (or inference) is often the most time consuming part of your application that directly affects user experience. A model that takes several hundreds of milliseconds to generate text translations or apply filters to images or generate product recommendations, can drive users away from your “sluggish”, “slow”, “frustrating to use” app.
By speeding up inference, you can reduce the overall application latency and deliver an app experience that can be described as “smooth”, “snappy”, and “delightful to use”. And you can speed up inference by offloading ML model prediction computation to an AI accelerator.
With great market needs comes great many product alternatives, so naturally there is more than one way to accelerate your ML models in the cloud.
In this blog post, I’ll explore three popular options:&lt;/p>
&lt;ol>
&lt;li>GPUs: Particularly, the high-performance NVIDIA T4 and NVIDIA V100 GPUs&lt;/li>
&lt;li>AWS Inferentia: A custom designed machine learning inference chip by AWS&lt;/li>
&lt;li>Amazon Elastic Inference (EI): An accelerator that saves cost by giving you access to variable-size GPU acceleration, for models that don’t need a dedicated GPU&lt;/li>
&lt;/ol>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
Read the full blog post here: &lt;br>&lt;br> &lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c">A complete guide to AI accelerators for deep learning inference — GPUs, AWS Inferentia and Amazon Elastic Inference&lt;/a>&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
&lt;img src="featured.png" alt="">&lt;/div>
&lt;/div></description></item><item><title>Blog: Choosing the right GPU for deep learning on AWS</title><link>/choosing-the-right-gpu-for-deep-learning-on-aws/</link><pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate><guid>/choosing-the-right-gpu-for-deep-learning-on-aws/</guid><description>
&lt;img src="/choosing-the-right-gpu-for-deep-learning-on-aws/featured_hu3cf755c2b3f91847dc184a8c7feac088_100672_640x0_resize_catmullrom_3.png" width="640" height="453"/>
&lt;p>On AWS, you can launch GPU instances with different GPU memory sizes (8 GB, 16 GB, 24 GB, 32 GB, 40 GB), NVIDIA GPU generations (Ampere, Turing, Volta, Maxwell, Kepler) different capabilities (FP64, FP32, FP16, INT8, Sparsity, TensorCores, NVLink), different number of GPUs per instance (1, 2, 4, 8, 16), and paired with different CPUs (Intel, AMD, Graviton2). You can also select instances with different vCPUs (core thread count), system memory and network bandwidth and add a range of storage options (object storage, network file systems, block storage, etc.) — in summary, you have options.&lt;/p>
&lt;p>My goal with this blog post is to provide you with guidance on how you can choose the right GPU instance on AWS for your deep learning projects. I’ll discuss key features and benefits of various EC2 GPU instances, and workloads that are best suited for each instance type and size. If you’re new to AWS, or new to GPUs, or new to deep learning, my hope is that you’ll find the information you need to make the right choice for your projects.&lt;/p>
&lt;h4 id="topics-covered-in-this-blog-post">Topics covered in this blog post:&lt;/h4>
&lt;ol>
&lt;li>Key recommendations for the busy data scientist/ML practitioner&lt;/li>
&lt;li>Why you should choose the right GPU instance not just the right GPU&lt;/li>
&lt;li>Deep dive on GPU instance types: P4, P3, G5 (G5g), G4, P2 and G3&lt;/li>
&lt;li>Other machine learning accelerators and instances on AWS&lt;/li>
&lt;li>Cost optimization tips when using GPU instances for ML&lt;/li>
&lt;li>What software and frameworks to use on AWS?&lt;/li>
&lt;li>Which GPUs to consider for HPC use-cases?&lt;/li>
&lt;li>A complete and unapologetically detailed spreadsheet of all AWS GPU instances and their features&lt;/li>
&lt;/ol>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
Read the full blog post here: &lt;br>&lt;br> &lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86">Choosing the right GPU for deep learning on AWS&lt;/a>&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
&lt;img src="featured.png" alt="">&lt;/div>
&lt;/div></description></item></channel></rss>