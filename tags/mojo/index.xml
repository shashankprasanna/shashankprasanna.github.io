<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Shashank Prasanna â€“ mojo</title><link>https://shashankprasanna.com/tags/mojo/</link><description>Recent content in mojo on Shashank Prasanna</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Mon, 08 May 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://shashankprasanna.com/tags/mojo/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Benchmarking Modular MojoðŸ”¥ and PyTorch torch.compile() on Mandelbrot function</title><link>https://shashankprasanna.com/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/</link><pubDate>Mon, 08 May 2023 00:00:00 +0000</pubDate><guid>https://shashankprasanna.com/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/</guid><description>
&lt;img src="https://shashankprasanna.com/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/featured-background_hu52d0dfd7da44399cff6626242a002e0e_3253184_640x0_resize_catmullrom_3.png" width="640" height="457"/>
&lt;p>Last week, Modular - an startup co-founded by Chris Lattner (of LLVM, Swift, MLIR fame), announced a brand new high-performance language called MojoðŸ”¥. MojoðŸ”¥ looks and reads like Python but that&amp;rsquo;s only on the surface, underneath the familiar Python syntax Mojo uses it&amp;rsquo;s own JIT and AOT compilation process to accelerate Python code. Although Mojo doesn&amp;rsquo;t fully support all of Python today, according to Mojo docs, over time Mojo is expected to become a superset of Python.
&lt;figure class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 410px">
&lt;img class="card-img-top" src="https://shashankprasanna.com/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/featured-background_hu52d0dfd7da44399cff6626242a002e0e_3253184_400x300_fit_catmullrom_3.png" width="400" height="285">
&lt;figcaption class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
PyTorch's got some MojoðŸ”¥
&lt;/p>
&lt;/figcaption>
&lt;/figure>
Mojo isn&amp;rsquo;t open-source yet and can only be accessed from the Mojo playground. So I promptly applied and got access to Mojo Playground a couple of days after the accouncement on May 3rd. First thing I wanted to do was to compare Mojo&amp;rsquo;s performance to PyTorch. While PyTorch is a popular framework for deep learning, its also a capable replacement for numpy as a high-performance scientific computing library.&lt;/p>
&lt;p>And my favorite feature of PyTorch is the new &lt;code>torch.compile()&lt;/code> API introduced in PyTorch 2.0 that can accelerate arbitrary functions (with limitations) written using the PyTorch API. It takes PyTorch highlevel API, optimizes it and generates C++ or GPU code to improve it&amp;rsquo;s performance. I&amp;rsquo;ve discussed &lt;code>torch.compile()&lt;/code> in great detail in my blog post, and I highly recommend reading it if you want to learn about how PyTorch compiler does operator fusion and CPU/GPU code-generation:&lt;/p>
&lt;div class="td-card card mb-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26">How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation&lt;/a>
&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>In this blog post I want to discuss relative performance between MojoðŸ”¥ and PyTorch, and I picked the Mandelbrot example that Jeremy Howard (or FastAI fame) demoed during the Modular keynote. I reimplemnted Mojo&amp;rsquo;s Mandelbrot example in PyTorch to compared it&amp;rsquo;s performance with Mojo. Before I get into the code, here are the results.&lt;/p>
&lt;p>&lt;strong>Summary: Mojo is fast! and PyTorch is no slouch either!&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Language/Framework&lt;/th>
&lt;th>Mandelbrot execution&lt;br>(200 iterations)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>PyTorch GPU &lt;code>torch.compile()&lt;/code>&lt;/td>
&lt;td>~165 Î¼s (micro seconds)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Mojo CPU&lt;/td>
&lt;td>~2.6 ms&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PyTorch CPU &lt;code>torch.compile()&lt;/code>&lt;/td>
&lt;td>~9 ms&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PyTorch GPU&lt;/td>
&lt;td>~15 ms&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PyTorch CPU&lt;/td>
&lt;td>~50 ms&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PyTorch Apple M2 MPS&lt;/td>
&lt;td>~70 ms&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Python/numpy&lt;/td>
&lt;td>~152 ms&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="interesting-observations">Interesting observations&lt;/h4>
&lt;ul>
&lt;li>Mojo is the fastest CPU implementation&lt;/li>
&lt;li>PyTorch GPU with &lt;code>torch.compile()&lt;/code> generates a fused cuda kernel making it the fastest on GPU&lt;/li>
&lt;li>PyTorch CPU with &lt;code>torch.compile()&lt;/code> which generates fused C++ code is still faster than PyTorch GPU without compilation&lt;/li>
&lt;/ul>
&lt;p>It should come as no surprise that PyTorch generated custom fused kernel for Mandelbrot function running on GPU is indeed faster than Mojo CPU, it&amp;rsquo;s not even a fair comparision. PyTorch CPU is only slightly slower, but makes up for performance with better usability. Mojo is harder to use and I&amp;rsquo;m positive the UX will improve over time.&lt;/p>
&lt;h2 id="benchmarks-and-caveats">Benchmarks and caveats&lt;/h2>
&lt;p>This is not a scientific benchmark test. This is a rather crude, and hacked-together-in-a-day example that should illustrate the performance differences and coding approaches, so take it with a grain of Sodium Chloride.&lt;/p>
&lt;h4 id="my-naive-testing-methodology">My naive testing methodology:&lt;/h4>
&lt;ul>
&lt;li>I use Jupyter&amp;rsquo;s native &lt;code>timeit&lt;/code> with &lt;code>10&lt;/code> repeats to benchmark and report mean and variance.&lt;/li>
&lt;li>For GPU, I call &lt;code>torch.cuda.synchronize()&lt;/code> before measurement to ensure that the kernel is fully executed before the timer starts.&lt;/li>
&lt;li>This code example also benchmarks tensors/arrays creation which is in the body of the function, which migh not be a feature that arises in real-world scenarios.&lt;/li>
&lt;li>I don&amp;rsquo;t measure the compilation time for PyTorch 2.0 and that does take a lot of time to generate loop-unrolled C++ code for CPU and NVPTX for GPU.&lt;/li>
&lt;/ul>
&lt;h4 id="hardware-differences">Hardware differences&lt;/h4>
&lt;p>Mojo is not open-source (yet), and the only way to run it is on the early access Mojo Playground which has a different configuration compared to my desktop running PyTorch. For the Apple M2 benchmark I use yet another computer which is a MacBook Pro laptop. Suffice to say, this is not a fair comparison. Hardware details:&lt;/p>
&lt;ul>
&lt;li>PyTorch on Desktop
&lt;ul>
&lt;li>CPU: Intel Core i7-9700K CPU @ 3.60GHz with 8 cores&lt;/li>
&lt;li>GPU: NVIDIA Titan V&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>PyTorch on Mac
&lt;ul>
&lt;li>Apple MacBook Pro with M2 Apple Silicon&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>MojoðŸ”¥ on Mojo Playground
&lt;ul>
&lt;li>Intel Xeon Platinum 8375C CPU @ 2.90GHz with 32 cores&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="alert alert-warning" role="alert">
If you have access to the same hardware as the Mojo Playground and have the ability to run PyTorch 2.0 on it. Please try and reproduce these results and let me know if you see better performance
&lt;/div>
&lt;h2 id="why-is-pytorch-so-much-faster-than-pythonnumpy-on-cpu">Why is PyTorch so much faster than Python/Numpy on CPU?&lt;/h2>
&lt;p>PyTorch is faster than Python/Numpy because the higher level PyTorch API calls highly optimized C++ routines implemented in the ATen library. These routines are eagerly evaluated, which means that each PyTorch API call is executed immediately which adds some function call overhead with every API call. To address this, PyTorch 2.0 introduced a compilation API called &lt;code>torch.compile()&lt;/code> which takes eager PyTorch code, optimizes it and generates C++ code with OpenMP pragmas for parallelization on CPU or generates GPU code using OpenAI Triton. This is similar to what Numba does for Python code, but PyTorch 2.0&amp;rsquo;s &lt;code>torch.compile()&lt;/code> is much more powerful because it can fuse multiple PyTorch API calls into a single kernel, which reduces function call overhead and improves performance.&lt;/p>
&lt;p>I&amp;rsquo;ve discussed this in detail in my PyTorch 2.0 blog post, but here is a screenshot of what the automatically generated fused kernels for C++ for CPU and OpenAI Triton for GPU look like for Mandelbrot function&lt;/p>
&lt;div class="alert alert-primary" role="alert">
&lt;i class='fa-brands fa-medium'>&lt;/i>
&lt;a href="https://medium.com/towards-data-science/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26">How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation&lt;/a>
&lt;/div>
&lt;div class="td-card-deck card-deck mb-4">
&lt;div class="td-card card mb-4">
&lt;div class="card-header">
Autogenerated C++ code for Mandelbrot function with &lt;code>torch.compile()&lt;/code>
&lt;/div>
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;a href="mandelbrot_cpp_fused.png">&lt;img src="mandelbrot_cpp_fused.png" alt="">&lt;/a>
&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
Click to zoom
&lt;/div>
&lt;/div>
&lt;div class="td-card card mb-4">
&lt;div class="card-header">
Autogenerated OpenAI Triton code for Mandelbrot function with &lt;code>torch.compile()&lt;/code>
&lt;/div>
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;a href="mandelbrot_gpu_fused.png">&lt;img src="mandelbrot_gpu_fused.png" alt="">&lt;/a>
&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
Click to zoom
&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="baseline-benchmarking-pythonnumpy">Baseline: Benchmarking Python/Numpy&lt;/h2>
&lt;p>Let&amp;rsquo;s start with a baseline. Here&amp;rsquo;s the Python/Numpy implementation of Mandelbrot function. I&amp;rsquo;ve also included a function to plot the Mandelbrot set using matplotlib. We&amp;rsquo;ll later modify this function to use PyTorch.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> numpy &lt;span style="color:#66d9ef">as&lt;/span> np
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> torch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> matplotlib.pyplot &lt;span style="color:#66d9ef">as&lt;/span> plt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> matplotlib.colors &lt;span style="color:#66d9ef">as&lt;/span> colors
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> time
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> warnings
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">mandelbrot_numpy&lt;/span>(max_iter&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">200&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Define the boundaries of the complex plane&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> xn &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">450&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> yn &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">375&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> xmin &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">2.25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> xmax &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0.75&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ymin &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1.25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ymax &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1.25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Create the grid of complex numbers&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x_values &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>linspace(xmin, xmax, xn, dtype&lt;span style="color:#f92672">=&lt;/span>np&lt;span style="color:#f92672">.&lt;/span>float64)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_values &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>linspace(ymin, ymax, yn, dtype&lt;span style="color:#f92672">=&lt;/span>np&lt;span style="color:#f92672">.&lt;/span>float64)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rx, iy &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>meshgrid(x_values, y_values, indexing&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;xy&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> rx&lt;span style="color:#f92672">.&lt;/span>copy()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y &lt;span style="color:#f92672">=&lt;/span> iy&lt;span style="color:#f92672">.&lt;/span>copy()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mask &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>zeros_like(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> i &lt;span style="color:#f92672">in&lt;/span> range(max_iter):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x_prev &lt;span style="color:#f92672">=&lt;/span> x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_prev &lt;span style="color:#f92672">=&lt;/span> y
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> x_prev&lt;span style="color:#f92672">**&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">-&lt;/span> y_prev&lt;span style="color:#f92672">**&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">+&lt;/span> rx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>&lt;span style="color:#f92672">*&lt;/span>x_prev&lt;span style="color:#f92672">*&lt;/span>y_prev &lt;span style="color:#f92672">+&lt;/span> iy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inside &lt;span style="color:#f92672">=&lt;/span> np&lt;span style="color:#f92672">.&lt;/span>sqrt(x&lt;span style="color:#f92672">**&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">+&lt;/span> y&lt;span style="color:#f92672">**&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>) &lt;span style="color:#f92672">&amp;lt;=&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mask&lt;span style="color:#f92672">+=&lt;/span>inside
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> mask
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">make_plot_python&lt;/span>(m):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> xn &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">450&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> yn &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">375&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dpi &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">32&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> width &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> height &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span> &lt;span style="color:#f92672">*&lt;/span> yn &lt;span style="color:#f92672">//&lt;/span> xn
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fig &lt;span style="color:#f92672">=&lt;/span> plt&lt;span style="color:#f92672">.&lt;/span>figure(&lt;span style="color:#ae81ff">1&lt;/span>, [width, height], dpi&lt;span style="color:#f92672">=&lt;/span>dpi)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ax &lt;span style="color:#f92672">=&lt;/span> fig&lt;span style="color:#f92672">.&lt;/span>add_axes([&lt;span style="color:#ae81ff">0.0&lt;/span>, &lt;span style="color:#ae81ff">0.0&lt;/span>, &lt;span style="color:#ae81ff">1.0&lt;/span>, &lt;span style="color:#ae81ff">1.0&lt;/span>], frame_on&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>, aspect&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> light &lt;span style="color:#f92672">=&lt;/span> colors&lt;span style="color:#f92672">.&lt;/span>LightSource(&lt;span style="color:#ae81ff">315&lt;/span>, &lt;span style="color:#ae81ff">10&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> image &lt;span style="color:#f92672">=&lt;/span> light&lt;span style="color:#f92672">.&lt;/span>shade(m, plt&lt;span style="color:#f92672">.&lt;/span>cm&lt;span style="color:#f92672">.&lt;/span>hot, colors&lt;span style="color:#f92672">.&lt;/span>PowerNorm(&lt;span style="color:#ae81ff">0.3&lt;/span>), blend_mode&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;hsv&amp;#39;&lt;/span>, vert_exag&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1.5&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> plt&lt;span style="color:#f92672">.&lt;/span>imshow(image)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> plt&lt;span style="color:#f92672">.&lt;/span>axis(&lt;span style="color:#e6db74">&amp;#34;off&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> plt&lt;span style="color:#f92672">.&lt;/span>show()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Output:&lt;/strong>
With a baseline established let&amp;rsquo;s compare the performance of PyTorch and Mojo.
&lt;img src="mandel_numpy.png" alt="">&lt;/p>
&lt;h2 id="benchmarking-mojo-on-the-mojo-playground">Benchmarking Mojo on the Mojo playground&lt;/h2>
&lt;p>This comparision is a bit unfair because MojoðŸ”¥ playground has a 32 core Xeon CPU, but I only have a modest 4 year old 8 core desktop CPU. The clock frequency, memory bandwidth and cache sizes and number of cores are all different, but I can&amp;rsquo;t install PyTorch on Mojo playground so this is the best I can do for comparision for now.&lt;/p>
&lt;p>At the bottom of this notebook, I add these few lines of code to measure the execution time of the mandelbrot function.&lt;/p>
&lt;p>&lt;img src="mojo_playground.png" alt="">&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> Time &lt;span style="color:#f92672">import&lt;/span> now
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>let eval_begin &lt;span style="color:#f92672">=&lt;/span> now()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>let mandelbrot_set &lt;span style="color:#f92672">=&lt;/span> compute_mandelbrot_simd()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>let eval_end &lt;span style="color:#f92672">=&lt;/span> now()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>let execution_time &lt;span style="color:#f92672">=&lt;/span> (eval_end &lt;span style="color:#f92672">-&lt;/span> eval_begin)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print(&lt;span style="color:#e6db74">&amp;#34;execution_time:&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print(F64(execution_time) &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#ae81ff">1000000&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Output&lt;/strong>
I found it difficult to benchmark Mojo, because Python benchmarking tools don&amp;rsquo;t work very readily. I ran the above code several times to ensure that the results are consistent.
&lt;img src="benchmark_mojo.png" alt="">&lt;/p>
&lt;h2 id="benchmarking-pytorch-cpu">Benchmarking PyTorch CPU&lt;/h2>
&lt;p>To update our mandelbrot function from numpy implementation to PyTorch implementation I made the following small changes&lt;/p>
&lt;ul>
&lt;li>replace &lt;code>np&lt;/code> with &lt;code>torch&lt;/code>&lt;/li>
&lt;li>add &lt;code>device=device&lt;/code> to the tensor creation calls, this allows us to pass the appropriate CPU, GPU or Apple MPS device to PyTorch.&lt;/li>
&lt;/ul>
&lt;p>Updated mandelbrot function:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">mandelbrot_pytorch&lt;/span>(device&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;cpu&amp;#39;&lt;/span>, max_iter&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">200&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Define the boundaries of the complex plane&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> xn &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">450&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> yn &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">375&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> xmin &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">2.25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> xmax &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0.75&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ymin &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1.25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ymax &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">1.25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Create the grid of complex numbers&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x_values &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>linspace(xmin, xmax, xn, device&lt;span style="color:#f92672">=&lt;/span>device)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_values &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>linspace(ymin, ymax, yn, device&lt;span style="color:#f92672">=&lt;/span>device)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rx, iy &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>meshgrid(x_values, y_values, indexing&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;xy&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> rx&lt;span style="color:#f92672">.&lt;/span>clone()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y &lt;span style="color:#f92672">=&lt;/span> iy&lt;span style="color:#f92672">.&lt;/span>clone()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mask &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>zeros_like(x, device&lt;span style="color:#f92672">=&lt;/span>device)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> i &lt;span style="color:#f92672">in&lt;/span> range(max_iter):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x_prev &lt;span style="color:#f92672">=&lt;/span> x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y_prev &lt;span style="color:#f92672">=&lt;/span> y
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> x_prev&lt;span style="color:#f92672">**&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">-&lt;/span> y_prev&lt;span style="color:#f92672">**&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">+&lt;/span> rx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> y &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>&lt;span style="color:#f92672">*&lt;/span>x_prev&lt;span style="color:#f92672">*&lt;/span>y_prev &lt;span style="color:#f92672">+&lt;/span> iy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inside &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>sqrt(x&lt;span style="color:#f92672">**&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">+&lt;/span> y&lt;span style="color:#f92672">**&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>) &lt;span style="color:#f92672">&amp;lt;=&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mask&lt;span style="color:#f92672">+=&lt;/span>inside
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> mask
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now let&amp;rsquo;s call our &lt;code>mandelbrot_pytorch&lt;/code> function with &lt;code>device='cpu'&lt;/code>
&lt;img src="mandel_pytorch_cpu.png" alt="">&lt;/p>
&lt;h2 id="benchmarking-pytorch-cpu-with-torchcompile">Benchmarking PyTorch CPU with &lt;code>torch.compile()&lt;/code>&lt;/h2>
&lt;p>You can compile PyTorch functions using &lt;code>torch.compile()&lt;/code> and TorchInductor will optimize and generate C++ code with OpenMP pragmas for parallization. This will significantly improve the performance of the function. All of this happens under the hood when you run the code below, but you can pass an additional argument &lt;code>options={'trace.enabled':True}&lt;/code> to see the generated code. I discuss this is further detail in my PyTorch 2.0 blog post:
&lt;div class="td-card card mb-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26">How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation&lt;/a>
&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;/div>
&lt;/p>
&lt;p>Let&amp;rsquo;s compile our mandelbrot function for CPU backend:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>device &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;cpu&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mandelbrot_compiled &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>compile(mandelbrot_pytorch)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mandelbrot_set &lt;span style="color:#f92672">=&lt;/span> mandelbrot_compiled(device)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>make_plot_python(mandelbrot_set&lt;span style="color:#f92672">.&lt;/span>numpy())
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Output&lt;/strong>
&lt;img src="mandel_pytorch_cpu_compile.png" alt="">&lt;/p>
&lt;p>The generated C++ code looks like this&lt;/p>
&lt;figure class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 773px">
&lt;img class="card-img-top" src="https://shashankprasanna.com/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/mandelbrot_cpp_fused_hu7e430cbf97eb3e8735ed525bf4b9a7bc_268273_800x400_fit_catmullrom_3.png" width="763" height="400">
&lt;figcaption class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="benchmarking-pytorch-gpu">Benchmarking PyTorch GPU&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>device &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;cuda&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mandelbrot_set &lt;span style="color:#f92672">=&lt;/span> mandelbrot_pytorch(device)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>make_plot_python(mandelbrot_set&lt;span style="color:#f92672">.&lt;/span>cpu()&lt;span style="color:#f92672">.&lt;/span>numpy())
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Output&lt;/strong>
&lt;img src="mandel_pytorch_gpu.png" alt="">&lt;/p>
&lt;h2 id="benchmarking-pytorch-gpu-with-torchcompile">Benchmarking PyTorch GPU with &lt;code>torch.compile()&lt;/code>&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>device &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;cuda&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mandelbrot_compiled &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>compile(mandelbrot_pytorch)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mandelbrot_set &lt;span style="color:#f92672">=&lt;/span> mandelbrot_compiled(device)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>make_plot_python(mandelbrot_set&lt;span style="color:#f92672">.&lt;/span>cpu()&lt;span style="color:#f92672">.&lt;/span>numpy())
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="mandel_pytorch_gpu_compile.png" alt="">&lt;/p>
&lt;p>The generated OpenAI Triton code which gets compiled to NVPTX for GPUs, looks like this&lt;/p>
&lt;figure class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 728px">
&lt;img class="card-img-top" src="https://shashankprasanna.com/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/mandelbrot_gpu_fused_huc397d05ec5af180dad415181b17e01e2_296767_800x400_fit_catmullrom_3.png" width="718" height="400">
&lt;figcaption class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="benchmarking-pytorch-apple-m1m2-silicon-with-mps-support">Benchmarking PyTorch Apple M1/M2 Silicon with MPS support&lt;/h2>
&lt;p>&lt;img src="mandel_pytorch_apple_m2.png" alt="">&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>That&amp;rsquo;s it folks! I hope you enjoyed this quick comparision of PyTorch and MojoðŸ”¥.
Mojo is fast, but doesn&amp;rsquo;t have the same level of usability of PyTorch, but that may just be just a matter of time and community support. PyTorch&amp;rsquo;s one-two punch combo of eager mode with high-level Tensor API and compilation with &lt;code>torch.compile()&lt;/code> is a powerful combination today. However, the PyTorch ecosystem is also quite fragmented with multiple code paths for different accelerators: TorchInductor (GPUs, CPUs), XLA (TPU, AWS Tranium/Inferentia), custom bindings/bridges (Intel Habana, MPS), and some accelerators like GPUs implement all paths. This leaves the end user and hardware vendors in a dilemma.&lt;/p>
&lt;p>One of the main benefit of Mojo that I see is the ability to write OpenAI Triton style kernel code in the Python language with fast or faster then C++ performance. This would make supporting custom ops for inference easier. We&amp;rsquo;re certainly living in an exciting times for AI Infra, AI accelerators and AI frameworks. Maybe we&amp;rsquo;re at the cusp of another LLVM moment but for AI.&lt;/p>
&lt;p>If you enjoyed reading this, check out my other blog posts on &lt;a href="https://medium.com/@shashankprasanna">Medium&lt;/a> or reach out to me on social media, links are on the homepage. Cheers!&lt;/p></description></item></channel></rss>