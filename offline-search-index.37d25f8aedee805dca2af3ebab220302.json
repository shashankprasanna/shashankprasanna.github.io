[{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"Personal Blog"},{"body":"Here are some of my popular blogposts with over 150k views. Read these and more on my medium blog: https://medium.com/@shashankprasanna Choosing the right GPU for deep learning on AWS AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators A complete guide to AI accelerators for deep learning inference ‚Äî GPUs, AWS Inferentia and Amazon Elastic Inference ","categories":"","description":"","excerpt":"Here are some of my popular blogposts with over 150k views. Read these ‚Ä¶","ref":"/popular_blog_posts/","tags":"","title":"Popular blog posts"},{"body":"Here are a list of PyTorch 2.0 Ask the engineers series I‚Äôve hosted. Watch to get a deep dive on PyTorch 2.0 new features.\nVisit the PyTorch YouTube channel https://www.youtube.com/@PyTorch/ ","categories":"","description":"","excerpt":"Here are a list of PyTorch 2.0 Ask the engineers series I‚Äôve hosted. ‚Ä¶","ref":"/pytorch2-livestreams/","tags":"","title":"PyTorch 2.0 livestreams"},{"body":"Here are 5 GPU instance recommendations on AWS that should serve majority of deep learning use-cases. For a complete deep dive into choosing the right GPU for deep learning on AWS, read my blog post:\nChoosing the right GPU for deep learning on AWS Highest performing multi-GPU instance on AWS Instance: p4d.24xlarge\nWhen to use it: When you need all the performance you can get. Use it for distributed training on large models and datasets.\nWhat you get: 8 x NVIDIA A100 GPUs with 40 GB GPU memory per GPU. Based on the latest NVIDIA Ampere architecture. Includes 3rd generation NVLink for fast multi-GPU training.\nHighest performing single-GPU instance on AWS Instance: p3.2xlarge\nWhen to use it: When you want the highest performance Single GPU and you‚Äôre fine with 16 GB of GPU memory.\nWhat you get: 1 x NVIDIA V100 GPU with 16 GB of GPU memory. Based on the older NVIDIA Volta architecture. The best performing single-GPU is still the NVIDIA A100 on P4 instance, but you can only get 8 x NVIDIA A100 GPUs on P4. This GPU has a slight performance edge over NVIDIA A10G on G5 instance discussed next, but G5 is far more cost-effective and has more GPU memory.\nBest performance/cost, single-GPU instance on AWS Instance: g5.xlarge\nWhen to use it: When you want high-performance, more GPU memory at lower cost than P3 instance\nWhat you get: 1 x NVIDIA A10G GPU with 24 GB of GPU memory, based on the latest Ampere architecture. NVIDIA A10G can be seen as a lower powered cousin of the A100 on the p4d.24xlarge so it‚Äôs easy to migrate and scale when you need more compute. Consider larger sizes withg5.(2/4/8/16)xlarge for the same single-GPU with more vCPUs and higher system memory if you have more pre or post processing steps.\nBest performance/cost, multi-GPU instance on AWS Instance: p3.(8/16)xlarge\nWhen to use it: Cost-effective multi-GPU model development and training.\nWhat you get: p3.8xlarge has 4 x NVIDIA V100 GPUs and p3.16xlarge has 8 x NVIDIA V100 GPUs with 16 GB of GPU memory on each GPU, based on the older NVIDIA Volta architecture. For larger models, datasets and faster performance consider P4 instances.\nHigh-performance GPU instance at a budget on AWS Instance: g4dn.xlarge\nWhen to use it: Lower performance than other options at lower cost for model development and training. Cost effective model inference deployment.\nWhat you get: 1 x NVIDIA T4 GPU with 16 GB of GPU memory. Based on the previous generation NVIDIA Turing architecture. Consider g4dn.(2/4/8/16)xlarge for more vCPUs and higher system memory if you have more pre or post processing.\nRelated blog posts AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution Choosing the right GPU for deep learning on AWS How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators ","categories":"","description":"In a hurry? Here are the best GPUs for Deep Learining on AWS","excerpt":"In a hurry? Here are the best GPUs for Deep Learining on AWS","ref":"/best-gpus-on-aws-for-deep-learning/","tags":"","title":"Best GPUs on AWS for Deep Learning"},{"body":"\nAn AI accelerator is a dedicated processor designed to accelerate machine learning computations. Machine learning, and particularly its subset, deep learning is primarily composed of a large number of linear algebra computations, (i.e. matrix-matrix, matrix-vector operations) and these operations can be easily parallelized. AI accelerators are specialized hardware designed to accelerate these basic machine learning computations and improve performance, reduce latency and reduce cost of deploying machine learning based applications.\nWhy do we need specialized AI accelerators? The two most important reasons for building dedicated processors for machine learning are:\nEnergy efficiency Faster performance Recent trends to improve model accuracy, have been to introduce larger models with more parameters and train them on larger data sets. As model sizes get larger, and current processors won‚Äôt be able to deliver the processing power needed to train or run inference on these models under tight time-to-train and inference latency requirements.\nGeneral purpose processors like CPUs trade-off energy efficiency for versatility and special purpose processors (AI accelerators) trade off versatility for energy efficiency. AI accelerators on the other hand can be designed with features to minimize memory access, offer larger on-chip cache and include dedicated hardware features to accelerate matrix-matrix computations. Since AI accelerators are purpose built devices it is ‚Äúaware‚Äù of the algorithms that it runs on and its dedicated features will run it more efficiently than a general purpose processor.\nList of popular AI accelerators for training\nNVIDIA GPUs: Available on AWS, GCP, Azure and at your local computer store (See my recommendation list on the left menu) AWS Tranium: Available on AWS Intel Habana Gaudi: Available on AWS (v1) and Intel DevCloud (v1 and v2) Google Cloud TPUs: Available on GCP and via Colab (v1-v4) List of popular AI accelerators for inference\nNVIDIA GPUs: Available on AWS, GCP, Azure (See my recommendation list on the left menu) AWS Inferentia: Available on AWS (See my recommend blog post below) Intel Habana Gaudi: Available on AWS and Intel DevCloud (v1 and v2) Google Cloud TPUs: Available on GCP and via Colab (v1-v4) Note: Modern GPUs have dedicated silicon (TensorCores) and precision types (TF32, BF16) designed for deep learning bringing them closer to dedicated AI accelerators vs. general purpose parallel processors Recommended blog posts AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution A complete guide to AI accelerators for deep learning inference ‚Äî GPUs, AWS Inferentia and Amazon Elastic Inference ","categories":"","description":"An AI accelerator is a dedicated processor designed to accelerate machine learning computations.","excerpt":"An AI accelerator is a dedicated processor designed to accelerate ‚Ä¶","ref":"/what-is-an-ai-accelerator/","tags":"","title":"What is an AI accelerator?"},{"body":" Last updated: Feb 17th 2022 Here is a complete list of all Amazon EC2 GPU instance types on AWS that I‚Äôve painstakenly compiled, because you can‚Äôt find this information anywhere on AWS. In the tabular format below, you‚Äôll find more detailed information about GPU type, interconnect, Thermal design power (TDP), precision types supported etc.\nGraphical format From my blog post: Choosing the right GPU for deep learning on AWS Tabular format With more information than you were probably looking for üòä\nArchitecture NVIDIA GPU Instance type Instance name Number of GPUs GPU Memory (per GPU) GPU Interconnect (NVLink / PCIe) Thermal\nDesign Power (TDP) from nvidia-smi Tensor Cores (mixed-precision) Precision Support CPU Type Nitro based Ampere A100 P4 p4d.24xlarge 8 40 GB NVLink gen 3 (600 GB/s) 400W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 Intel Xeon Scalable (Cascade Lake) Yes Ampere A10G G5 g5.xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.2xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.4xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.8xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.16xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.12xlarge 4 24 GB PCIe 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.24xlarge 4 24 GB PCIe 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.48xlarge 8 24 GB PCIe 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Turing T4G G5 g5g.xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.2xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.4xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.8xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.16xlarge 2 16 GB PCIe 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.metal 2 16 GB PCIe 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4 G4 g4dn.xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.2xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.4xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.8xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.16xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.12xlarge 4 16 GB PCIe 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.metal 8 16 GB PCIe 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Volta V100 P3 p3.2xlarge 1 16 GB NA (single GPU) 300W Tensor Cores (Gen 1) FP64, FP32, FP16 Intel Xeon (Broadwell) No Volta V100 P3 p3.8xlarge 4 16 GB NVLink gen 2 (300 GB/s) 300W Tensor Cores (Gen 1) FP64, FP32, FP16 Intel Xeon (Broadwell) No Volta V100 P3 p3.16xlarge 8 16 GB NVLink gen 2 (300 GB/s) 300W Tensor Cores (Gen 1) FP64, FP32, FP16 Intel Xeon (Broadwell) No Volta V100* P3 p3dn.24xlarge 8 32 GB NVLink gen 2 (300 GB/s) 300W Tensor Cores (Gen 1) FP64, FP32, FP16 Intel Xeon (Skylake) Yes Kepler K80 P2 p2.xlarge 1 12 GB NA (single GPU) 149W No FP64, FP32 Intel Xeon (Broadwell) No Kepler K80 P2 p2.8xlarge 8 12 GB PCIe 149W No FP64, FP32 Intel Xeon (Broadwell) No Kepler K80 P2 p2.16xlarge 16 12 GB PCIe 149W No FP64, FP32 Intel Xeon (Broadwell) No Maxwell M60 G3 g3s.xlarge 1 8 GB PCIe 150W No FP32 Intel Xeon (Broadwell) No Maxwell M60 G3 g3.4xlarge 1 8 GB PCIe 150W No FP32 Intel Xeon (Broadwell) No Maxwell M60 G3 g3.8xlarge 2 8 GB PCIe 150W No FP32 Intel Xeon (Broadwell) No Maxwell M60 G3 g3.16xlarge 4 8 GB PCIe 150W No FP32 Intel Xeon (Broadwell) No ","categories":"","description":"A complete list of all Amazon EC2 GPU instance types on AWS that I've painstakenly compiled, because you can't find this information anywhere in AWS docs","excerpt":"A complete list of all Amazon EC2 GPU instance types on AWS that I've ‚Ä¶","ref":"/complete-list-of-all-aws-gpu-instances/","tags":"","title":"AWS GPU instances complete list"},{"body":"\n","categories":"","description":"","excerpt":"\n","ref":"/quick-guides/","tags":"","title":"Quick guides"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ai/","tags":"","title":"AI"},{"body":"We‚Äôve all dealt with the frustration of poor or incomplete documentation in software projects. In their 2021 state of the Octoverse survey, GitHub found that easy to use documentation, boosted developer productivity by 50% and improved contribution quality, yet it continues to be an under-invested area across open-source projects.\nUsing OpenAI Codex you can use to automatically edit existing code to add documentation using only natural language instructions. This new feature will let you spend more time developing new features and reviewing generated documentation instead of writing documentation from scratch. In addition to generating documentation, Codex‚Äôs edit feature can refactor code, update logic, translate between programming languages and change coding styles.\nIn this blog post, we‚Äôll take a look at how Codex edit feature works through a code example. One of my favorite areas of machine learning (ML) research is automated machine learning (AutoML) or low-code ML. I‚Äôll use Codex to implement a simple data science workflow only using natural language instructions and without writing any ML code. We‚Äôll perform the following steps only using Codex‚Äôs Edit API and discuss its features as we implement the code example:\nGenerate python code to load a dataset and visualize it\nGenerate python code to train a machine learning classifier on the dataset\nEdit generated code to customize it for the specific problem dataset\nEdit updated code to add detailed documentation\nSounds exciting? Let‚Äôs get started!\nUsing OpenAI Codex edit feature to generate fully documented code to train a classifier on a tabular dataset Let‚Äôs use OpenAI Codex to solve the Titanic - Machine Learning from Disaster Kaggle competition. The titanic dataset consists of passenger information like name, age, gender, etc. and if they survived the disaster or not. We‚Äôll use natural language prompts to iteratively build code and documentation. To download the dataset, you‚Äôll need to create a free Kaggle account and download it either via the web page or by following the instructions on the Kaggle API GitHub page.\nNext, you‚Äôll need an Open AI account. If you‚Äôre new to OpenAI, head over to https://openai.com/api/login/ and create your free trial account. This will give you access to $18 in free credit that you can use for upto 3 months.\nAfter you‚Äôve created your account, you‚Äôll have access to your unique API keys that you can use to invoke the OpenAI API using the official Python, Node.js or any of the community supported libraries. Since our example generates Python code, we‚Äôll use the Python bindings.\nStep 1: Setting up OpenAI Python package Open your favorite IDE to get started, I prefer Jupyter Lab as it‚Äôs the most popular IDE used by data scientists. Import openai Python package and specify your organization and API keys. You can find these on your Open AI account.\nimport os import openai openai.organization = \"org-XXXXXXXXXXXXXXX\" openai.api_key = \"sk-XXXXXXXXXXXXXXXX\" Step 2: Generate python code to load an analyze the dataset We can use the Codex edit feature to edit existing code or generate new code. Let‚Äôs start by generating some fresh code to load our Titanic CSV dataset and analyze it.\nHere‚Äôs what the arguments to the create() function mean:\nInput: Provide a prompt as a starting point. We leave it empty since we‚Äôre going to generate new code and not edit existing code.\nInstructions: Provide instructions to Codex on what code to generate. Here we describe that we want to generate code to load and analyze our CSV dataset.\nEngine: Provide the engine for Codex edit feature, the latest name and version is Code-davinci-edit-001\nTemperature and top_p: used to control how deterministic the model is in generating a response. I choose a lower value for top_p (=0.2) to get a consistent response.\nYou can find more information about the API in the Edits documentation.\nfrom IPython.display import display input_prompt = ''' ''' instruction = ''' Write python function to load a CSV file called titanic.csv into a dataframe and use display function''' response = openai.Edit.create(input=input_prompt, instruction=instruction, engine=\"code-davinci-edit-001\", temperature=1, top_p=0.2) Output:\nWhen you run the code, you should see an output that looks like the screenshot below. From the output we can see that the dataset has both numeric and categorical variables. This information will come in handy later when we ask Codex to edit our function. For now, let‚Äôs continue to build on this program to classify the dataset.\nStep 3: Generate a Python program to classify the dataset In this step, rather than leave the input prompt empty, let‚Äôs provide the function signature we expect so Codex can fill out the rest of the program. Under instructions we provide:\n‚ÄúWrite python function to load a CSV file and perform binary XGBoost classification on a target column: Extending our previous example, we‚Äôll ask Codex to generate a program to use XGBoost to classify our dataset based on a specified target variable. If you‚Äôre new to machine learning, XGBoost is a fantastic works-out-of-the-box classifier for tabular datasets that often requires little to no fine tuning to get acceptable results.\ninput_prompt = ''' def csv_classification(csv_file, target_column): ''' instruction = ''' Write python function to load a CSV file and perform binary XGBoost classification on a target column''' response = openai.Edit.create(input=input_prompt, instruction=instruction, engine=\"code-davinci-edit-001\", temperature=1, top_p=0.2) generated_code = response[\"choices\"][0][\"text\"] print(generated_code) Output:\nCodex generates the following code to classify our dataset using XGBoost. If you‚Äôre new to machine learning, let‚Äôs just take a moment to appreciate how much time we saved by having Codex generate this vs. writing this from scratch. We still need to test if this code works, so let‚Äôs go ahead and do that in the next step.\nexec(generated_code) csv_classification('titanic.csv','Survived') Output:\nOh no! the generated code throws an error! Not what we want to see, but let‚Äôs take a closer look at the error message. The error message says:\nWe have categorical variables that need to be specified as categorical variables. If we go back to the output of Step 1, we can verify from that the Name, Sex, Ticket, Cabin and Embarked are indeed categorical.\nSince we have categorical variables, we need to instruct XGBoost to enable categorical support using `enable_categorical` argument.\nTo remedy this, let‚Äôs make Codex do all the hard work by simply copying the error message and including it in the instructions of the next step.\nStep 4: Fixing errors in the generated code by updating the instructions In this step, we start with the generated code from Step 3, and provide instructions to make changes specified in the error message. We provide the following instructions:\n‚ÄúUpdate code to set dataframe columns Name, Sex, Ticket, Cabin, Embarked to categorical and set DMatrix parameter 'enable_categorical' to 'True'‚Äù input_prompt = generated_code instruction = ''' Update code to set dataframe columns Name, Sex, Ticket, Cabin, Embarked to categorical and set DMatrix parameter 'enable_categorical' to 'True' ''' response = openai.Edit.create(input=input_prompt, instruction=instruction, engine=\"code-davinci-edit-001\", temperature=1, top_p=0.2) generated_code = response[\"choices\"][0][\"text\"] print(generated_code) Output:\nThis generates updated code that addresses the categorical variable errors by converting the specified column into categorical and by setting enable_categorical to True when creating the XGBoost‚Äôs native DMatrix data structure. Again let‚Äôs take a moment to appreciate how little we had to do to generate code for a generic tabular data classifier using XGBoost.\nLet‚Äôs go ahead and execute this code to train our classifier:\nexec(generated_code) csv_classification('titanic.csv','Survived') Output:\nSure enough, our code now runs without any issues and trains the classifier to 83% accuracy on the test set. Note that this is a very simple dataset, but the generated code can now be used for a number of tabular classification problems. Need to modify the code to change features? Just repeat the instructions in this step to specify what needs to be updated.\nStep 5: Generate detailed documentation Finally, let‚Äôs use the generated code and add detailed documentation to it using Codex. To do that we provide natural language instructions on how and where to add comments:\n‚ÄúAdd a detailed paragraph at the top of the code describing what the code is doing, and add detailed comments explaining every line of code‚Äù instruction = ''' Add a detailed paragraph at the top of the code describing what the code is doing, and add detailed comments explaining every line of code ''' response = openai.Edit.create(input=generated_code, instruction=instruction, engine=\"code-davinci-edit-001\", temperature=1, top_p=0.2) documented_code = response[\"choices\"][0][\"text\"] print(documented_code) Output:\nHere you can see the detailed description of the full program, you can also see that the additional comments were automatically added under the function definition describing each line of code.\nIf you‚Äôre new to XGBoost, you can also use the generated documentation to learn about what each of the hyperparameters mean! Now you know that eta is actually the learning rate, and max_depth is the maximum depth of each of the 20 trees XGBoost uses. How cool is that?\nWe now have a fully functional and documented code that you can use to submit your results to the Titanic Kaggle competition!\nNow it‚Äôs your turn! If this piqued your interest head on over to https://openai.com/api/login/ and create your free trial account. I‚Äôm making all the code used in this blog post is available as a Jupyter notebook on GitHub so you can run the entire example with a single click\nNow it‚Äôs your turn! How will you use Codex‚Äôs new edit feature? Let me know by reaching out to me at @shshnkp ","categories":"","description":"Use OpenAI CODEX to automatically edit existing code to add documentation, only using only natural language instructions","excerpt":"Use OpenAI CODEX to automatically edit existing code to add ‚Ä¶","ref":"/automatically-generate-code-and-documentation-using-openai-codex/","tags":["openai","codex","AI"],"title":"Automatically generate code and documentation using OpenAI CODEX"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/codex/","tags":"","title":"codex"},{"body":"Shashank Prasanna Hi! üëãüèΩ I‚Äôm a multi-disciplinary engineer, technology communicator and doodler. I speak the languages of math, statistics, machine learning, linear algebra, numerical optimization, non-linear dynamical systems (particularly chaotic ones), high-performance computing (HPC) and specialized machine learning hardware (AI Accelerators). Upcoming talks Updated: 03/07/2023 March 9th 2023: NYU Meetup on What‚Äôs new in PyTorch 2.0 (Link TBD) March 30th 2023: Prinston Univ workshop on PyTorch 2.0 profiling and debugging (Link TBD) üë®üèΩ‚Äçüíª Work I‚Äôm an AI/ML Open-Source and PyTorch Developer Advocate at Meta. I‚Äôve previously worked at Amazon Web Services (AWS), MathWorks (MATLAB \u0026 Simulink) and NVIDIA in various roles including software development and product marketing. I find most joy in education and I put a lot of my energy in content creation and story telling.\nüèÉüèΩNot Work: I‚Äôm a recreational runner and a ‚òïÔ∏è coffee nut.\nüéì Education I have a masters in Electrical Engineering with a specialization in Control Theory and Non-linear Dynamics. My graduate research was advised by Dr. Leon Iasemidis.\nIn my graduate thesis Directional Information Flow and Applications I discussed the application of transfer entropy (a model-free, information theoretic measure) to the detection of epileptogenic focus in the brain (origin of epileptic seizures).\n","categories":"","description":"","excerpt":"Shashank Prasanna Hi! üëãüèΩ I‚Äôm a multi-disciplinary engineer, technology ‚Ä¶","ref":"/","tags":"","title":""},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/openai/","tags":"","title":"openai"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"}]