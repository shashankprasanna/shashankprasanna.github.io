<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog on Shashank Prasanna</title><link>/blog/</link><description>Recent content in Blog on Shashank Prasanna</description><generator>Hugo</generator><language>en</language><lastBuildDate>Fri, 23 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Why I joined Modular AI?</title><link>/why-i-joined-modular-ai/</link><pubDate>Fri, 23 Jun 2023 00:00:00 +0000</pubDate><guid>/why-i-joined-modular-ai/</guid><description>&lt;p&gt;&lt;img src="/why-i-joined-modular-ai/featured-background.png" alt=""&gt;
In the past decade, I&amp;rsquo;ve had the very good fortune of working for companies that make some of the best and proven developer productivity tools. If you&amp;rsquo;re an engineer who&amp;rsquo;s built control systems for rockets, cars or robots you&amp;rsquo;ve used MATLAB and Simulink. If you&amp;rsquo;re an AI developer you likely have an NVIDIA GPU at arm&amp;rsquo;s reach (ssh to AWS EC2 counts as arm&amp;rsquo;s reach). If you&amp;rsquo;re a developer running software in production you&amp;rsquo;ve likely relied on AWS services for their scalability and and reliability for deployment. If you&amp;rsquo;re an AI researcher there is no better tool than PyTorch to go from research paper to trained model.&lt;/p&gt;</description></item><item><title>Benchmarking Modular Mojoüî• and PyTorch torch.compile() on Mandelbrot function</title><link>/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/</link><pubDate>Mon, 08 May 2023 00:00:00 +0000</pubDate><guid>/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/</guid><description>&lt;p&gt;Last week, Modular - an startup co-founded by Chris Lattner (of LLVM, Swift, MLIR fame), announced a brand new high-performance language called Mojoüî•. Mojoüî• looks and reads like Python but that&amp;rsquo;s only on the surface, underneath the familiar Python syntax Mojo uses it&amp;rsquo;s own JIT and AOT compilation process to accelerate Python code. Although Mojo doesn&amp;rsquo;t fully support all of Python today, according to Mojo docs, over time Mojo is expected to become a superset of Python.
&lt;figure class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 410px"&gt;
&lt;img class="card-img-top" src="/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/featured-background_hu_4d82b718f6caf1c3.png" width="400" height="285"&gt;
&lt;figcaption class="card-body px-0 pt-2 pb-0"&gt;
&lt;p class="card-text"&gt;


PyTorch's got some Mojoüî•

&lt;/p&gt;</description></item><item><title>How Pytorch 2.0 accelerates deep learning with operator fusion and CPU/GPU code-generation</title><link>/how-pytorch-2.0-accelerates-deep-learning-with-operator-fusion-and-cpu/gpu-code-generation/</link><pubDate>Sun, 30 Apr 2023 00:00:00 +0000</pubDate><guid>/how-pytorch-2.0-accelerates-deep-learning-with-operator-fusion-and-cpu/gpu-code-generation/</guid><description>&lt;p&gt;Computer programming is magical. We write code in human readable languages, and as though by magic, it gets translated into electric currents through silicon transistors making them behave like switches and allowing them to implement complex logic ‚Äî just so we can enjoy cat videos on the internet. Between the programming language and hardware processors that run it, is an important piece of technology ‚Äî the compiler. A compiler‚Äôs job is to translate and simplify our human readable language code into instructions that a processor understands.&lt;/p&gt;</description></item><item><title>What is an AI accelerator?</title><link>/what-is-an-ai-accelerator/</link><pubDate>Wed, 11 Jan 2023 00:00:00 +0000</pubDate><guid>/what-is-an-ai-accelerator/</guid><description>&lt;!-- ![](ai-hardware-spectrum-featured.png) --&gt;
&lt;img src="ai-hardware-spectrum-featured.png" width="800"/&gt;
&lt;p&gt;An AI accelerator is a dedicated processor designed to accelerate machine learning computations. Machine learning, and particularly its subset, deep learning is primarily composed of a large number of linear algebra computations, (i.e. matrix-matrix, matrix-vector operations) and these operations can be easily parallelized. AI accelerators are specialized hardware designed to accelerate these basic machine learning computations and improve performance, reduce latency and reduce cost of deploying machine learning based applications.&lt;/p&gt;</description></item><item><title>AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution</title><link>/ai-accelerators-and-machine-learning-algorithms-co-design-and-evolution/</link><pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate><guid>/ai-accelerators-and-machine-learning-algorithms-co-design-and-evolution/</guid><description>&lt;p&gt;If you told me a few years ago that data scientists would be using Docker containers in their day to day work, I wouldn‚Äôt have believed you. As a member of the broader machine learning (ML) community I always considered Docker, Kubernetes, Swarm (remember that?) exotic infrastructure tools for IT/Ops experts. Today it‚Äôs a different story, rarely a day goes by when I don‚Äôt use a Docker container for training or hosting a model.
An attribute of machine learning development that makes it different from traditional software development is that it relies on specialized hardware such as GPUs, Habana Gaudi, AWS Inferentia to accelerate training and inference. This makes it challenging to have containerized deployments that are hardware-agnostic, which is one of the key benefits of containers. In this blog post I‚Äôll discuss how Docker and container technologies have evolved to address this challenge. We‚Äôll discuss:&lt;/p&gt;</description></item><item><title>How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators</title><link>/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators/</link><pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate><guid>/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators/</guid><description>&lt;p&gt;If you told me a few years ago that data scientists would be using Docker containers in their day to day work, I wouldn‚Äôt have believed you. As a member of the broader machine learning (ML) community I always considered Docker, Kubernetes, Swarm (remember that?) exotic infrastructure tools for IT/Ops experts. Today it‚Äôs a different story, rarely a day goes by when I don‚Äôt use a Docker container for training or hosting a model.
An attribute of machine learning development that makes it different from traditional software development is that it relies on specialized hardware such as GPUs, Habana Gaudi, AWS Inferentia to accelerate training and inference. This makes it challenging to have containerized deployments that are hardware-agnostic, which is one of the key benefits of containers. In this blog post I‚Äôll discuss how Docker and container technologies have evolved to address this challenge. We‚Äôll discuss:&lt;/p&gt;</description></item><item><title>Automatically generate code and documentation using OpenAI CODEX</title><link>/automatically-generate-code-and-documentation-using-openai-codex/</link><pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate><guid>/automatically-generate-code-and-documentation-using-openai-codex/</guid><description>&lt;p&gt;We&amp;rsquo;ve all dealt with the frustration of poor or incomplete documentation in software projects. In their &lt;a href="https://octoverse.github.com/creating-documentation/"&gt;2021 state of the Octoverse survey&lt;/a&gt;, GitHub found that easy to use documentation, boosted developer productivity by 50% and improved contribution quality, yet it continues to be an under-invested area across open-source projects.&lt;/p&gt;
&lt;p&gt;Using OpenAI Codex you can use to automatically edit existing code to add documentation using only natural language instructions. This new feature will let you spend more time developing new features and reviewing generated documentation instead of writing documentation from scratch. In addition to generating documentation, Codex&amp;rsquo;s edit feature can refactor code, update logic, translate between programming languages and change coding styles.&lt;/p&gt;</description></item><item><title>AWS GPU instances complete list</title><link>/complete-list-of-all-aws-gpu-instances/</link><pubDate>Thu, 02 Jun 2022 00:00:00 +0000</pubDate><guid>/complete-list-of-all-aws-gpu-instances/</guid><description>&lt;p&gt;Here is a complete list of all Amazon EC2 GPU instance types on AWS that I&amp;rsquo;ve painstakenly compiled, because you can&amp;rsquo;t find this information anywhere on AWS.
In the tabular format below, you&amp;rsquo;ll find more detailed information about GPU type, interconnect, Thermal design power (TDP), precision types supported etc.&lt;/p&gt;
&lt;div class="td-card card border me-4"&gt;
&lt;div class="card-body"&gt;
 &lt;h5 class="card-title"&gt;
 &lt;img src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*1qFKgyph2CT2Dxat3tSqmQ.png" alt=""&gt;&lt;/h5&gt;
 &lt;p class="card-text"&gt;
 
&lt;/p&gt;
 &lt;/div&gt;
 &lt;div class="card-footer"&gt;
 From my blog post: &lt;i class='fa-brands fa-medium'&gt;&lt;/i&gt; &lt;a href="https://medium.com/towards-data-science/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86"&gt;Choosing the right GPU for deep learning on AWS&lt;/a&gt;&lt;/div&gt;
 &lt;/div&gt;

&lt;h2 id="tabular-format"&gt;Tabular format&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;With more information than you were probably looking for&lt;/em&gt; üòä&lt;/p&gt;</description></item><item><title>Best GPUs on AWS for Deep Learning</title><link>/best-gpus-on-aws-for-deep-learning/</link><pubDate>Wed, 11 May 2022 00:00:00 +0000</pubDate><guid>/best-gpus-on-aws-for-deep-learning/</guid><description>&lt;p&gt;Here are 5 GPU instance recommendations on AWS that should serve majority of deep learning use-cases. For a complete deep dive into choosing the right GPU for deep learning on AWS, read my blog post:&lt;/p&gt;


&lt;div class="alert alert-primary" role="alert"&gt;


 &lt;h3 id="choosing-the-right-gpu-for-deep-learning-on-aws"&gt;&lt;i class='fa-brands fa-medium'&gt;&lt;/i&gt; &lt;a href="https://medium.com/towards-data-science/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86"&gt;Choosing the right GPU for deep learning on AWS&lt;/a&gt;&lt;/h3&gt;


&lt;/div&gt;

&lt;div class="td-card card border me-4"&gt;
&lt;div class="card-header"&gt;
 &lt;strong&gt;Highest performing multi-GPU instance on AWS&lt;/strong&gt;
 &lt;/div&gt;
&lt;div class="card-body"&gt;
 &lt;p class="card-text"&gt;
&lt;p&gt;&lt;img src="/best-gpus-on-aws-for-deep-learning/gpu1.png" alt="GPU"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instance: p4d.24xlarge&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When to use it:&lt;/strong&gt; When you need all the performance you can get. Use it for distributed training on large models and datasets.&lt;/p&gt;</description></item><item><title>A complete guide to AI accelerators for deep learning inference ‚Äî GPUs, AWS Inferentia and Amazon Elastic Inference</title><link>/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/</guid><description>&lt;h3 id="lets-start-by-answering-the-question-what-is-an-ai-accelerator"&gt;Let‚Äôs start by answering the question ‚ÄúWhat is an AI accelerator?‚Äù&lt;/h3&gt;
&lt;p&gt;An AI accelerator is a dedicated processor designed to accelerate machine learning computations. Machine learning, and particularly its subset, deep learning is primarily composed of a large number of linear algebra computations, (i.e. matrix-matrix, matrix-vector operations) and these operations can be easily parallelized. AI accelerators are specialized hardware designed to accelerate these basic machine learning computations and improve performance, reduce latency and reduce cost of deploying machine learning based applications.&lt;/p&gt;</description></item><item><title>Choosing the right GPU for deep learning on AWS</title><link>/choosing-the-right-gpu-for-deep-learning-on-aws/</link><pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate><guid>/choosing-the-right-gpu-for-deep-learning-on-aws/</guid><description>&lt;p&gt;On AWS, you can launch GPU instances with different GPU memory sizes (8 GB, 16 GB, 24 GB, 32 GB, 40 GB), NVIDIA GPU generations (Ampere, Turing, Volta, Maxwell, Kepler) different capabilities (FP64, FP32, FP16, INT8, Sparsity, TensorCores, NVLink), different number of GPUs per instance (1, 2, 4, 8, 16), and paired with different CPUs (Intel, AMD, Graviton2). You can also select instances with different vCPUs (core thread count), system memory and network bandwidth and add a range of storage options (object storage, network file systems, block storage, etc.) ‚Äî in summary, you have options.&lt;/p&gt;</description></item></channel></rss>