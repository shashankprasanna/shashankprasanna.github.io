[{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/","title":"A Tour of PyTorch 2.0","tags":[],"description":"","content":"A Tour of PyTorch 2.0 Abstract: Learn about PyTorch 2.0 with a deep dive into the technology stack that powers the new torch.compile() API: TorchDynamo, AITAutograd, and TorchInductor. The new compiler stack reduces training times across a wide range of workloads while being fully backwards compatible. Bring your laptops, or connect to remote GPU powered systems and learn how to compile your code, profile and debug to improve performance.\nLearning objectives At the end of this tutorial you\u0026rsquo;ll be able to:\nDescribe the advantages of using torch.compile() Demonstrate how to use torch.compile() to speed up PyTorch Explain what happens under the hood when you call torch.compile() List different Intermediate Representations (IR) in Pytorch Recognize the importance of operator fusion and code generation Agenda Topics Duration Setup and getting started 10 mins What\u0026rsquo;s New in PyTorch 2.0 10 mins Hand\u0026rsquo;s On: Simple benchmarks with torch.compile() in PyTorch 2.0 20 mins Hand\u0026rsquo;s On: What happens under the hood when you call torch.compile() ? 15 mins Hand\u0026rsquo;s On: Profiling pytorch code with torch.profiler and inspecting the compiler process 15 mins Resources + Q\u0026amp;A 10 mins "},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/1_workshop_setup/","title":"Workshop setup","tags":[],"description":"","content":"How to use this website System requirements To run this workshop, you\u0026rsquo;ll need access to a computer, preferably with a GPU and PyTorch 2.0 installed. In this section you\u0026rsquo;ll find instructions to run this content on local, free and paid cloud options\n"},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_getting_started/a_simple_example/","title":"A simple example","tags":[],"description":"","content":"How to use torch.compile() : A simple example Load necessary modules, we\u0026rsquo;ll discuss the strange torch._dynamo bit a little later.\nimport torch import torch.nn as nn from torchvision.models import resnet import torch._dynamo device = torch.device(\u0026#34;cuda\u0026#34;) if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; Define a very simple NN and compile it with torch.compile() and compare the results with your model to convince yourself that it won\u0026rsquo;t break your code.\nclass MLP(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(32, 64) def forward(self, x): x = self.fc1(x) x = torch.nn.functional.relu(x) return x model = MLP() input = torch.randn(8, 32) torch._dynamo.reset() # Only needed if you call this cell repeatedly compiled_model = torch.compile(model) # Alternatively you can also pass the backend compiled_model = torch.compile(model, backend=\u0026#39;inductor\u0026#39;) output = model(input) # triggers compilation of forward graph on the first run output_compiled = compiled_model(input) torch.all(output == output_compiled) Here is the output, congratulations! you ran you first torch.compile() compilation task. Now that you\u0026rsquo;re convinced that the compilation doesn\u0026rsquo;t change the output, let\u0026rsquo;s proceed with benchmarking!\ntensor(True) "},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/4_inspecting_torch_compile/inspecting_torch_compile/","title":"Inspecting torch.compile()","tags":[],"description":"","content":"In this section we\u0026rsquo;ll define a very simple function and analyze what\u0026rsquo;s happening under the hood.\nimport torch import math import os import matplotlib.pyplot as plt from torch import optim import torch._dynamo from torchvision import models from torch.profiler import profile, record_function, ProfilerActivity pi = math.pi device = torch.device(\u0026#34;cuda\u0026#34;) if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; Define a simple function sin^2(x) + cos^2(x), as you know this equals 1 for all real values of x\ndef fn(x): return torch.sin(x)**2 + torch.cos(x)**2 Now let\u0026rsquo;s define a 1million x 1 tensor to pass to our function\ntorch.manual_seed(0) x = torch.rand(1000000, requires_grad=True).to(device) out = fn(x) torch.linalg.norm(out-1) \u0026lt;= 1e-4 Sure enough you should see the following output:\ntensor(True, device='cuda:0') Now let\u0026rsquo;s write a simple function to intercept what the compiler sees. To do that we create a fake compiler and pass it to torch.compile(). We do this to take a look at the next intermediate code representation from our high level code\ntorch.manual_seed(0) x = torch.rand(1000000, requires_grad=True).to(device) def inspect_backend(gm, sample_inputs): gm.print_readable() return gm.forward torch._dynamo.reset() compiled_model = torch.compile(fn, backend=inspect_backend) out = compiled_model(x) Otuput: import torch._dynamo from torch.fx.passes.graph_drawer import FxGraphDrawer from torch._functorch.aot_autograd import aot_module_simplified def inspect_backend(gm, sample_inputs): def fw(gm, sample_inputs): gm.print_readable() g = FxGraphDrawer(gm, \u0026#39;fn\u0026#39;) with open(\u0026#34;forward.svg\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(g.get_dot_graph().create_svg()) return gm.forward def bw(gm, sample_inputs): gm.print_readable() g = FxGraphDrawer(gm, \u0026#39;fn\u0026#39;) with open(\u0026#34;backward.svg\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(g.get_dot_graph().create_svg()) return gm.forward # Invoke AOTAutograd return aot_module_simplified( gm, sample_inputs, fw_compiler=fw, bw_compiler=bw ) torch._dynamo.reset() compiled_model = torch.compile(fn, backend=inspect_backend) out = compiled_model(x).sum().backward() Output:\ntorch._dynamo.reset() compiled_model = torch.compile(fn, backend=\u0026#39;inductor\u0026#39;, options={\u0026#39;trace.enabled\u0026#39;:True, \u0026#39;trace.graph_diagram\u0026#39;:True}) out = compiled_model(x).sum().backward() "},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/5_profiling-torch_compile/profiling_torch_compile/","title":"Profiling torch.compile()","tags":[],"description":"","content":"For this example, we\u0026rsquo;ll define yet another simple function. We\u0026rsquo;ll use the taylor series expansion of sin(x). We choose this in particular for 2 reasons.\nTo show torch.compile can generate C++/CUDA code for a variety of functions not just neural network models You can grow this function to be arbtrarily large by simply adding more terms to the series expansion import torch import math import os import matplotlib.pyplot as plt from torch import optim import torch._dynamo from torchvision import models from torch.profiler import profile, record_function, ProfilerActivity pi = math.pi device = torch.device(\u0026#34;cuda\u0026#34;) if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; Define our sin(x) taylor series expansion\ndef sin_taylor(x,n,device): sinx=0. factorial = lambda d: torch.lgamma(torch.tensor(d+1,device=device)).exp() for i in range(n): sinx += ((-1.)**i)*(x**(1+2*i))/factorial(2*i+1) return sinx Visualize the output\nimport matplotlib.pyplot as plt fig, ax = plt.subplots(3,3,figsize=(8,7)) rads = torch.linspace(0,2*pi,100) ax=[b for a in ax for b in a] counter=0 for i in range(1,10): sinx = lambda x: sin_taylor(x,i,device) ax[counter].plot(rads,sinx(rads.to(device)).cpu(),label=i) ax[counter].set_title(f\u0026#39;sin(x) Taylor series terms: {i}\u0026#39;) ax[counter].set_xlabel(\u0026#39;radians (0-2*pi)\u0026#39;) ax[counter].set_ylabel(\u0026#39;sin(x)\u0026#39;) ax[counter].set_ylim([-1.2, 1.2]) ax[counter].set_xlim([0,7]) ax[counter].title.set_fontsize(8) ax[counter].grid() counter+=1 fig.tight_layout() Output: You can see that the wave starts to resemble a sin wave after adding 9 terms.\ntorch.manual_seed(0) x = torch.rand(10000000, requires_grad=True).to(device) n=10 model = lambda x: sin_taylor(x,n,device) %time out = model(x).sum().backward() Compile model with debug mode enabled\ntorch._dynamo.reset() compiled_model = torch.compile(model,options={\u0026#39;trace.enabled\u0026#39;:True, \u0026#39;trace.graph_diagram\u0026#39;:True}) out = compiled_model(x).sum().backward() Profile uncompiled code! from torch.profiler import profile, record_function, ProfilerActivity with profile(activities=[ProfilerActivity.CUDA,ProfilerActivity.CPU]) as prof: out = model(x).sum().backward() print(prof.key_averages(group_by_stack_n=5).table(sort_by=\u0026#34;self_cuda_time_total\u0026#34;, row_limit=5)) prof.export_chrome_trace(\u0026#34;no_compile_trace.json\u0026#34;) ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg Self CUDA Self CUDA % CUDA total CUDA time avg # of Calls ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ cudaLaunchKernel 1.35% 450.000us 1.35% 450.000us 4.091us 12.766ms 49.45% 12.766ms 116.055us 110 aten::copy_ 0.23% 78.000us 72.46% 24.196ms 2.016ms 11.268ms 43.65% 11.404ms 950.333us 12 Memcpy DtoH (Device -\u0026gt; Pageable) 0.00% 0.000us 0.00% 0.000us 0.000us 11.117ms 43.06% 11.117ms 11.117ms 1 aten::mul 0.91% 303.000us 1.36% 455.000us 11.375us 6.036ms 23.38% 6.378ms 159.450us 40 void at::native::vectorized_elementwise_kernel\u0026lt;4, at... 0.00% 0.000us 0.00% 0.000us 0.000us 4.067ms 15.75% 4.067ms 135.567us 30 ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 33.393ms Self CUDA time total: 25.816ms Profile compiled code! torch._dynamo.reset() compiled_model = torch.compile(model,options={\u0026#39;trace.enabled\u0026#39;:True, \u0026#39;trace.graph_diagram\u0026#39;:True}) out = compiled_model(x) [2023-03-30 18:28:24,754] torch._inductor.debug: [WARNING] model__1_forward_4 debug trace: /pytorch-examples/pytorch2-workshop-examples/torch_compile_debug/run_2023_03_30_18_28_23_632206-pid_5849/aot_torchinductor/model__1_forward_4.2 Writing FX graph to file: /pytorch-examples/pytorch2-workshop-examples/torch_compile_debug/run_2023_03_30_18_28_23_632206-pid_5849/aot_torchinductor/model__1_forward_4.2/graph_diagram.svg from torch.profiler import profile, record_function, ProfilerActivity with profile(activities=[ProfilerActivity.CUDA, ProfilerActivity.CPU], ) as prof: out = compiled_model(x) print(prof.key_averages(group_by_stack_n=5).table(sort_by=\u0026#34;self_cuda_time_total\u0026#34;, row_limit=-1)) prof.export_chrome_trace(\u0026#34;compiled_trace.json\u0026#34;) ------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg Self CUDA Self CUDA % CUDA total CUDA time avg # of Calls ------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ triton__0d1d2d 0.00% 0.000us 0.00% 0.000us 0.000us 139.000us 100.00% 139.000us 139.000us 1 cudaDeviceSynchronize 100.00% 7.000us 100.00% 7.000us 7.000us 0.000us 0.00% 0.000us 0.000us 1 ------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 7.000us Self CUDA time total: 139.000us STAGE:2023-03-30 18:28:24 5849:5849 ActivityProfilerController.cpp:311] Completed Stage: Warm Up STAGE:2023-03-30 18:28:24 5849:5849 ActivityProfilerController.cpp:317] Completed Stage: Collection STAGE:2023-03-30 18:28:24 5849:5849 ActivityProfilerController.cpp:321] Completed Stage: Post Processing Tracing output\n"},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/2_whats_new/","title":"What&#39;s New in PyTorch 2.0?","tags":[],"description":"","content":" Sit back, relax and watch the presentation 😊 Slide content is below for reference.\nAdditional resources: PyTorch 2.0 blog for what\u0026rsquo;s new: https://pytorch.org/blog/\n"},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_getting_started/benchmarking_resnet/","title":"Benchmarking Resnet","tags":[],"description":"","content":"Benchmark a Resnet18 model Now we\u0026rsquo;ll add some additional code to benchmark the training and inference workflows with random data. We\u0026rsquo;ll use the default settings with torch.compile() and then introduce different modes of operation: torch.compile(...,mode='reduce-overhead') and torch.compile(...,mode='max-autotune')\nPerformance depends on various factors such as your model type, system configuration and GPU type You may not always see a speedup. In the subsequent sections we\u0026rsquo;ll see how to profile and identify potential issues.\nTo benchmark our models we\u0026rsquo;ll use torch.utils.benchmark\nimport torch.utils.benchmark as benchmark def run_batch_inference(model, batch=1): x = torch.randn(batch, 3, 224, 224).to(device) model(x) def run_batch_train(model, optimizer, batch=16): x = torch.randn(batch, 3, 224, 224).to(device) optimizer.zero_grad() out = model(x) out.sum().backward() optimizer.step() model = resnet.resnet18(weights=resnet.ResNet18_Weights.IMAGENET1K_V1).to(device) batch = 16 torch._dynamo.reset() compiled_model = torch.compile(model) optimizer = torch.optim.SGD(model.parameters(), lr=0.01) t_model = benchmark.Timer( stmt=\u0026#39;run_batch_train(model, optimizer, batch)\u0026#39;, setup=\u0026#39;from __main__ import run_batch_train\u0026#39;, globals={\u0026#39;model\u0026#39;: model,\u0026#39;optimizer\u0026#39;:optimizer, \u0026#39;batch\u0026#39;:batch}) t_compiled_model = benchmark.Timer( stmt=\u0026#39;run_batch_train(model, optimizer, batch)\u0026#39;, setup=\u0026#39;from __main__ import run_batch_train\u0026#39;, globals={\u0026#39;model\u0026#39;: compiled_model, \u0026#39;optimizer\u0026#39;:optimizer, \u0026#39;batch\u0026#39;:batch}) t_model_runs = t_model.timeit(100) t_compiled_model_runs = t_compiled_model.timeit(100) print(t_model_runs) print(t_compiled_model_runs) print(f\u0026#34;\\nResnet18 Training speedup: {100*(t_model_runs.mean - t_compiled_model_runs.mean) / t_model_runs.mean: .2f}%\u0026#34;) What do you see? Your speed up will depend on the hardware you have:\nResnet18 Training speedup: X.XX% Now let\u0026rsquo;s benchmark inference only, and we\u0026rsquo;ll use torch.compile() with a specific mode:'reduce-overhead'. This mode reduces CPU overhead by using CUDA graphs.\nbatch = 1 torch._dynamo.reset() compiled_model = torch.compile(model, mode=\u0026#39;reduce-overhead\u0026#39;) t_model = benchmark.Timer( stmt=\u0026#39;run_batch_inference(model, batch)\u0026#39;, setup=\u0026#39;from __main__ import run_batch_inference\u0026#39;, globals={\u0026#39;model\u0026#39;: model, \u0026#39;batch\u0026#39;:batch}) t_compiled_model = benchmark.Timer( stmt=\u0026#39;run_batch_inference(model, batch)\u0026#39;, setup=\u0026#39;from __main__ import run_batch_inference\u0026#39;, globals={\u0026#39;model\u0026#39;: compiled_model, \u0026#39;batch\u0026#39;:batch}) t_model_runs = t_model.timeit(100) t_compiled_model_runs = t_compiled_model.timeit(100) print(f\u0026#34;\\nResnet18 Inference speedup: {100*(t_model_runs.mean - t_compiled_model_runs.mean) / t_model_runs.mean: .2f}%\u0026#34;) What do you see?\nResnet18 Inference speedup: XX.XX% On my system with an NVIDIA Titan V I see about 25%-30% speedup.\nPerformance depends on your sytem configuration, GPU type. You may not always see a speedup!\n"},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_getting_started/","title":"Using torch.compile()","tags":[],"description":"","content":"In this section we\u0026rsquo;ll run benchmark common models and check performance improvements with torch.compile() Run all the subsection 3.1, 3.1, \u0026amp; 3.3 on the same Jupyter notebook so you don\u0026rsquo;t have to import multiple times.\nInstructions Open up a new Jupyter Notebook Copy code in the code cells in the instructions and paste it in your Jupyter Notebook cells Run the cell Discuss output with the instructor "},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_getting_started/hugging_face/","title":"Benchmarking Huggingface","tags":[],"description":"","content":"Let\u0026rsquo;s try a language mode with HuggingFace\nimport torch from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC from datasets import load_dataset torch.set_float32_matmul_precision(\u0026#39;high\u0026#39;) def run_hf_inference(model, input_values): # retrieve logits logits = model(input_values).logits # take argmax and decode predicted_ids = torch.argmax(logits, dim=-1) transcription = processor.batch_decode(predicted_ids) # load model and processor processor = Wav2Vec2Processor.from_pretrained(\u0026#34;facebook/wav2vec2-large-960h-lv60-self\u0026#34;) model = Wav2Vec2ForCTC.from_pretrained(\u0026#34;facebook/wav2vec2-large-960h-lv60-self\u0026#34;).cuda() # load dummy dataset and read soundfiles ds = load_dataset(\u0026#34;patrickvonplaten/librispeech_asr_dummy\u0026#34;, \u0026#34;clean\u0026#34;, split=\u0026#34;validation\u0026#34;) # tokenize input_values = processor(ds[0][\u0026#34;audio\u0026#34;][\u0026#34;array\u0026#34;], return_tensors=\u0026#34;pt\u0026#34;, padding=\u0026#34;longest\u0026#34;).input_values.cuda() batch = 1 torch._dynamo.reset() compiled_model = torch.compile(model, mode=\u0026#39;max-autotune\u0026#39;) t_model = benchmark.Timer( stmt=\u0026#39;run_hf_inference(model, input_values)\u0026#39;, setup=\u0026#39;from __main__ import run_hf_inference\u0026#39;, globals={\u0026#39;model\u0026#39;: model, \u0026#39;input_values\u0026#39;:input_values}) t_compiled_model = benchmark.Timer( stmt=\u0026#39;run_hf_inference(model, input_values)\u0026#39;, setup=\u0026#39;from __main__ import run_hf_inference\u0026#39;, globals={\u0026#39;model\u0026#39;: compiled_model, \u0026#39;input_values\u0026#39;:input_values}) t_model_runs = t_model.timeit(100) t_compiled_model_runs = t_compiled_model.timeit(100) print(f\u0026#34;\\nHuggingface Inference speedup: {100*(t_model_runs.mean - t_compiled_model_runs.mean) / t_model_runs.mean: .2f}%\u0026#34;) Output: You should see an output like this showing auto-tune in action where the compiler is running multiple CUDA kernels to determine the most performant one.\nYou\u0026rsquo;ll be trading off slower compile time for potentially faster training time.\nWhat speedup do you see? Discuss!\nHuggingface Inference speedup: X.XX% "},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/4_inspecting_torch_compile/","title":"What happens under the hood?","tags":[],"description":"","content":"In this section we\u0026rsquo;ll take a look at what\u0026rsquo;s happening under the hood We\u0026rsquo;ll discuss\nHow to enable debug mode Visualize computational graphs before and after optimizations See generated optimized C++/CUDA code "},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/5_profiling-torch_compile/","title":"Profiling your compiled code","tags":[],"description":"","content":"In this section we will profile your uncompiled and compiled code using torch.profiler and visualize stack trace Instructions\nCreate a new Jupter notebook Copy each cell and run the code Follow the instructions to visualize stack trace "},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/1_workshop_setup/free_options/","title":"Free CPU and GPU options","tags":[],"description":"","content":"Install locally If you have a CPU-only or a CPU+GPU laptop or workstation you can install PyTorch 2.0 locally by following these instructions:\nhttps://pytorch.org/get-started/locally/\nGoogle Colab Google colab is a free cloud hosted Jupyter based environment. You can get access to GPUs and TPUs to run PyTorch 2.0. Log into Google colab with your google account and open up a new Jupyter Notebook.\n!pip3 install --upgrade --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 Test PyTorch version with:\nimport torch torch.__version__ Amazon SageMaker Studio Lab Amazon SageMaker Studio Lab offers free cloud hosted Jupter lab based environment with CPU and GPU options. Follow these instructions to setup your custom conda environment with PyTorch 2.0:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/studio-lab-use-manage.html#studio-lab-use-manage-conda\nIt usually takes 1 week to get your Studio Lab account approved after you request it. If you need something right away, this may not be the best options for you.\n"},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/tags/","title":"Tags","tags":[],"description":"","content":""}]