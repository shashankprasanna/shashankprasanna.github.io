[{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/","title":"A Tour of PyTorch 2.0","tags":[],"description":"","content":"A Tour of PyTorch 2.0 Approx. duration: 1.5-2 hours\nAbstract: Learn about PyTorch 2.0 with a deep dive into the technology stack that powers the new torch.compile() API: TorchDynamo, AITAutograd, and TorchInductor. The new compiler stack reduces training times across a wide range of workloads while being fully backwards compatible. Bring your laptops, or connect to remote GPU powered systems and learn how to compile your code, profile and debug to improve performance.\nAgenda Topics Duration Setup and getting started 10 mins What\u0026rsquo;s New in PyTorch 2.0 10 mins Hand\u0026rsquo;s On: Simple benchmarks with torch.compile() in PyTorch 2.0 20 mins Hand\u0026rsquo;s On: What happens under the hood? Visualizing IR and computational graphs with trace.enabled 15 mins Hand\u0026rsquo;s On: Profiling pytorch code with torch.profiler and inspecting the compiler process 15 mins Resources + Q\u0026amp;A 10 mins "},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/1_getting_started/","title":"Getting Started","tags":[],"description":"","content":"System requirements To run this workshop, you\u0026rsquo;ll need access to a computer with preferably a GPU and PyTorch 2.0 installed.\nFollowing these instructions to install PyTorch 2.0\nhttps://pytorch.org/get-started/locally/\nThe code for the workshop is also hosted on\nhttps://github.com/shashankprasanna/pytorch2-workshop-examples.git\n"},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/2_whats_new/","title":"What&#39;s New in PyTorch 2.0","tags":[],"description":"","content":" Sit back, relax and watch the presentation :)\n"},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_simple_benchmarks/","title":"Benchmarking torch.compile()","tags":[],"description":"","content":"Benchmarking torch.compile() import torch.utils.benchmark as benchmark import torch import torch.nn as nn from torchvision.models import resnet import torch._dynamo device = torch.device(\u0026#34;cuda\u0026#34;) if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; How to use torch.compile() class MLP(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(32, 64) def forward(self, x): x = self.fc1(x) x = torch.nn.functional.relu(x) return x model = MLP() input = torch.randn(8, 32) torch._dynamo.reset() # Only needed if you call this cell repeatedly compiled_model = torch.compile(model) # Alternatively you can also pass the backend compiled_model = torch.compile(model, backend=\u0026#39;inductor\u0026#39;) output = model(input) # triggers compilation of forward graph on the first run output_compiled = compiled_model(input) torch.all(output == output_compiled) Output:\ntensor(True) Benchmark Resnet18 def run_batch_inference(model, batch=1): x = torch.randn(batch, 3, 224, 224).to(device) model(x) def run_batch_train(model, optimizer, batch=16): x = torch.randn(batch, 3, 224, 224).to(device) optimizer.zero_grad() out = model(x) out.sum().backward() optimizer.step() model = resnet.resnet18(weights=resnet.ResNet18_Weights.IMAGENET1K_V1).to(device) batch = 16 torch._dynamo.reset() compiled_model = torch.compile(model) optimizer = torch.optim.SGD(model.parameters(), lr=0.01) t_model = benchmark.Timer( stmt=\u0026#39;run_batch_train(model, optimizer, batch)\u0026#39;, setup=\u0026#39;from __main__ import run_batch_train\u0026#39;, globals={\u0026#39;model\u0026#39;: model,\u0026#39;optimizer\u0026#39;:optimizer, \u0026#39;batch\u0026#39;:batch}) t_compiled_model = benchmark.Timer( stmt=\u0026#39;run_batch_train(model, optimizer, batch)\u0026#39;, setup=\u0026#39;from __main__ import run_batch_train\u0026#39;, globals={\u0026#39;model\u0026#39;: compiled_model, \u0026#39;optimizer\u0026#39;:optimizer, \u0026#39;batch\u0026#39;:batch}) t_model_runs = t_model.timeit(100) t_compiled_model_runs = t_compiled_model.timeit(100) print(t_model_runs) print(t_compiled_model_runs) print(f\u0026#34;\\nResnet18 Training speedup: {100*(t_model_runs.mean - t_compiled_model_runs.mean) / t_model_runs.mean: .2f}%\u0026#34;) Performance depends on your sytem configuration and GPU type. You may not always see a speedup!\nOutput:\nResnet18 Training speedup: 4.78% batch = 1 torch._dynamo.reset() compiled_model = torch.compile(model, mode=\u0026#39;reduce-overhead\u0026#39;) t_model = benchmark.Timer( stmt=\u0026#39;run_batch_inference(model, batch)\u0026#39;, setup=\u0026#39;from __main__ import run_batch_inference\u0026#39;, globals={\u0026#39;model\u0026#39;: model, \u0026#39;batch\u0026#39;:batch}) t_compiled_model = benchmark.Timer( stmt=\u0026#39;run_batch_inference(model, batch)\u0026#39;, setup=\u0026#39;from __main__ import run_batch_inference\u0026#39;, globals={\u0026#39;model\u0026#39;: compiled_model, \u0026#39;batch\u0026#39;:batch}) t_model_runs = t_model.timeit(100) t_compiled_model_runs = t_compiled_model.timeit(100) print(f\u0026#34;\\nResnet18 Inference speedup: {100*(t_model_runs.mean - t_compiled_model_runs.mean) / t_model_runs.mean: .2f}%\u0026#34;) Performance depends on your sytem configuration and GPU type. You may not always see a speedup!\nResnet18 Inference speedup: 31.43% Huggingface from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC from datasets import load_dataset def run_hf_inference(model, input_values): # retrieve logits logits = model(input_values).logits # take argmax and decode predicted_ids = torch.argmax(logits, dim=-1) transcription = processor.batch_decode(predicted_ids) # load model and processor processor = Wav2Vec2Processor.from_pretrained(\u0026#34;facebook/wav2vec2-large-960h-lv60-self\u0026#34;) model = Wav2Vec2ForCTC.from_pretrained(\u0026#34;facebook/wav2vec2-large-960h-lv60-self\u0026#34;).cuda() # load dummy dataset and read soundfiles ds = load_dataset(\u0026#34;patrickvonplaten/librispeech_asr_dummy\u0026#34;, \u0026#34;clean\u0026#34;, split=\u0026#34;validation\u0026#34;) # tokenize input_values = processor(ds[0][\u0026#34;audio\u0026#34;][\u0026#34;array\u0026#34;], return_tensors=\u0026#34;pt\u0026#34;, padding=\u0026#34;longest\u0026#34;).input_values.cuda() Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Found cached dataset librispeech_asr_dummy (/root/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc) It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug. batch = 1 torch._dynamo.reset() compiled_model = torch.compile(model, mode=\u0026#39;max-autotune\u0026#39;) t_model = benchmark.Timer( stmt=\u0026#39;run_hf_inference(model, input_values)\u0026#39;, setup=\u0026#39;from __main__ import run_hf_inference\u0026#39;, globals={\u0026#39;model\u0026#39;: model, \u0026#39;input_values\u0026#39;:input_values}) t_compiled_model = benchmark.Timer( stmt=\u0026#39;run_hf_inference(model, input_values)\u0026#39;, setup=\u0026#39;from __main__ import run_hf_inference\u0026#39;, globals={\u0026#39;model\u0026#39;: compiled_model, \u0026#39;input_values\u0026#39;:input_values}) t_model_runs = t_model.timeit(100) t_compiled_model_runs = t_compiled_model.timeit(100) print(f\u0026#34;\\nHuggingface Inference speedup: {100*(t_model_runs.mean - t_compiled_model_runs.mean) / t_model_runs.mean: .2f}%\u0026#34;) Huggingface Inference speedup: 4.27% "},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/4_inspecting_torch_compile/","title":"What happens under the hood?","tags":[],"description":"","content":"import torch import math import os import matplotlib.pyplot as plt from torch import optim import torch._dynamo from torchvision import models from torch.profiler import profile, record_function, ProfilerActivity pi = math.pi device = torch.device(\u0026#34;cuda\u0026#34;) if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; def fn(x): return torch.sin(x)**2 + torch.cos(x)**2 torch.manual_seed(0) x = torch.rand(1000000, requires_grad=True).to(device) out = fn(x) torch.linalg.norm(out-1) torch.manual_seed(0) x = torch.rand(1000000, requires_grad=True).to(device) def inspect_backend(gm, sample_inputs): gm.print_readable() return gm.forward torch._dynamo.reset() compiled_model = torch.compile(fn, backend=inspect_backend) out = compiled_model(x) import torch._dynamo from torch.fx.passes.graph_drawer import FxGraphDrawer from torch._functorch.aot_autograd import aot_module_simplified def inspect_backend(gm, sample_inputs): def fw(gm, sample_inputs): gm.print_readable() g = FxGraphDrawer(gm, \u0026#39;fn\u0026#39;) with open(\u0026#34;forward.svg\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(g.get_dot_graph().create_svg()) return gm.forward def bw(gm, sample_inputs): gm.print_readable() g = FxGraphDrawer(gm, \u0026#39;fn\u0026#39;) with open(\u0026#34;backward.svg\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(g.get_dot_graph().create_svg()) return gm.forward # Invoke AOTAutograd return aot_module_simplified( gm, sample_inputs, fw_compiler=fw, bw_compiler=bw ) torch._dynamo.reset() compiled_model = torch.compile(fn, backend=inspect_backend) out = compiled_model(x).sum().backward() compiled_model = torch.compile(fn, backend=\u0026#39;inductor\u0026#39;, options={\u0026#39;trace.enabled\u0026#39;:True, \u0026#39;trace.graph_diagram\u0026#39;:True}) out = compiled_model(x).sum().backward() "},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/5_profiling-torch_compile/","title":"Profiling your compiled code","tags":[],"description":"","content":"import torch import math import os import matplotlib.pyplot as plt from torch import optim import torch._dynamo from torchvision import models from torch.profiler import profile, record_function, ProfilerActivity pi = math.pi device = torch.device(\u0026#34;cuda\u0026#34;) if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; def sin_taylor(x,n,device): sinx=0. factorial = lambda d: torch.lgamma(torch.tensor(d+1,device=device)).exp() for i in range(n): sinx += ((-1.)**i)*(x**(1+2*i))/factorial(2*i+1) return sinx import matplotlib.pyplot as plt fig, ax = plt.subplots(3,3,figsize=(8,7)) rads = torch.linspace(0,2*pi,100) ax=[b for a in ax for b in a] counter=0 for i in range(1,10): sinx = lambda x: sin_taylor(x,i,device) ax[counter].plot(rads,sinx(rads.to(device)).cpu(),label=i) ax[counter].set_title(f\u0026#39;sin(x) Taylor series terms: {i}\u0026#39;) ax[counter].set_xlabel(\u0026#39;radians (0-2*pi)\u0026#39;) ax[counter].set_ylabel(\u0026#39;sin(x)\u0026#39;) ax[counter].set_ylim([-1.2, 1.2]) ax[counter].set_xlim([0,7]) ax[counter].title.set_fontsize(8) ax[counter].grid() counter+=1 fig.tight_layout() torch.manual_seed(0) x = torch.rand(10000000, requires_grad=True).to(device) n=10 model = lambda x: sin_taylor(x,n,device) %time out = model(x).sum().backward() CPU times: user 27.4 ms, sys: 9.07 ms, total: 36.5 ms Wall time: 33.8 ms torch._dynamo.reset() compiled_model = torch.compile(model,options={\u0026#39;trace.enabled\u0026#39;:True, \u0026#39;trace.graph_diagram\u0026#39;:True}) out = compiled_model(x).sum().backward() [2023-03-30 18:28:23,779] torch._inductor.debug: [WARNING] model__0_forward_1 debug trace: /pytorch-examples/pytorch2-workshop-examples/torch_compile_debug/run_2023_03_30_18_28_23_632206-pid_5849/aot_torchinductor/model__0_forward_1.0 Writing FX graph to file: /pytorch-examples/pytorch2-workshop-examples/torch_compile_debug/run_2023_03_30_18_28_23_632206-pid_5849/aot_torchinductor/model__0_forward_1.0/graph_diagram.svg [2023-03-30 18:28:24,016] torch._inductor.debug: [WARNING] model__0_backward_2 debug trace: /pytorch-examples/pytorch2-workshop-examples/torch_compile_debug/run_2023_03_30_18_28_23_632206-pid_5849/aot_torchinductor/model__0_backward_2.1 Writing FX graph to file: /pytorch-examples/pytorch2-workshop-examples/torch_compile_debug/run_2023_03_30_18_28_23_632206-pid_5849/aot_torchinductor/model__0_backward_2.1/graph_diagram.svg %time out = compiled_model(x).sum().backward() CPU times: user 35.6 ms, sys: 6.84 ms, total: 42.5 ms Wall time: 16.6 ms from torch.profiler import profile, record_function, ProfilerActivity with profile(activities=[ProfilerActivity.CUDA,ProfilerActivity.CPU]) as prof: out = model(x).sum().backward() print(prof.key_averages(group_by_stack_n=5).table(sort_by=\u0026#34;self_cuda_time_total\u0026#34;, row_limit=5)) prof.export_chrome_trace(\u0026#34;no_compile_trace.json\u0026#34;) ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg Self CUDA Self CUDA % CUDA total CUDA time avg # of Calls ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ cudaLaunchKernel 1.35% 450.000us 1.35% 450.000us 4.091us 12.766ms 49.45% 12.766ms 116.055us 110 aten::copy_ 0.23% 78.000us 72.46% 24.196ms 2.016ms 11.268ms 43.65% 11.404ms 950.333us 12 Memcpy DtoH (Device -\u0026gt; Pageable) 0.00% 0.000us 0.00% 0.000us 0.000us 11.117ms 43.06% 11.117ms 11.117ms 1 aten::mul 0.91% 303.000us 1.36% 455.000us 11.375us 6.036ms 23.38% 6.378ms 159.450us 40 void at::native::vectorized_elementwise_kernel\u0026lt;4, at... 0.00% 0.000us 0.00% 0.000us 0.000us 4.067ms 15.75% 4.067ms 135.567us 30 ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 33.393ms Self CUDA time total: 25.816ms STAGE:2023-03-30 18:28:24 5849:5849 ActivityProfilerController.cpp:311] Completed Stage: Warm Up STAGE:2023-03-30 18:28:24 5849:5849 ActivityProfilerController.cpp:317] Completed Stage: Collection STAGE:2023-03-30 18:28:24 5849:5849 ActivityProfilerController.cpp:321] Completed Stage: Post Processing torch._dynamo.reset() compiled_model = torch.compile(model,options={\u0026#39;trace.enabled\u0026#39;:True, \u0026#39;trace.graph_diagram\u0026#39;:True}) out = compiled_model(x) [2023-03-30 18:28:24,754] torch._inductor.debug: [WARNING] model__1_forward_4 debug trace: /pytorch-examples/pytorch2-workshop-examples/torch_compile_debug/run_2023_03_30_18_28_23_632206-pid_5849/aot_torchinductor/model__1_forward_4.2 Writing FX graph to file: /pytorch-examples/pytorch2-workshop-examples/torch_compile_debug/run_2023_03_30_18_28_23_632206-pid_5849/aot_torchinductor/model__1_forward_4.2/graph_diagram.svg from torch.profiler import profile, record_function, ProfilerActivity with profile(activities=[ProfilerActivity.CUDA], ) as prof: out = compiled_model(x) print(prof.key_averages(group_by_stack_n=5).table(sort_by=\u0026#34;self_cuda_time_total\u0026#34;, row_limit=-1)) prof.export_chrome_trace(\u0026#34;compiled_trace.json\u0026#34;) ------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg Self CUDA Self CUDA % CUDA total CUDA time avg # of Calls ------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ triton__0d1d2d 0.00% 0.000us 0.00% 0.000us 0.000us 139.000us 100.00% 139.000us 139.000us 1 cudaDeviceSynchronize 100.00% 7.000us 100.00% 7.000us 7.000us 0.000us 0.00% 0.000us 0.000us 1 ------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 7.000us Self CUDA time total: 139.000us STAGE:2023-03-30 18:28:24 5849:5849 ActivityProfilerController.cpp:311] Completed Stage: Warm Up STAGE:2023-03-30 18:28:24 5849:5849 ActivityProfilerController.cpp:317] Completed Stage: Collection STAGE:2023-03-30 18:28:24 5849:5849 ActivityProfilerController.cpp:321] Completed Stage: Post Processing "},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"http://shashankprasanna.com/workshops/a-tour-of-pytorch2/tags/","title":"Tags","tags":[],"description":"","content":""}]