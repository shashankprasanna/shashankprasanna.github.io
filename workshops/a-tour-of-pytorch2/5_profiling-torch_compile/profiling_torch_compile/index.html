<!doctype html><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.111.3"><meta name=description content><link rel=icon href=/workshops/a-tour-of-pytorch2/images/favicon.png type=image/png><title>Profiling torch.compile() - A Tour of PyTorch 2.0</title><link href=/workshops/a-tour-of-pytorch2/css/nucleus.css?1680293318 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/fontawesome-all.min.css?1680293318 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/hybrid.css?1680293318 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/featherlight.min.css?1680293318 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/perfect-scrollbar.min.css?1680293318 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/auto-complete.css?1680293318 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/atom-one-dark-reasonable.css?1680293318 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/theme.css?1680293318 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/tabs.css?1680293318 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/hugo-theme.css?1680293318 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/theme-mine.css?1680293318 rel=stylesheet><script src=/workshops/a-tour-of-pytorch2/js/jquery-3.3.1.min.js?1680293318></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style></head><body data-url=/workshops/a-tour-of-pytorch2/5_profiling-torch_compile/profiling_torch_compile/><nav id=sidebar><div id=header-wrapper><div id=header><a id=logo href=/workshops/a-tour-of-pytorch2/><svg width="230" height="121" viewBox="0 0 488 121" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="group"><path id="Path" d="M63.1 28.3l-6.6 6.6c10.8 10.8 10.8 28.2.0 38.8-10.8 10.8-28.2 10.8-38.8.0-10.8-10.8-10.8-28.2.0-38.8L34.8 17.8 37.2 15.4V2.5L11.4 28.3C-3 42.7-3 65.9 11.4 80.3c14.4 14.4 37.6 14.4 51.7.0 14.4-14.5 14.4-37.6.0-52z" fill="#ee4c2c" fill-opacity="1" stroke="none"/><path id="Path-1" d="M55 21.9c0 2.651-2.149 4.8-4.8 4.8-2.651.0-4.8-2.149-4.8-4.8s2.149-4.8 4.8-4.8c2.651.0 4.8 2.149 4.8 4.8z" fill="#ee4c2c" fill-opacity="1" stroke="none"/></g><g id="group-1"><g id="group-2"><path id="Path-2" d="M129.8 61.3H118.7V89.8H110.3V8.7h20.4c21.3.0 31.5 10.5 31.5 25.2C162.5 52 149.9 61.3 129.8 61.3zM130.7 16.8C129.8 16.8 119 16.8 119 16.8V54.1L130.4 53.8C145.7 53.5 154.1 47.5 154.1 34.9 154.1 23.1 145.7 16.8 130.7 16.8z" fill="#000" fill-opacity="1" stroke="none"/><path id="Path-3" d="M199.8 89.5 195 102.4C189.6 116.8 183.9 121 175.8 121 171.3 121 168 119.8 164.4 118.3L166.8 110.8C169.5 112.3 172.5 113.5 175.8 113.5 180.3 113.5 183.6 111.1 188.1 99.7L192 89.2 168.9 30.6H177.6l18.6 49 18.3-49H222.9z" fill="#000" fill-opacity="1" stroke="none"/><path id="Path-4" d="M250.3 16.8V90.1H241.9V16.8H213.4V9h65.2v7.8C278.5 16.8 250.3 16.8 250.3 16.8z" fill="#000" fill-opacity="1" stroke="none"/><path id="Path-5" d="M302.3 91.6c-16.5.0-28.5-12.3-28.5-31.2s12.6-31.5 29.4-31.5 28.5 12.3 28.5 31.2C331.4 79 318.8 91.6 302.3 91.6zM302.6 36.4c-12.6.0-20.7 9.9-20.7 24C281.9 74.8 290.3 84.7 302.9 84.7 315.5 84.7 323.6 74.8 323.6 60.7 323.6 46 315.2 36.4 302.6 36.4z" fill="#000" fill-opacity="1" stroke="none"/><path id="Path-6" d="M351.8 90.1H343.7V30.6L351.8 28.8V41.4C355.7 33.9 361.4 28.8 369.2 28.8 373.1 28.8 376.7 30 379.7 31.5L377.6 39C374.9 37.5 371.9 36.3 368.6 36.3 362.3 36.3 356.6 41.1 351.8 51.6z" fill="#000" fill-opacity="1" stroke="none"/><path id="Path-7" d="M411.3 91.6c-18 0-29.1-12.9-29.1-31.2.0-18.6 12.3-31.5 29.1-31.5C418.5 28.9 424.8 30.7 429.9 34L427.8 41.2c-4.5-3-10.2-4.8-16.5-4.8-12.9.0-20.7 9.6-20.7 23.7C390.6 74.5 399 84.1 411.6 84.1c6 0 12-1.8 16.5-4.8L429.9 86.8C424.5 89.8 418.2 91.6 411.3 91.6z" fill="#000" fill-opacity="1" stroke="none"/><path id="Path-8" d="M479.5 90.1V51.6C479.5 41.1 475.3 36.6 466.9 36.6 460 36.6 453.4 40.2 448.6 45V90.1H440.5V2.7L448.6.9V38.5C454.9 32.2 462.7 29.2 469.3 29.2c11.4.0 18.6 7.5 18.6 20.4V90.2H479.5z" fill="#000" fill-opacity="1" stroke="none"/></g></g></svg></a></div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label>
<input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script type=text/javascript src=/workshops/a-tour-of-pytorch2/js/lunr.min.js?1680293318></script>
<script type=text/javascript src=/workshops/a-tour-of-pytorch2/js/auto-complete.js?1680293318></script>
<script type=text/javascript>var baseurl="http://shashankprasanna.com/workshops/a-tour-of-pytorch2/"</script><script type=text/javascript src=/workshops/a-tour-of-pytorch2/js/search.js?1680293318></script></div><div class=highlightable><ul class=topics><li data-nav-id=/workshops/a-tour-of-pytorch2/1_workshop_setup/ title="Workshop setup" class=dd-item><a href=/workshops/a-tour-of-pytorch2/1_workshop_setup/><b>1. </b>Workshop setup</a><ul><li data-nav-id=/workshops/a-tour-of-pytorch2/1_workshop_setup/free_options/ title="Free CPU and GPU options" class=dd-item><a href=/workshops/a-tour-of-pytorch2/1_workshop_setup/free_options/>Free CPU and GPU options</a></li></ul></li><li data-nav-id=/workshops/a-tour-of-pytorch2/2_whats_new/ title="What's New in PyTorch 2.0?" class=dd-item><a href=/workshops/a-tour-of-pytorch2/2_whats_new/><b>2. </b>What's New in PyTorch 2.0?</a></li><li data-nav-id=/workshops/a-tour-of-pytorch2/3_getting_started/ title="Using torch.compile()" class=dd-item><a href=/workshops/a-tour-of-pytorch2/3_getting_started/><b>3. </b>Using torch.compile()</a><ul><li data-nav-id=/workshops/a-tour-of-pytorch2/3_getting_started/a_simple_example/ title="A simple example" class=dd-item><a href=/workshops/a-tour-of-pytorch2/3_getting_started/a_simple_example/>3.1 A simple example</a></li><li data-nav-id=/workshops/a-tour-of-pytorch2/3_getting_started/benchmarking_resnet/ title="Benchmarking Resnet" class=dd-item><a href=/workshops/a-tour-of-pytorch2/3_getting_started/benchmarking_resnet/>3.2 Benchmarking Resnet</a></li><li data-nav-id=/workshops/a-tour-of-pytorch2/3_getting_started/hugging_face/ title="Benchmarking Huggingface" class=dd-item><a href=/workshops/a-tour-of-pytorch2/3_getting_started/hugging_face/>3.3 Benchmarking Huggingface</a></li></ul></li><li data-nav-id=/workshops/a-tour-of-pytorch2/4_inspecting_torch_compile/ title="What happens under the hood?" class=dd-item><a href=/workshops/a-tour-of-pytorch2/4_inspecting_torch_compile/><b>4. </b>What happens under the hood?</a><ul><li data-nav-id=/workshops/a-tour-of-pytorch2/4_inspecting_torch_compile/inspecting_torch_compile/ title="Inspecting torch.compile()" class=dd-item><a href=/workshops/a-tour-of-pytorch2/4_inspecting_torch_compile/inspecting_torch_compile/>4.1 Inspecting torch.compile()</a></li></ul></li><li data-nav-id=/workshops/a-tour-of-pytorch2/5_profiling-torch_compile/ title="Profiling your compiled code" class="dd-item
parent"><a href=/workshops/a-tour-of-pytorch2/5_profiling-torch_compile/><b>5. </b>Profiling your compiled code</a><ul><li data-nav-id=/workshops/a-tour-of-pytorch2/5_profiling-torch_compile/profiling_torch_compile/ title="Profiling torch.compile()" class="dd-item active"><a href=/workshops/a-tour-of-pytorch2/5_profiling-torch_compile/profiling_torch_compile/>5.1 Profiling torch.compile()</a></li></ul></li></ul><section id=shortcuts><h3>Links</h3><ul><li><a class=padding href=https://github.com/shashankprasanna/pytorch2-workshop-examples><i class='fab fa-fw fa-github'></i> Notebooks on GitHub</a></li><li><a class=padding href=https://shashankprasanna.com>Contact: shashankprasanna.com</a></li></ul></section><section id=footer></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i></a></span>
<span id=toc-menu><i class="fas fa-list-alt"></i></span>
<span class=links>Profiling torch.compile()</span></div><div class=progress><div class=wrapper><nav id=TableOfContents><ul><li><ul><li></li></ul></li></ul></nav></div></div></div></div><div id=head-tags></div><div id=body-inner><h1>Profiling torch.compile()</h1><p>For this example, we&rsquo;ll define yet another simple function. We&rsquo;ll use the taylor series expansion of sin(x). We choose this in particular for 2 reasons.</p><ul><li>To show <code>torch.compile</code> can generate C++/CUDA code for a variety of functions not just neural network models</li><li>You can grow this function to be arbtrarily large by simply adding more terms to the series expansion</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> optim
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch._dynamo
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision <span style=color:#f92672>import</span> models
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.profiler <span style=color:#f92672>import</span> profile, record_function, ProfilerActivity
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pi <span style=color:#f92672>=</span> math<span style=color:#f92672>.</span>pi
</span></span><span style=display:flex><span>device <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#34;cuda&#34;</span>) <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available() <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;cpu&#34;</span>
</span></span></code></pre></div><p>Define our sin(x) taylor series expansion</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sin_taylor</span>(x,n,device):
</span></span><span style=display:flex><span>    sinx<span style=color:#f92672>=</span><span style=color:#ae81ff>0.</span>
</span></span><span style=display:flex><span>    factorial <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> d: torch<span style=color:#f92672>.</span>lgamma(torch<span style=color:#f92672>.</span>tensor(d<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>,device<span style=color:#f92672>=</span>device))<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n):
</span></span><span style=display:flex><span>        sinx <span style=color:#f92672>+=</span> ((<span style=color:#f92672>-</span><span style=color:#ae81ff>1.</span>)<span style=color:#f92672>**</span>i)<span style=color:#f92672>*</span>(x<span style=color:#f92672>**</span>(<span style=color:#ae81ff>1</span><span style=color:#f92672>+</span><span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>i))<span style=color:#f92672>/</span>factorial(<span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sinx
</span></span></code></pre></div><p>Visualize the output</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>fig, ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>3</span>,figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>7</span>))
</span></span><span style=display:flex><span>rads <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>pi,<span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>ax<span style=color:#f92672>=</span>[b <span style=color:#66d9ef>for</span> a <span style=color:#f92672>in</span> ax <span style=color:#66d9ef>for</span> b <span style=color:#f92672>in</span> a]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>counter<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    sinx <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> x: sin_taylor(x,i,device)
</span></span><span style=display:flex><span>    ax[counter]<span style=color:#f92672>.</span>plot(rads,sinx(rads<span style=color:#f92672>.</span>to(device))<span style=color:#f92672>.</span>cpu(),label<span style=color:#f92672>=</span>i)
</span></span><span style=display:flex><span>    ax[counter]<span style=color:#f92672>.</span>set_title(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;sin(x) Taylor series terms: </span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>    ax[counter]<span style=color:#f92672>.</span>set_xlabel(<span style=color:#e6db74>&#39;radians (0-2*pi)&#39;</span>)
</span></span><span style=display:flex><span>    ax[counter]<span style=color:#f92672>.</span>set_ylabel(<span style=color:#e6db74>&#39;sin(x)&#39;</span>)
</span></span><span style=display:flex><span>    ax[counter]<span style=color:#f92672>.</span>set_ylim([<span style=color:#f92672>-</span><span style=color:#ae81ff>1.2</span>, <span style=color:#ae81ff>1.2</span>])
</span></span><span style=display:flex><span>    ax[counter]<span style=color:#f92672>.</span>set_xlim([<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>7</span>])
</span></span><span style=display:flex><span>    ax[counter]<span style=color:#f92672>.</span>title<span style=color:#f92672>.</span>set_fontsize(<span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>    ax[counter]<span style=color:#f92672>.</span>grid()
</span></span><span style=display:flex><span>    counter<span style=color:#f92672>+=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>fig<span style=color:#f92672>.</span>tight_layout()
</span></span></code></pre></div><p>Output:
You can see that the wave starts to resemble a sin wave after adding 9 terms.</p><p><img src=../taylor_sin.png alt=png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>10000000</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>n<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> x: sin_taylor(x,n,device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>time out <span style=color:#f92672>=</span> model(x)<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward()
</span></span></code></pre></div><p>Compile model with debug mode enabled</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>_dynamo<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span>compiled_model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>compile(model,options<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;trace.enabled&#39;</span>:<span style=color:#66d9ef>True</span>, <span style=color:#e6db74>&#39;trace.graph_diagram&#39;</span>:<span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>out <span style=color:#f92672>=</span> compiled_model(x)<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward()
</span></span></code></pre></div><h4 id=profile-uncompiled-code>Profile uncompiled code!</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch.profiler <span style=color:#f92672>import</span> profile, record_function, ProfilerActivity
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> profile(activities<span style=color:#f92672>=</span>[ProfilerActivity<span style=color:#f92672>.</span>CUDA,ProfilerActivity<span style=color:#f92672>.</span>CPU]) <span style=color:#66d9ef>as</span> prof:
</span></span><span style=display:flex><span>    out <span style=color:#f92672>=</span> model(x)<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(prof<span style=color:#f92672>.</span>key_averages(group_by_stack_n<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>)<span style=color:#f92672>.</span>table(sort_by<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;self_cuda_time_total&#34;</span>, row_limit<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>prof<span style=color:#f92672>.</span>export_chrome_trace(<span style=color:#e6db74>&#34;no_compile_trace.json&#34;</span>)
</span></span></code></pre></div><pre><code>-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                       cudaLaunchKernel         1.35%     450.000us         1.35%     450.000us       4.091us      12.766ms        49.45%      12.766ms     116.055us           110  
                                            aten::copy_         0.23%      78.000us        72.46%      24.196ms       2.016ms      11.268ms        43.65%      11.404ms     950.333us            12  
                       Memcpy DtoH (Device -&gt; Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us      11.117ms        43.06%      11.117ms      11.117ms             1  
                                              aten::mul         0.91%     303.000us         1.36%     455.000us      11.375us       6.036ms        23.38%       6.378ms     159.450us            40  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.067ms        15.75%       4.067ms     135.567us            30  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 33.393ms
Self CUDA time total: 25.816ms
</code></pre><h4 id=profile-compiled-code>Profile compiled code!</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>_dynamo<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span>compiled_model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>compile(model,options<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;trace.enabled&#39;</span>:<span style=color:#66d9ef>True</span>, <span style=color:#e6db74>&#39;trace.graph_diagram&#39;</span>:<span style=color:#66d9ef>True</span>})
</span></span><span style=display:flex><span>out <span style=color:#f92672>=</span> compiled_model(x)
</span></span></code></pre></div><pre><code>[2023-03-30 18:28:24,754] torch._inductor.debug: [WARNING] model__1_forward_4 debug trace: /pytorch-examples/pytorch2-workshop-examples/torch_compile_debug/run_2023_03_30_18_28_23_632206-pid_5849/aot_torchinductor/model__1_forward_4.2


Writing FX graph to file: /pytorch-examples/pytorch2-workshop-examples/torch_compile_debug/run_2023_03_30_18_28_23_632206-pid_5849/aot_torchinductor/model__1_forward_4.2/graph_diagram.svg
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch.profiler <span style=color:#f92672>import</span> profile, record_function, ProfilerActivity
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> profile(activities<span style=color:#f92672>=</span>[ProfilerActivity<span style=color:#f92672>.</span>CUDA, ProfilerActivity<span style=color:#f92672>.</span>CPU],
</span></span><span style=display:flex><span>            ) <span style=color:#66d9ef>as</span> prof:
</span></span><span style=display:flex><span>    out <span style=color:#f92672>=</span> compiled_model(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(prof<span style=color:#f92672>.</span>key_averages(group_by_stack_n<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>)<span style=color:#f92672>.</span>table(sort_by<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;self_cuda_time_total&#34;</span>, row_limit<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>prof<span style=color:#f92672>.</span>export_chrome_trace(<span style=color:#e6db74>&#34;compiled_trace.json&#34;</span>)
</span></span></code></pre></div><pre><code>-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
           triton__0d1d2d         0.00%       0.000us         0.00%       0.000us       0.000us     139.000us       100.00%     139.000us     139.000us             1  
    cudaDeviceSynchronize       100.00%       7.000us       100.00%       7.000us       7.000us       0.000us         0.00%       0.000us       0.000us             1  
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 7.000us
Self CUDA time total: 139.000us



STAGE:2023-03-30 18:28:24 5849:5849 ActivityProfilerController.cpp:311] Completed Stage: Warm Up
STAGE:2023-03-30 18:28:24 5849:5849 ActivityProfilerController.cpp:317] Completed Stage: Collection
STAGE:2023-03-30 18:28:24 5849:5849 ActivityProfilerController.cpp:321] Completed Stage: Post Processing
</code></pre><p>Tracing output</p><p><img src=../tracing.png alt></p><footer class=footline></footer></div></div><div id=navigation><a class="nav nav-prev" href=/workshops/a-tour-of-pytorch2/5_profiling-torch_compile/ title="Profiling your compiled code"><i class="fa fa-chevron-left"></i></a>
<a class="nav nav-next" href=/workshops/a-tour-of-pytorch2/1_workshop_setup/ title="Workshop setup" style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=/workshops/a-tour-of-pytorch2/js/clipboard.min.js?1680293318></script>
<script src=/workshops/a-tour-of-pytorch2/js/perfect-scrollbar.min.js?1680293318></script>
<script src=/workshops/a-tour-of-pytorch2/js/perfect-scrollbar.jquery.min.js?1680293318></script>
<script src=/workshops/a-tour-of-pytorch2/js/jquery.sticky.js?1680293318></script>
<script src=/workshops/a-tour-of-pytorch2/js/featherlight.min.js?1680293318></script>
<script src=/workshops/a-tour-of-pytorch2/js/highlight.pack.js?1680293318></script>
<script>hljs.initHighlightingOnLoad()</script><script src=/workshops/a-tour-of-pytorch2/js/modernizr.custom-3.6.0.js?1680293318></script>
<script src=/workshops/a-tour-of-pytorch2/js/learn.js?1680293318></script>
<script src=/workshops/a-tour-of-pytorch2/js/hugo-learn.js?1680293318></script>
<script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src=/workshops/a-tour-of-pytorch2/mermaid/mermaid.js?1680293318></script>
<script>mermaid.initialize({startOnLoad:!0})</script></body></html>