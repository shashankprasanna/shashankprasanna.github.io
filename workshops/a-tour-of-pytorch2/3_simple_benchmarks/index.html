<!doctype html><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.111.3"><meta name=description content><link rel=icon href=/workshops/a-tour-of-pytorch2/images/favicon.png type=image/png><title>Benchmarking torch.compile() - A Tour of PyTorch 2.0</title><link href=/workshops/a-tour-of-pytorch2/css/nucleus.css?1680204378 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/fontawesome-all.min.css?1680204378 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/hybrid.css?1680204378 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/featherlight.min.css?1680204378 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/perfect-scrollbar.min.css?1680204378 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/auto-complete.css?1680204378 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/atom-one-dark-reasonable.css?1680204378 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/theme.css?1680204378 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/tabs.css?1680204378 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/hugo-theme.css?1680204378 rel=stylesheet><link href=/workshops/a-tour-of-pytorch2/css/theme-mine.css?1680204378 rel=stylesheet><script src=/workshops/a-tour-of-pytorch2/js/jquery-3.3.1.min.js?1680204378></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style></head><body data-url=/workshops/a-tour-of-pytorch2/3_simple_benchmarks/><nav id=sidebar><div id=header-wrapper><div id=header><a id=logo href=/><svg width="250" height="121" viewBox="0 0 488 121" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="group"><path id="Path" d="M63.1 28.3l-6.6 6.6c10.8 10.8 10.8 28.2.0 38.8-10.8 10.8-28.2 10.8-38.8.0-10.8-10.8-10.8-28.2.0-38.8L34.8 17.8 37.2 15.4V2.5L11.4 28.3C-3 42.7-3 65.9 11.4 80.3c14.4 14.4 37.6 14.4 51.7.0 14.4-14.5 14.4-37.6.0-52z" fill="#ee4c2c" fill-opacity="1" stroke="none"/><path id="Path-1" d="M55 21.9c0 2.651-2.149 4.8-4.8 4.8-2.651.0-4.8-2.149-4.8-4.8s2.149-4.8 4.8-4.8c2.651.0 4.8 2.149 4.8 4.8z" fill="#ee4c2c" fill-opacity="1" stroke="none"/></g><g id="group-1"><g id="group-2"><path id="Path-2" d="M129.8 61.3H118.7V89.8H110.3V8.7h20.4c21.3.0 31.5 10.5 31.5 25.2C162.5 52 149.9 61.3 129.8 61.3zM130.7 16.8C129.8 16.8 119 16.8 119 16.8V54.1L130.4 53.8C145.7 53.5 154.1 47.5 154.1 34.9 154.1 23.1 145.7 16.8 130.7 16.8z" fill="#000" fill-opacity="1" stroke="none"/><path id="Path-3" d="M199.8 89.5 195 102.4C189.6 116.8 183.9 121 175.8 121 171.3 121 168 119.8 164.4 118.3L166.8 110.8C169.5 112.3 172.5 113.5 175.8 113.5 180.3 113.5 183.6 111.1 188.1 99.7L192 89.2 168.9 30.6H177.6l18.6 49 18.3-49H222.9z" fill="#000" fill-opacity="1" stroke="none"/><path id="Path-4" d="M250.3 16.8V90.1H241.9V16.8H213.4V9h65.2v7.8C278.5 16.8 250.3 16.8 250.3 16.8z" fill="#000" fill-opacity="1" stroke="none"/><path id="Path-5" d="M302.3 91.6c-16.5.0-28.5-12.3-28.5-31.2s12.6-31.5 29.4-31.5 28.5 12.3 28.5 31.2C331.4 79 318.8 91.6 302.3 91.6zM302.6 36.4c-12.6.0-20.7 9.9-20.7 24C281.9 74.8 290.3 84.7 302.9 84.7 315.5 84.7 323.6 74.8 323.6 60.7 323.6 46 315.2 36.4 302.6 36.4z" fill="#000" fill-opacity="1" stroke="none"/><path id="Path-6" d="M351.8 90.1H343.7V30.6L351.8 28.8V41.4C355.7 33.9 361.4 28.8 369.2 28.8 373.1 28.8 376.7 30 379.7 31.5L377.6 39C374.9 37.5 371.9 36.3 368.6 36.3 362.3 36.3 356.6 41.1 351.8 51.6z" fill="#000" fill-opacity="1" stroke="none"/><path id="Path-7" d="M411.3 91.6c-18 0-29.1-12.9-29.1-31.2.0-18.6 12.3-31.5 29.1-31.5C418.5 28.9 424.8 30.7 429.9 34L427.8 41.2c-4.5-3-10.2-4.8-16.5-4.8-12.9.0-20.7 9.6-20.7 23.7C390.6 74.5 399 84.1 411.6 84.1c6 0 12-1.8 16.5-4.8L429.9 86.8C424.5 89.8 418.2 91.6 411.3 91.6z" fill="#000" fill-opacity="1" stroke="none"/><path id="Path-8" d="M479.5 90.1V51.6C479.5 41.1 475.3 36.6 466.9 36.6 460 36.6 453.4 40.2 448.6 45V90.1H440.5V2.7L448.6.9V38.5C454.9 32.2 462.7 29.2 469.3 29.2c11.4.0 18.6 7.5 18.6 20.4V90.2H479.5z" fill="#000" fill-opacity="1" stroke="none"/></g></g></svg></a></div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label>
<input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script type=text/javascript src=/workshops/a-tour-of-pytorch2/js/lunr.min.js?1680204378></script>
<script type=text/javascript src=/workshops/a-tour-of-pytorch2/js/auto-complete.js?1680204378></script>
<script type=text/javascript>var baseurl="http://shashankprasanna.com/workshops/a-tour-of-pytorch2/"</script><script type=text/javascript src=/workshops/a-tour-of-pytorch2/js/search.js?1680204378></script></div><div class=highlightable><ul class=topics><li data-nav-id=/workshops/a-tour-of-pytorch2/1_getting_started/ title="Getting Started" class=dd-item><a href=/workshops/a-tour-of-pytorch2/1_getting_started/><b>1. </b>Getting Started</a></li><li data-nav-id=/workshops/a-tour-of-pytorch2/2_whats_new/ title="What's New in PyTorch 2.0" class=dd-item><a href=/workshops/a-tour-of-pytorch2/2_whats_new/><b>2. </b>What's New in PyTorch 2.0</a></li><li data-nav-id=/workshops/a-tour-of-pytorch2/3_simple_benchmarks/ title="Benchmarking torch.compile()" class="dd-item
active"><a href=/workshops/a-tour-of-pytorch2/3_simple_benchmarks/><b>3. </b>Benchmarking torch.compile()</a></li><li data-nav-id=/workshops/a-tour-of-pytorch2/4_inspecting_torch_compile/ title="What happens under the hood?" class=dd-item><a href=/workshops/a-tour-of-pytorch2/4_inspecting_torch_compile/><b>4. </b>What happens under the hood?</a></li><li data-nav-id=/workshops/a-tour-of-pytorch2/5_profiling-torch_compile/ title="Profiling your compiled code" class=dd-item><a href=/workshops/a-tour-of-pytorch2/5_profiling-torch_compile/><b>5. </b>Profiling your compiled code</a></li></ul><section id=footer><hr><div class=column><a href=https://shashankprasanna.com target=_blank><font color=black size=2>shashankprasanna.com</font><div class=column><a href=https://twitter.com/shshnkp target=_blank><font color=black size=2><i class="fab fa-twitter"></i>&nbsp;&nbsp;@shshnkp</font></a></div><div class=column><a href=https://www.youtube.com/@shashank.prasanna target=_blank><font color=black size=2><i class="fab fa-youtube"></i>&nbsp;&nbsp;@shashank.prasanna</font></a></div><div class=column><a href=https://medium.com/@shashankprasanna target=_blank><font color=black size=2><i class="fab fa-medium"></i>&nbsp;&nbsp;@shashankprasanna</font></a></div><div class=column><a href=https://www.linkedin.com/in/shashankprasanna/ target=_blank><font color=black size=2><i class="fab fa-linkedin"></i>&nbsp;&nbsp;&nbsp;shashankprasanna</font></a></div><div class=column><a href=https://github.com/shashankprasanna/ target=_blank><font color=black size=2><i class="fab fa-github"></i>&nbsp;&nbsp;&nbsp;shashankprasanna</font></a></div></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i></a></span>
<span id=toc-menu><i class="fas fa-list-alt"></i></span>
<span class=links>Benchmarking torch.compile()</span></div><div class=progress><div class=wrapper><nav id=TableOfContents><ul><li><ul><li><a href=#benchmarking-torchcompile>Benchmarking <code>torch.compile()</code></a></li><li><a href=#how-to-use-torchcompile>How to use <code>torch.compile()</code></a></li><li><a href=#benchmark-resnet18>Benchmark Resnet18</a></li></ul></li></ul></nav></div></div></div></div><div id=head-tags></div><div id=body-inner><h1>Benchmarking torch.compile()</h1><h3 id=benchmarking-torchcompile>Benchmarking <code>torch.compile()</code></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.utils.benchmark <span style=color:#66d9ef>as</span> benchmark
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision.models <span style=color:#f92672>import</span> resnet
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch._dynamo
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>device <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>device(<span style=color:#e6db74>&#34;cuda&#34;</span>) <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available() <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;cpu&#34;</span>
</span></span></code></pre></div><h3 id=how-to-use-torchcompile>How to use <code>torch.compile()</code></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MLP</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>64</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>fc1(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>relu(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> MLP()
</span></span><span style=display:flex><span>input <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>32</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>_dynamo<span style=color:#f92672>.</span>reset() <span style=color:#75715e># Only needed if you call this cell repeatedly</span>
</span></span><span style=display:flex><span>compiled_model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>compile(model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Alternatively you can also pass the backend</span>
</span></span><span style=display:flex><span>compiled_model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>compile(model, backend<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;inductor&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> model(input)
</span></span><span style=display:flex><span><span style=color:#75715e># triggers compilation of forward graph on the first run</span>
</span></span><span style=display:flex><span>output_compiled <span style=color:#f92672>=</span> compiled_model(input)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>all(output <span style=color:#f92672>==</span> output_compiled)
</span></span></code></pre></div><p>Output:</p><pre tabindex=0><code>    tensor(True)
</code></pre><h3 id=benchmark-resnet18>Benchmark Resnet18</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_batch_inference</span>(model, batch<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(batch, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>)<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>    model(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_batch_train</span>(model, optimizer, batch<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(batch, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>)<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>    out <span style=color:#f92672>=</span> model(x)
</span></span><span style=display:flex><span>    out<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> resnet<span style=color:#f92672>.</span>resnet18(weights<span style=color:#f92672>=</span>resnet<span style=color:#f92672>.</span>ResNet18_Weights<span style=color:#f92672>.</span>IMAGENET1K_V1)<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>batch <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>_dynamo<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span>compiled_model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>compile(model)
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>SGD(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>t_model <span style=color:#f92672>=</span> benchmark<span style=color:#f92672>.</span>Timer(
</span></span><span style=display:flex><span>    stmt<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;run_batch_train(model, optimizer, batch)&#39;</span>,
</span></span><span style=display:flex><span>    setup<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;from __main__ import run_batch_train&#39;</span>,
</span></span><span style=display:flex><span>    globals<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;model&#39;</span>: model,<span style=color:#e6db74>&#39;optimizer&#39;</span>:optimizer, <span style=color:#e6db74>&#39;batch&#39;</span>:batch})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>t_compiled_model <span style=color:#f92672>=</span> benchmark<span style=color:#f92672>.</span>Timer(
</span></span><span style=display:flex><span>    stmt<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;run_batch_train(model, optimizer, batch)&#39;</span>,
</span></span><span style=display:flex><span>    setup<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;from __main__ import run_batch_train&#39;</span>,
</span></span><span style=display:flex><span>    globals<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;model&#39;</span>: compiled_model, <span style=color:#e6db74>&#39;optimizer&#39;</span>:optimizer, <span style=color:#e6db74>&#39;batch&#39;</span>:batch})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>t_model_runs <span style=color:#f92672>=</span> t_model<span style=color:#f92672>.</span>timeit(<span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>t_compiled_model_runs <span style=color:#f92672>=</span> t_compiled_model<span style=color:#f92672>.</span>timeit(<span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(t_model_runs)
</span></span><span style=display:flex><span>print(t_compiled_model_runs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Resnet18 Training speedup: </span><span style=color:#e6db74>{</span><span style=color:#ae81ff>100</span><span style=color:#f92672>*</span>(t_model_runs<span style=color:#f92672>.</span>mean <span style=color:#f92672>-</span> t_compiled_model_runs<span style=color:#f92672>.</span>mean) <span style=color:#f92672>/</span> t_model_runs<span style=color:#f92672>.</span>mean<span style=color:#e6db74>:</span><span style=color:#e6db74> .2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>%&#34;</span>)
</span></span></code></pre></div><div class="notices warning"><p>Performance depends on your sytem configuration and GPU type.
You may not always see a speedup!</p></div><p>Output:</p><pre tabindex=0><code>    Resnet18 Training speedup:  4.78%
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>batch <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>_dynamo<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span>compiled_model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>compile(model, mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;reduce-overhead&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>t_model <span style=color:#f92672>=</span> benchmark<span style=color:#f92672>.</span>Timer(
</span></span><span style=display:flex><span>    stmt<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;run_batch_inference(model, batch)&#39;</span>,
</span></span><span style=display:flex><span>    setup<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;from __main__ import run_batch_inference&#39;</span>,
</span></span><span style=display:flex><span>    globals<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;model&#39;</span>: model, <span style=color:#e6db74>&#39;batch&#39;</span>:batch})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>t_compiled_model <span style=color:#f92672>=</span> benchmark<span style=color:#f92672>.</span>Timer(
</span></span><span style=display:flex><span>    stmt<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;run_batch_inference(model, batch)&#39;</span>,
</span></span><span style=display:flex><span>    setup<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;from __main__ import run_batch_inference&#39;</span>,
</span></span><span style=display:flex><span>    globals<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;model&#39;</span>: compiled_model, <span style=color:#e6db74>&#39;batch&#39;</span>:batch})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>t_model_runs <span style=color:#f92672>=</span> t_model<span style=color:#f92672>.</span>timeit(<span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>t_compiled_model_runs <span style=color:#f92672>=</span> t_compiled_model<span style=color:#f92672>.</span>timeit(<span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Resnet18 Inference speedup: </span><span style=color:#e6db74>{</span><span style=color:#ae81ff>100</span><span style=color:#f92672>*</span>(t_model_runs<span style=color:#f92672>.</span>mean <span style=color:#f92672>-</span> t_compiled_model_runs<span style=color:#f92672>.</span>mean) <span style=color:#f92672>/</span> t_model_runs<span style=color:#f92672>.</span>mean<span style=color:#e6db74>:</span><span style=color:#e6db74> .2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>%&#34;</span>)
</span></span></code></pre></div><div class="notices warning"><p>Performance depends on your sytem configuration and GPU type.
You may not always see a speedup!</p></div><pre tabindex=0><code>   
    Resnet18 Inference speedup:  31.43%
</code></pre><h1 id=huggingface>Huggingface</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> Wav2Vec2Processor, Wav2Vec2ForCTC
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_hf_inference</span>(model, input_values):
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># retrieve logits</span>
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> model(input_values)<span style=color:#f92672>.</span>logits
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># take argmax and decode</span>
</span></span><span style=display:flex><span>    predicted_ids <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>argmax(logits, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    transcription <span style=color:#f92672>=</span> processor<span style=color:#f92672>.</span>batch_decode(predicted_ids)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># load model and processor</span>
</span></span><span style=display:flex><span>processor <span style=color:#f92672>=</span> Wav2Vec2Processor<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;facebook/wav2vec2-large-960h-lv60-self&#34;</span>)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> Wav2Vec2ForCTC<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;facebook/wav2vec2-large-960h-lv60-self&#34;</span>)<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># load dummy dataset and read soundfiles</span>
</span></span><span style=display:flex><span>ds <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;patrickvonplaten/librispeech_asr_dummy&#34;</span>, <span style=color:#e6db74>&#34;clean&#34;</span>, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;validation&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># tokenize</span>
</span></span><span style=display:flex><span>input_values <span style=color:#f92672>=</span> processor(ds[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;audio&#34;</span>][<span style=color:#e6db74>&#34;array&#34;</span>], return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>, padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;longest&#34;</span>)<span style=color:#f92672>.</span>input_values<span style=color:#f92672>.</span>cuda()
</span></span></code></pre></div><pre><code>Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Found cached dataset librispeech_asr_dummy (/root/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)
It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>batch <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>_dynamo<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span>compiled_model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>compile(model, mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;max-autotune&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>t_model <span style=color:#f92672>=</span> benchmark<span style=color:#f92672>.</span>Timer(
</span></span><span style=display:flex><span>    stmt<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;run_hf_inference(model, input_values)&#39;</span>,
</span></span><span style=display:flex><span>    setup<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;from __main__ import run_hf_inference&#39;</span>,
</span></span><span style=display:flex><span>    globals<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;model&#39;</span>: model, <span style=color:#e6db74>&#39;input_values&#39;</span>:input_values})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>t_compiled_model <span style=color:#f92672>=</span> benchmark<span style=color:#f92672>.</span>Timer(
</span></span><span style=display:flex><span>    stmt<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;run_hf_inference(model, input_values)&#39;</span>,
</span></span><span style=display:flex><span>    setup<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;from __main__ import run_hf_inference&#39;</span>,
</span></span><span style=display:flex><span>    globals<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#39;model&#39;</span>: compiled_model, <span style=color:#e6db74>&#39;input_values&#39;</span>:input_values})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>t_model_runs <span style=color:#f92672>=</span> t_model<span style=color:#f92672>.</span>timeit(<span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>t_compiled_model_runs <span style=color:#f92672>=</span> t_compiled_model<span style=color:#f92672>.</span>timeit(<span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Huggingface Inference speedup: </span><span style=color:#e6db74>{</span><span style=color:#ae81ff>100</span><span style=color:#f92672>*</span>(t_model_runs<span style=color:#f92672>.</span>mean <span style=color:#f92672>-</span> t_compiled_model_runs<span style=color:#f92672>.</span>mean) <span style=color:#f92672>/</span> t_model_runs<span style=color:#f92672>.</span>mean<span style=color:#e6db74>:</span><span style=color:#e6db74> .2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>%&#34;</span>)
</span></span></code></pre></div><pre><code>Huggingface Inference speedup:  4.27%
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python></code></pre></div><footer class=footline></footer></div></div><div id=navigation><a class="nav nav-prev" href=/workshops/a-tour-of-pytorch2/2_whats_new/ title="What's New in PyTorch 2.0"><i class="fa fa-chevron-left"></i></a>
<a class="nav nav-next" href=/workshops/a-tour-of-pytorch2/4_inspecting_torch_compile/ title="What happens under the hood?" style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=/workshops/a-tour-of-pytorch2/js/clipboard.min.js?1680204378></script>
<script src=/workshops/a-tour-of-pytorch2/js/perfect-scrollbar.min.js?1680204378></script>
<script src=/workshops/a-tour-of-pytorch2/js/perfect-scrollbar.jquery.min.js?1680204378></script>
<script src=/workshops/a-tour-of-pytorch2/js/jquery.sticky.js?1680204378></script>
<script src=/workshops/a-tour-of-pytorch2/js/featherlight.min.js?1680204378></script>
<script src=/workshops/a-tour-of-pytorch2/js/highlight.pack.js?1680204378></script>
<script>hljs.initHighlightingOnLoad()</script><script src=/workshops/a-tour-of-pytorch2/js/modernizr.custom-3.6.0.js?1680204378></script>
<script src=/workshops/a-tour-of-pytorch2/js/learn.js?1680204378></script>
<script src=/workshops/a-tour-of-pytorch2/js/hugo-learn.js?1680204378></script>
<script src=/workshops/a-tour-of-pytorch2/mermaid/mermaid.js?1680204378></script>
<script>mermaid.initialize({startOnLoad:!0})</script></body></html>