<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Using torch.compile() on A Tour of PyTorch 2.0</title><link>http://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_getting_started/</link><description>Recent content in Using torch.compile() on A Tour of PyTorch 2.0</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 29 Mar 2023 15:47:39 -0700</lastBuildDate><atom:link href="http://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_getting_started/index.xml" rel="self" type="application/rss+xml"/><item><title>A simple example</title><link>http://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_getting_started/a_simple_example/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_getting_started/a_simple_example/</guid><description>How to use torch.compile() : A simple example Load necessary modules, we&amp;rsquo;ll discuss the strange torch._dynamo bit a little later.
import torch import torch.nn as nn from torchvision.models import resnet import torch._dynamo device = torch.device(&amp;#34;cuda&amp;#34;) if torch.cuda.is_available() else &amp;#34;cpu&amp;#34; Define a very simple NN and compile it with torch.compile() and compare the results with your model to convince yourself that it won&amp;rsquo;t break your code.
class MLP(nn.Module): def __init__(self): super().__init__() self.</description></item><item><title>Benchmarking Resnet</title><link>http://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_getting_started/benchmarking_resnet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_getting_started/benchmarking_resnet/</guid><description>Benchmark a Resnet18 model Now we&amp;rsquo;ll add some additional code to benchmark the training and inference workflows with random data. We&amp;rsquo;ll use the default settings with torch.compile() and then introduce different modes of operation: torch.compile(...,mode='reduce-overhead') and torch.compile(...,mode='max-autotune')
Performance depends on various factors such as your model type, system configuration and GPU type You may not always see a speedup. In the subsequent sections we&amp;rsquo;ll see how to profile and identify potential issues.</description></item><item><title>Benchmarking Huggingface</title><link>http://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_getting_started/hugging_face/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://shashankprasanna.com/workshops/a-tour-of-pytorch2/3_getting_started/hugging_face/</guid><description>Let&amp;rsquo;s try a language mode with HuggingFace
import torch from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC from datasets import load_dataset torch.set_float32_matmul_precision(&amp;#39;high&amp;#39;) def run_hf_inference(model, input_values): # retrieve logits logits = model(input_values).logits # take argmax and decode predicted_ids = torch.argmax(logits, dim=-1) transcription = processor.batch_decode(predicted_ids) # load model and processor processor = Wav2Vec2Processor.from_pretrained(&amp;#34;facebook/wav2vec2-large-960h-lv60-self&amp;#34;) model = Wav2Vec2ForCTC.from_pretrained(&amp;#34;facebook/wav2vec2-large-960h-lv60-self&amp;#34;).cuda() # load dummy dataset and read soundfiles ds = load_dataset(&amp;#34;patrickvonplaten/librispeech_asr_dummy&amp;#34;, &amp;#34;clean&amp;#34;, split=&amp;#34;validation&amp;#34;) # tokenize input_values = processor(ds[0][&amp;#34;audio&amp;#34;][&amp;#34;array&amp;#34;], return_tensors=&amp;#34;pt&amp;#34;, padding=&amp;#34;longest&amp;#34;).input_values.cuda() batch = 1 torch.</description></item></channel></rss>