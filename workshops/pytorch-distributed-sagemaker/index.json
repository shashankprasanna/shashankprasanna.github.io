[{"uri":"/1_setup/login_aws_account.html","title":"1.1 Login to your temporary workshop AWS Account","tags":[],"description":"","content":" TEMPORARY ACCOUNTS ARE ONLY AVAILABLE DURING THE LIVE WORKSHOP. FEEL FREE TO FOLLOW THE STEPS IN YOUR OWN PERSONAL AWS ACCOUNT.\nGet your temporary AWS account Click on the link at the bottom of the browser as show below.\nClick on Accept Terms \u0026amp; Login Click on Email One-Time OTP (Allow for up to 2 mins to receive the passcode) Provide your email address Enter your OTP code Click on AWS Console Click on Open AWS Console In the AWS Console click on Amazon SageMaker Click on Amazon SageMaker Studio and then click on Open Studio You should now have Amazon SageMaker Studio interface open on your browser "},{"uri":"/2_distributed_training/why_how_distributed.html","title":"2.1 Why distributed training ","tags":[],"description":"","content":"\nPyTorch supports several different distributed training backends that perform the All-Reduce operation "},{"uri":"/appendix/docs.html","title":"Documentation resources","tags":[],"description":"","content":"1. SageMaker SDK API guide https://sagemaker.readthedocs.io/en/stable/\n2. SageMaker Sample Notebooks on GitHub https://github.com/awslabs/amazon-sagemaker-examples\n3. SageMaker Developer Guide https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html\n4. SageMaker API Reference https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_Operations_Amazon_SageMaker_Service.html\n"},{"uri":"/","title":"Welcome to PyTorch Distributed Training with Amazon SageMaker Workshop","tags":[],"description":"","content":"PyTorch Distributed Training with Amazon SageMaker Presented by Shashank Prasanna Approx. duration: 2 hours\nAbstract: Reducing time-to-train of your PyTorch models is crucial in improving your productivity and reducing your time-to-solution. In this workshop, you will learn how to efficiently scale your training workloads to multiple instances, with Amazon SageMaker doing the heavy-lifting for you. You don’t have to manage compute, storage and networking infrastructure, simply bring in your PyTorch code and distribute training across large number of CPUs and GPUs. The AWS PyTorch team will also discuss their latest PyTorch feature contributions.\nLearning objective:\nLearn how to get started with AWS on PyTorch Learn about the most recent PyTorch and AWS libraries for deep learning Best practices to reduce training times for your deep learning models Agenda Topics Duration (90 mins) Setup and getting started 20 mins Problem overview and dataset preparation 20 mins Distributed Training with SageMaker 40 mins Wrap Up 10 mins "},{"uri":"/1_setup.html","title":"1. Getting started","tags":[],"description":"","content":"For this workshop you\u0026rsquo;ll get access to a temporary AWS Account already pre-configured with Amazon SageMaker Studio. Follow the steps in this section to login to your AWS Account and download the workshop material.\n"},{"uri":"/1_setup/download_workshop_content.html","title":"1.2 Download workshop content","tags":[],"description":"","content":" Feel free to follow along with the presenter on the stage\nOpen a new terminal window Clone the workshop content In the terminal paste the following command to clone the workshop content repo:\ngit clone https://github.com/shashankprasanna/pytorch-sagemaker-distributed-workshop.git Double click on the pytorch-sagemaker-distributed-workshop folder Double click on notebooks folder Double click on the the first notebook Choose the PyTorch Kernel shown below and hit select "},{"uri":"/2_distributed_training/sagemaker_and_pytorch.html","title":"2.2 How Amazon SageMaker and PyTorch work together","tags":[],"description":"","content":" Shared responsibility model "},{"uri":"/3_clean-up/cleanup.html","title":"Delete all resources","tags":[],"description":"","content":"This workshop creates the following resources:\nSageMaker Endpoints S3 objects SageMaker apps If you completed section 2.2, the \u0026ldquo;Delete resources\u0026rdquo; section at the end deletes running SageMaker Endpoints and all S3 objects created during the workshop.\nYou can also delete the endpoints by navigating to AWS Console \u0026gt; Amazon SageMaker. In the left menu click on Inference \u0026gt; Endpoints. Select the endpoint you want to delete and click on Action \u0026gt; Delete.\nFor additional information about deleting SageMaker resources, please visit the following documentation page: https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html\nDeleting SageMaker Studio apps Using the AWS Console SageMaker Studio also creates apps such as JupyterServer (used to run notebooks), DataWrangler and Debugger which run on EC2 instances. Use the following instructions to shutdown running apps:\nNavigate to AWS Console \u0026gt; Amazon SageMaker \u0026gt; Amazon SageMaker Studio. This will open up the SageMaker Studio Control Panel. Click on the Studio user who’s resources you want to delete.\nUnder User Details click on “Delete app” to delete all running apps. Keep the “default” App if you want to continue working with SageMaker Studio and want to launch new notebooks.\nUsing the SageMaker Studio In SageMaker Studio Notebook, click on the running apps menu which is 3rd from the top. Click on all the power buttons to shut down apps. Keep the running instances if you want to continue working on SageMaker Notebook.\nFor more information about deleting Studio resources, Studio domain and how to delete resources using AWS CLI visit the following documentation page: https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-delete-domain.html?icmpid=docs_sagemaker_console_studio\n"},{"uri":"/2_distributed_training.html","title":"2. Distributed Training with PyTorch and Amazon SageMaker ","tags":[],"description":"","content":"In this section you\u0026rsquo;ll learn how to prepare your PyTorch training scripts and run distributed training jobs with Amazon SageMaker You will cover: Preparing your dataset for Amazon SageMaker by uploading them to Amazon S3 Writing your PyTorch training script for distributed training Writing your Amazon SageMaker SDK functions to run distributed training Running distributed training jobs on specified number of CPU instances Deploying your trained models to endpoints using SageMaker and evaluating them BONUS: Running high-performance and large scale training on GPUs "},{"uri":"/2_distributed_training/pytorch_native_dist_training.html","title":"2.3 Notebook: PyTorch Native Distributed Training with Amazon SageMaker","tags":[],"description":"","content":"Open the following notebook to follow along Notebook: 1_pytorch_dist_native_cpu.ipynb\nFollow along with the presenter to walkthrough and execute the notebook\n"},{"uri":"/2_distributed_training/pytorch_smddp_dist_training.html","title":"2.4 Bonus: PyTorch SageMaker Data Parallel Distributed Training with Amazon SageMaker","tags":[],"description":"","content":" Note: This notebook won\u0026rsquo;t run on your instances since it needs 8 GPU instances that are not supported on the temporary accounts.\nOpen the following notebook to follow along Notebook: 2_pytorch_dist_smddp_gpu.ipynb\nFollow along with the presenter to walkthrough and execute the notebook\n"},{"uri":"/appendix/blogposts_videos.html","title":"Blogposts and videos","tags":[],"description":"","content":"1. ML blog posts https://medium.com/@shashankprasanna 2. AWS Blog posts https://aws.amazon.com/blogs/machine-learning/ "},{"uri":"/3_clean-up.html","title":"3. Clean up resources","tags":[],"description":"","content":"In this section you\u0026rsquo;ll find instructions to clean up resources used for this workshop.\n"},{"uri":"/appendix.html","title":"Appendix","tags":[],"description":"","content":""},{"uri":"/categories.html","title":"Categories","tags":[],"description":"","content":""},{"uri":"/tags.html","title":"Tags","tags":[],"description":"","content":""}]