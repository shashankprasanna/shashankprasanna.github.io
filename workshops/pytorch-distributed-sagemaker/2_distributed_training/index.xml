<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2. Distributed Training with PyTorch and Amazon SageMaker on Distributed ML training with PyTorch and Amazon SageMaker</title><link>/2_distributed_training.html</link><description>Recent content in 2. Distributed Training with PyTorch and Amazon SageMaker on Distributed ML training with PyTorch and Amazon SageMaker</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="/2_distributed_training/index.xml" rel="self" type="application/rss+xml"/><item><title>2.1 Why distributed training</title><link>/2_distributed_training/why_how_distributed.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/2_distributed_training/why_how_distributed.html</guid><description>
PyTorch supports several different distributed training backends that perform the All-Reduce operation</description></item><item><title>2.2 How Amazon SageMaker and PyTorch work together</title><link>/2_distributed_training/sagemaker_and_pytorch.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/2_distributed_training/sagemaker_and_pytorch.html</guid><description> Shared responsibility model</description></item><item><title>2.3 Notebook: PyTorch Native Distributed Training with Amazon SageMaker</title><link>/2_distributed_training/pytorch_native_dist_training.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/2_distributed_training/pytorch_native_dist_training.html</guid><description>Open the following notebook to follow along Notebook: 1_pytorch_dist_native_cpu.ipynb
Follow along with the presenter to walkthrough and execute the notebook</description></item><item><title>2.4 Bonus: PyTorch SageMaker Data Parallel Distributed Training with Amazon SageMaker</title><link>/2_distributed_training/pytorch_smddp_dist_training.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/2_distributed_training/pytorch_smddp_dist_training.html</guid><description>Note: This notebook won&amp;rsquo;t run on your instances since it needs 8 GPU instances that are not supported on the temporary accounts.
Open the following notebook to follow along Notebook: 2_pytorch_dist_smddp_gpu.ipynb
Follow along with the presenter to walkthrough and execute the notebook</description></item></channel></rss>