[{"body":" Reach out if you‚Äôd like an instructor led hands-on workshop ‚ö°Ô∏è More coming soon! ‚ö°Ô∏è üõ†Ô∏è Workshop: A Tour of PyTorch 2.0 Learn about PyTorch 2.0 with a deep dive into the technology stack that powers the new torch.compile() API: TorchDynamo, AITAutograd, PrimTorch and TorchInductor. The new compiler stack reduces training times across a wide range of workloads while being fully backwards compatible. Bring your laptops, or connect to remote GPU powered systems to run the examples. üõ†Ô∏è Workshop: PyTorch Distributed Training on AWS Learn how to efficiently scale your training workloads to multiple instances with Amazon SageMaker. SageMaker manages your compute, storage and networking infrastructure, simply bring in your PyTorch code and learn how to distribute training across large number of CPUs and GPUs. Relevant blog posts AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution Choosing the right GPU for deep learning on AWS How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators A complete guide to AI accelerators for deep learning inference ‚Äî GPUs, AWS Inferentia and Amazon Elastic Inference ","categories":"","description":"","excerpt":" Reach out if you‚Äôd like an instructor led hands-on workshop ‚ö°Ô∏è More ‚Ä¶","ref":"/workshops/","tags":"","title":"PyTorch Workshops"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"Personal Blog"},{"body":"Here are few of my popular blogposts. Read these and more on my medium blog: https://medium.com/@shashankprasanna Choosing the right GPU for deep learning on AWS AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators A complete guide to AI accelerators for deep learning inference ‚Äî GPUs, AWS Inferentia and Amazon Elastic Inference ","categories":"","description":"","excerpt":"Here are few of my popular blogposts. Read these and more on my medium ‚Ä¶","ref":"/popular_blog_posts/","tags":"","title":"Popular blog posts"},{"body":"Interviews with PyTorch 2.0 engineers\nVisit the PyTorch YouTube channel https://www.youtube.com/@PyTorch/ ","categories":"","description":"","excerpt":"Interviews with PyTorch 2.0 engineers\nVisit the PyTorch YouTube ‚Ä¶","ref":"/pytorch2-livestreams/","tags":"","title":"PyTorch 2.0 interviews"},{"body":"Here are 5 GPU instance recommendations on AWS that should serve majority of deep learning use-cases. For a complete deep dive into choosing the right GPU for deep learning on AWS, read my blog post:\nChoosing the right GPU for deep learning on AWS Highest performing multi-GPU instance on AWS Instance: p4d.24xlarge\nWhen to use it: When you need all the performance you can get. Use it for distributed training on large models and datasets.\nWhat you get: 8 x NVIDIA A100 GPUs with 40 GB GPU memory per GPU. Based on the latest NVIDIA Ampere architecture. Includes 3rd generation NVLink for fast multi-GPU training.\nHighest performing single-GPU instance on AWS Instance: p3.2xlarge\nWhen to use it: When you want the highest performance Single GPU and you‚Äôre fine with 16 GB of GPU memory.\nWhat you get: 1 x NVIDIA V100 GPU with 16 GB of GPU memory. Based on the older NVIDIA Volta architecture. The best performing single-GPU is still the NVIDIA A100 on P4 instance, but you can only get 8 x NVIDIA A100 GPUs on P4. This GPU has a slight performance edge over NVIDIA A10G on G5 instance discussed next, but G5 is far more cost-effective and has more GPU memory.\nBest performance/cost, single-GPU instance on AWS Instance: g5.xlarge\nWhen to use it: When you want high-performance, more GPU memory at lower cost than P3 instance\nWhat you get: 1 x NVIDIA A10G GPU with 24 GB of GPU memory, based on the latest Ampere architecture. NVIDIA A10G can be seen as a lower powered cousin of the A100 on the p4d.24xlarge so it‚Äôs easy to migrate and scale when you need more compute. Consider larger sizes withg5.(2/4/8/16)xlarge for the same single-GPU with more vCPUs and higher system memory if you have more pre or post processing steps.\nBest performance/cost, multi-GPU instance on AWS Instance: p3.(8/16)xlarge\nWhen to use it: Cost-effective multi-GPU model development and training.\nWhat you get: p3.8xlarge has 4 x NVIDIA V100 GPUs and p3.16xlarge has 8 x NVIDIA V100 GPUs with 16 GB of GPU memory on each GPU, based on the older NVIDIA Volta architecture. For larger models, datasets and faster performance consider P4 instances.\nHigh-performance GPU instance at a budget on AWS Instance: g4dn.xlarge\nWhen to use it: Lower performance than other options at lower cost for model development and training. Cost effective model inference deployment.\nWhat you get: 1 x NVIDIA T4 GPU with 16 GB of GPU memory. Based on the previous generation NVIDIA Turing architecture. Consider g4dn.(2/4/8/16)xlarge for more vCPUs and higher system memory if you have more pre or post processing.\nRelated blog posts AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution Choosing the right GPU for deep learning on AWS How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators ","categories":"","description":"In a hurry? Here are the best GPUs for Deep Learining on AWS","excerpt":"In a hurry? Here are the best GPUs for Deep Learining on AWS","ref":"/best-gpus-on-aws-for-deep-learning/","tags":"","title":"Best GPUs on AWS for Deep Learning"},{"body":"\nAn AI accelerator is a dedicated processor designed to accelerate machine learning computations. Machine learning, and particularly its subset, deep learning is primarily composed of a large number of linear algebra computations, (i.e. matrix-matrix, matrix-vector operations) and these operations can be easily parallelized. AI accelerators are specialized hardware designed to accelerate these basic machine learning computations and improve performance, reduce latency and reduce cost of deploying machine learning based applications.\nWhy do we need specialized AI accelerators? The two most important reasons for building dedicated processors for machine learning are:\nEnergy efficiency Faster performance Recent trends to improve model accuracy, have been to introduce larger models with more parameters and train them on larger data sets. As model sizes get larger, and current processors won‚Äôt be able to deliver the processing power needed to train or run inference on these models under tight time-to-train and inference latency requirements.\nGeneral purpose processors like CPUs trade-off energy efficiency for versatility and special purpose processors (AI accelerators) trade off versatility for energy efficiency. AI accelerators on the other hand can be designed with features to minimize memory access, offer larger on-chip cache and include dedicated hardware features to accelerate matrix-matrix computations. Since AI accelerators are purpose built devices it is ‚Äúaware‚Äù of the algorithms that it runs on and its dedicated features will run it more efficiently than a general purpose processor.\nList of popular AI accelerators for training\nNVIDIA GPUs: Available on AWS, GCP, Azure and at your local computer store (See my recommendation list on the left menu) AWS Tranium: Available on AWS Intel Habana Gaudi: Available on AWS (v1) and Intel DevCloud (v1 and v2) Google Cloud TPUs: Available on GCP and via Colab (v1-v4) List of popular AI accelerators for inference\nNVIDIA GPUs: Available on AWS, GCP, Azure (See my recommendation list on the left menu) AWS Inferentia: Available on AWS (See my recommend blog post below) Intel Habana Gaudi: Available on AWS and Intel DevCloud (v1 and v2) Google Cloud TPUs: Available on GCP and via Colab (v1-v4) Note: Modern GPUs have dedicated silicon (TensorCores) and precision types (TF32, BF16) designed for deep learning bringing them closer to dedicated AI accelerators vs. general purpose parallel processors Recommended blog posts AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution A complete guide to AI accelerators for deep learning inference ‚Äî GPUs, AWS Inferentia and Amazon Elastic Inference ","categories":"","description":"An AI accelerator is a dedicated processor designed to accelerate machine learning computations.","excerpt":"An AI accelerator is a dedicated processor designed to accelerate ‚Ä¶","ref":"/what-is-an-ai-accelerator/","tags":"","title":"What is an AI accelerator?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/quick-guides/","tags":"","title":"üìö Quick guides"},{"body":"Here is a complete list of all Amazon EC2 GPU instance types on AWS that I‚Äôve painstakenly compiled, because you can‚Äôt find this information anywhere on AWS. In the tabular format below, you‚Äôll find more detailed information about GPU type, interconnect, Thermal design power (TDP), precision types supported etc.\nFrom my blog post: Choosing the right GPU for deep learning on AWS Tabular format With more information than you were probably looking for üòä\nArchitecture NVIDIA GPU Instance type Instance name Number of GPUs GPU Memory (per GPU) GPU Interconnect (NVLink / PCIe) Thermal\nDesign Power (TDP) from nvidia-smi Tensor Cores (mixed-precision) Precision Support CPU Type Nitro based Ampere A100 P4 p4d.24xlarge 8 40 GB NVLink gen 3 (600 GB/s) 400W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 Intel Xeon Scalable (Cascade Lake) Yes Ampere A10G G5 g5.xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.2xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.4xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.8xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.16xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.12xlarge 4 24 GB PCIe 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.24xlarge 4 24 GB PCIe 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.48xlarge 8 24 GB PCIe 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Turing T4G G5 g5g.xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.2xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.4xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.8xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.16xlarge 2 16 GB PCIe 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.metal 2 16 GB PCIe 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4 G4 g4dn.xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.2xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.4xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.8xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.16xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.12xlarge 4 16 GB PCIe 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.metal 8 16 GB PCIe 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Volta V100 P3 p3.2xlarge 1 16 GB NA (single GPU) 300W Tensor Cores (Gen 1) FP64, FP32, FP16 Intel Xeon (Broadwell) No Volta V100 P3 p3.8xlarge 4 16 GB NVLink gen 2 (300 GB/s) 300W Tensor Cores (Gen 1) FP64, FP32, FP16 Intel Xeon (Broadwell) No Volta V100 P3 p3.16xlarge 8 16 GB NVLink gen 2 (300 GB/s) 300W Tensor Cores (Gen 1) FP64, FP32, FP16 Intel Xeon (Broadwell) No Volta V100* P3 p3dn.24xlarge 8 32 GB NVLink gen 2 (300 GB/s) 300W Tensor Cores (Gen 1) FP64, FP32, FP16 Intel Xeon (Skylake) Yes Kepler K80 P2 p2.xlarge 1 12 GB NA (single GPU) 149W No FP64, FP32 Intel Xeon (Broadwell) No Kepler K80 P2 p2.8xlarge 8 12 GB PCIe 149W No FP64, FP32 Intel Xeon (Broadwell) No Kepler K80 P2 p2.16xlarge 16 12 GB PCIe 149W No FP64, FP32 Intel Xeon (Broadwell) No Maxwell M60 G3 g3s.xlarge 1 8 GB PCIe 150W No FP32 Intel Xeon (Broadwell) No Maxwell M60 G3 g3.4xlarge 1 8 GB PCIe 150W No FP32 Intel Xeon (Broadwell) No Maxwell M60 G3 g3.8xlarge 2 8 GB PCIe 150W No FP32 Intel Xeon (Broadwell) No Maxwell M60 G3 g3.16xlarge 4 8 GB PCIe 150W No FP32 Intel Xeon (Broadwell) No ","categories":"","description":"A complete list of all Amazon EC2 GPU instance types on AWS that I've painstakenly compiled, because you can't find this information anywhere in AWS docs","excerpt":"A complete list of all Amazon EC2 GPU instance types on AWS that I've ‚Ä¶","ref":"/complete-list-of-all-aws-gpu-instances/","tags":"","title":"AWS GPU instances complete list"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":" Updates Latest blog post: April 20th 2023: How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation March 30th 2023: Prinston Univ workshop on PyTorch 2.0 profiling and debugging March 9th 2023: NYU Meetup on What‚Äôs new in PyTorch 2.0 Shashank Prasanna Hi! üëãüèΩ I‚Äôm a multi-disciplinary engineer, technology communicator and doodler. I ‚ù§Ô∏è machine learning, statistics, linear algebra, numerical optimization, control theory, non-linear dynamical systems (and chaos), high-performance computing (HPC) and specialized machine learning hardware (AI Accelerators). üë®üèΩ‚Äçüíª Work I‚Äôm an AI/ML Open-Source and PyTorch Developer Advocate at Meta. I‚Äôve worked at Amazon Web Services (AWS), MathWorks (MATLAB \u0026 Simulink) and NVIDIA in various roles including software development and product marketing. I find most joy in education and I put a lot of my energy in content creation and story telling.\nüèÉüèΩNot Work: I‚Äôm a recreational runner and a ‚òïÔ∏è coffee nut.\nüéì Education I‚Äôm an Electrical Engineering by training, with a specialization in Control Theory and Non-linear Dynamics. My graduate research was advised by Dr. Leon Iasemidis.\nIn my graduate thesis Directional Information Flow and Applications I discussed the application of transfer entropy (a model-free, information theoretic measure) to the detection of epileptogenic focus in the brain (origin of epileptic seizures).\n","categories":"","description":"","excerpt":" Updates Latest blog post: April 20th 2023: How Pytorch 2.0 ‚Ä¶","ref":"/","tags":"","title":""},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"}]