<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Shashank Prasanna –</title><link>/</link><description>Recent content on Shashank Prasanna</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Fri, 23 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Why I joined Modular AI?</title><link>/why-i-joined-modular-ai/</link><pubDate>Fri, 23 Jun 2023 00:00:00 +0000</pubDate><guid>/why-i-joined-modular-ai/</guid><description>
&lt;img src="/why-i-joined-modular-ai/featured-background_hu0046ad040241532c6a31c60d96bfb17a_168182_640x0_resize_catmullrom_3.png" width="640" height="388"/>
&lt;p>&lt;img src="featured-background.png" alt="">
In the past decade, I&amp;rsquo;ve had the very good fortune of working for companies that make some of the best and proven developer productivity tools. If you&amp;rsquo;re an engineer who&amp;rsquo;s built control systems for rockets, cars or robots you&amp;rsquo;ve used MATLAB and Simulink. If you&amp;rsquo;re an AI developer you likely have an NVIDIA GPU at arm&amp;rsquo;s reach (ssh to AWS EC2 counts as arm&amp;rsquo;s reach). If you&amp;rsquo;re a developer running software in production you&amp;rsquo;ve likely relied on AWS services for their scalability and and reliability for deployment. If you&amp;rsquo;re an AI researcher there is no better tool than PyTorch to go from research paper to trained model.&lt;/p>
&lt;p>What these tools have in common is that they are individually, in my personal opinion, UNPARALLELED at what they do. However, when you start stringing multiple of these tools together, then their productivity promise breaks down. You already know this if you&amp;rsquo;re training custom models using some or all of these tools. I&amp;rsquo;ve spend the last several years talking about the the role of AI software and specialized AI hardware and the challenges with using multiple frameworks and AI accelerators during development and deployment and you can read about them &lt;a href="https://shashankprasanna.com/popular_blog_posts/">in my blog posts.&lt;/a>&lt;/p>
&lt;p>If you start AI development today, you&amp;rsquo;ll need answers to these questions before you start your project since it has huge implication on what tools you will use:&lt;/p>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
&lt;h5 id="during-prototyping-and-development">During prototyping and development&lt;/h5>
&lt;/div>
&lt;div class="card-body">
&lt;p class="card-text">
1. Should you decide on the framework ahead of time (PyTorch, TensorFlow,?)
2. Should you decide where you're going to run it later (GPU, TPU, Intel Habana, AWS Silicon etc.)?
3. Should you write 5 versions of custom layer Op for each potential AI accelerators you want to target?
4. Will your code run as-is when you go from laptop to cloud? on x86 and ARM CPUs? what about GPUs? What about other AI accelerators (AWS Silicon, Intel Habana Gaudi)?
5. Will your code scale out of the box? will it scale on non-GPU AI accelerators? Will it work on all network hardware? between accelerators? between nodes?
6. If you want to write non-ML code (e.g. data processing, control algorithm etc.) will that accelerate on CPUs, GPUs, AI Accelerators? What language do you write it in?
&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
&lt;h5 id="during-production-deployment">During production deployment&lt;/h5>
&lt;/div>
&lt;div class="card-body">
&lt;p class="card-text">
1. What serving framework will you use, was it designed for production or for AI research and prototyping
2. Will it support multiple framework model formats?
3. Does it run on x86, ARM, AI accelerators for inference?
4. Can it run on embedded devices, what architectures are supported?
5. Can it do model parallel on different AI accelerators?
6. Can you meet your target throughput for desired latency at lower cost?
7. Can you monitor performance and accuracy metrics?
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>&lt;strong>It&amp;rsquo;s exhausting isn&amp;rsquo;t it?&lt;/strong>&lt;/p>
&lt;p>The disadvantage of using disparate systems is that you risk months or years of reworking, re-tooling and re-implementing when something very small changes, such as - deciding to run on different hardware: GPUs and another AI accelerators, or implement custom models for multiple different frameworks or languages.&lt;/p>
&lt;p>Unfortunately, there aren&amp;rsquo;t a great solutions for these challenges today. What we need a very small, tight set of tools, that is performant, reliable that works well together, runs everywhere and is modular. You see where I&amp;rsquo;m going with this.&lt;/p>
&lt;h2 id="enter-modular-ai">Enter Modular AI&lt;/h2>
&lt;p>And that brings me to why I decided to join Modular. I share Modular co-founders Chris Lattner (LLVM, Swift) and Tim Davis&amp;rsquo;s vision that AI tools are &lt;strong>broken today and can be better&lt;/strong> and when fixed, can infinitely improve developer productivity. &lt;strong>AI tools can be more usable. AI tools can be more accessible. AI tools can make developers more productive which will make AI itself more accessible to everyone in the world.&lt;/strong> There will be a future when you don&amp;rsquo;t have to have answers to the questions ahead of time. You start your work, introduce different hardware, introduce custom models and extensions and stuff will just work. On any system. Deployed anywhere. And you only use one language. And deploy with one runtime engine.&lt;/p>
&lt;p>If I&amp;rsquo;ve piqued your interest, come check out the products we&amp;rsquo;re building at &lt;a href="https://www.modular.com/">Modular AI&lt;/a>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mojo🔥&lt;/strong> programming language that combines the usability of Python with the performance of C. You can write your custom kernels and ops in Mojo, so your entire stack that&amp;rsquo;ll run on multiple AI accelerators is in one language which is a superset of Python!&lt;/li>
&lt;li>&lt;strong>Modular Engine&lt;/strong> is an AI inference engine built with Mojo🔥 that gives you unparalleled performance and runs PyTorch and TensorFlow models on multiple hardware.&lt;/li>
&lt;/ul>
&lt;p>Good tools make us productive, they bring us joy, the same joy you get when you find the right screw driver for that pesky loose screw, the right sized wrench, the right drill - no other makeshift tool will bring you that satisfaction of the right tool for the job. Practitioners in every domain need tools to build, create, solve, and implement more efficiently and AI is no different. &lt;strong>We need better AI tools for developers and I believe Modular will play a significant role in building them.&lt;/strong>&lt;/p>
&lt;p>We&amp;rsquo;re in early days, and Mojo and Modular Engine are not generally available yet, but you can try Mojo in the Mojo playground and see the performance speedups for yourself. &lt;a href="https://www.modular.com/get-started">Signup here to get access&lt;/a> to Mojo playground and learn more about other Modular products. Follow me on Twitter &lt;a href="https://twitter.com/shshnkp">@shshnkp&lt;/a>, read my blog posts on Medium (and here) &lt;a href="https://medium.com/@shashankprasanna">@shashank.prasanna&lt;/a> and connect with me on &lt;a href="https://www.linkedin.com/in/shashankprasanna/">LinkedIn&lt;/a> as I&amp;rsquo;ll be sharing a Modular&amp;rsquo;s journey and educational content. Thanks for reading!&lt;/p></description></item><item><title>Blog: Benchmarking Modular Mojo🔥 and PyTorch torch.compile() on Mandelbrot function</title><link>/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/</link><pubDate>Mon, 08 May 2023 00:00:00 +0000</pubDate><guid>/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/</guid><description>
&lt;img src="/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/featured-background_hu52d0dfd7da44399cff6626242a002e0e_3253184_640x0_resize_catmullrom_3.png" width="640" height="457"/>
&lt;p>Last week, Modular - an startup co-founded by Chris Lattner (of LLVM, Swift, MLIR fame), announced a brand new high-performance language called Mojo🔥. Mojo🔥 looks and reads like Python but that&amp;rsquo;s only on the surface, underneath the familiar Python syntax Mojo uses it&amp;rsquo;s own JIT and AOT compilation process to accelerate Python code. Although Mojo doesn&amp;rsquo;t fully support all of Python today, according to Mojo docs, over time Mojo is expected to become a superset of Python.
&lt;figure class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 410px">
&lt;img class="card-img-top" src="/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/featured-background_hu52d0dfd7da44399cff6626242a002e0e_3253184_400x300_fit_catmullrom_3.png" width="400" height="285">
&lt;figcaption class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
PyTorch's got some Mojo🔥
&lt;/p>
&lt;/figcaption>
&lt;/figure>
Mojo isn&amp;rsquo;t open-source yet and can only be accessed from the Mojo playground. So I promptly applied and got access to Mojo Playground a couple of days after the accouncement on May 3rd. First thing I wanted to do was to compare Mojo&amp;rsquo;s performance to PyTorch. While PyTorch is a popular framework for deep learning, its also a capable replacement for numpy as a high-performance scientific computing library.&lt;/p>
&lt;p>And my favorite feature of PyTorch is the new &lt;code>torch.compile()&lt;/code> API introduced in PyTorch 2.0 that can accelerate arbitrary functions (with limitations) written using the PyTorch API. It takes PyTorch highlevel API, optimizes it and generates C++ or GPU code to improve it&amp;rsquo;s performance. I&amp;rsquo;ve discussed &lt;code>torch.compile()&lt;/code> in great detail in my blog post, and I highly recommend reading it if you want to learn about how PyTorch compiler does operator fusion and CPU/GPU code-generation:&lt;/p>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26">How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation&lt;/a>&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>In this blog post I want to discuss relative performance between Mojo🔥 and PyTorch, and I picked the Mandelbrot example that Jeremy Howard (or FastAI fame) demoed during the Modular keynote. I reimplemnted Mojo&amp;rsquo;s Mandelbrot example in PyTorch to compared it&amp;rsquo;s performance with Mojo. Before I get into the code, here are the results.&lt;/p>
&lt;p>&lt;strong>Summary: Mojo is fast! and PyTorch is no slouch either!&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Language/Framework&lt;/th>
&lt;th>Mandelbrot execution&lt;br>(200 iterations)&lt;/th>
&lt;th>System&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>PyTorch GPU &lt;code>torch.compile()&lt;/code>&lt;/td>
&lt;td>~165 μs (micro seconds)&lt;/td>
&lt;td>Intel Core i7-9700K CPU @ 3.60GHz + NVIDIA Titan V&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Mojo CPU&lt;/td>
&lt;td>~2.6 ms&lt;/td>
&lt;td>Mojo Playground: Intel Xeon Platinum 8375C CPU @ 2.90GHz&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PyTorch CPU &lt;code>torch.compile()&lt;/code>&lt;/td>
&lt;td>~9 ms&lt;/td>
&lt;td>Intel Core i7-9700K CPU @ 3.60GHz&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PyTorch GPU&lt;/td>
&lt;td>~15 ms&lt;/td>
&lt;td>Intel Core i7-9700K CPU @ 3.60GHz + NVIDIA Titan V&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PyTorch CPU&lt;/td>
&lt;td>~50 ms&lt;/td>
&lt;td>Intel Core i7-9700K CPU @ 3.60GHz&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PyTorch Apple M2 MPS&lt;/td>
&lt;td>~70 ms&lt;/td>
&lt;td>Macbook Pro M2 Apple Silicon + 30-Core GPU + 16-Core Neural Engine&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Python/numpy&lt;/td>
&lt;td>~152 ms&lt;/td>
&lt;td>Intel Core i7-9700K CPU @ 3.60GHz&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="interesting-observations">Interesting observations&lt;/h4>
&lt;ul>
&lt;li>Mojo is the fastest CPU implementation&lt;/li>
&lt;li>PyTorch GPU with &lt;code>torch.compile()&lt;/code> generates a fused cuda kernel making it the fastest on GPU&lt;/li>
&lt;li>PyTorch CPU with &lt;code>torch.compile()&lt;/code> which generates fused C++ code is still faster than PyTorch GPU without compilation&lt;/li>
&lt;/ul>
&lt;p>It should come as no surprise that PyTorch generated custom fused kernel for Mandelbrot function running on GPU is indeed faster than Mojo CPU, it&amp;rsquo;s not even a fair comparision. PyTorch CPU is only slightly slower, but makes up for performance with better usability. Mojo is harder to use and I&amp;rsquo;m positive the UX will improve over time.&lt;/p>
&lt;h2 id="benchmarks-and-caveats">Benchmarks and caveats&lt;/h2>
&lt;p>&lt;strong>This is not a scientific benchmark test.&lt;/strong> This is a rather crude, and hacked-together-in-a-day example that should illustrate the performance differences and coding approaches, so take it with a grain of Sodium Chloride.&lt;/p>
&lt;h4 id="my-naive-testing-methodology">My naive testing methodology:&lt;/h4>
&lt;ul>
&lt;li>I use Jupyter&amp;rsquo;s native &lt;code>timeit&lt;/code> with &lt;code>10&lt;/code> repeats to benchmark and report mean and variance.&lt;/li>
&lt;li>For GPU, I call &lt;code>torch.cuda.synchronize()&lt;/code> before measurement to ensure that the kernel is fully executed.&lt;/li>
&lt;li>This code example also benchmarks tensors/arrays creation which is in the body of the function, which migh not be a feature that arises in real-world scenarios.&lt;/li>
&lt;li>I don&amp;rsquo;t measure the compilation time for PyTorch 2.0 and that does take a lot of time to generate loop-unrolled C++ code for CPU and NVPTX for GPU.&lt;/li>
&lt;/ul>
&lt;h4 id="hardware-differences">Hardware differences&lt;/h4>
&lt;p>Mojo is not open-source (yet), and the only way to run it is on the early access Mojo Playground which has a different configuration compared to my desktop running PyTorch. For the Apple M2 benchmark I also use a MacBook Pro laptop for M2 testing. Suffice to say, this is not a fair comparison. Hardware details:&lt;/p>
&lt;ul>
&lt;li>PyTorch on Desktop
&lt;ul>
&lt;li>CPU: Intel Core i7-9700K CPU @ 3.60GHz with 8 cores&lt;/li>
&lt;li>GPU: NVIDIA Titan V&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>PyTorch on Mac
&lt;ul>
&lt;li>Apple MacBook Pro with M2 Apple Silicon&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Mojo🔥 on Mojo Playground
&lt;ul>
&lt;li>Intel Xeon Platinum 8375C CPU @ 2.90GHz with 32 cores&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
&lt;strong>Update (05/09/23):&lt;/strong> Chris Lattner pointed out that Mojo doesn&amp;rsquo;t use all 32 cores on the Mojo Playground. That is mindblowing performance on 4 cores! 🤯
&lt;/div>
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;img src="chris_tweet.png" alt="">&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;/div>
&lt;h2 id="why-is-pytorch-so-much-faster-than-pythonnumpy-on-cpu">Why is PyTorch so much faster than Python/Numpy on CPU?&lt;/h2>
&lt;p>PyTorch is faster than Python/Numpy because the higher level PyTorch API calls highly optimized C++ routines implemented in the ATen library. These routines are eagerly evaluated, which means that each PyTorch API call is executed immediately which adds some function call overhead with every API call. To address this, PyTorch 2.0 introduced a compilation API called &lt;code>torch.compile()&lt;/code> which takes eager PyTorch code, optimizes it and generates C++ code with OpenMP pragmas for parallelization on CPU or generates GPU code using OpenAI Triton. This is similar to what Numba does for Python code, but PyTorch 2.0&amp;rsquo;s &lt;code>torch.compile()&lt;/code> is much more powerful because it can fuse multiple PyTorch API calls into a single kernel, which reduces function call overhead and improves performance.&lt;/p>
&lt;p>I&amp;rsquo;ve discussed this in detail in my PyTorch 2.0 blog post, but here is a screenshot of what the automatically generated fused kernels for C++ for CPU and OpenAI Triton for GPU look like for Mandelbrot function&lt;/p>
&lt;div class="alert alert-primary" role="alert">
&lt;i class='fa-brands fa-medium'>&lt;/i>
&lt;a href="https://medium.com/towards-data-science/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26">How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation&lt;/a>
&lt;/div>
&lt;div class="td-card-group card-group p-0 mb-4">
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
Autogenerated C++ code for Mandelbrot function with &lt;code>torch.compile()&lt;/code>
&lt;/div>
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;a href="mandelbrot_cpp_fused.png">&lt;img src="mandelbrot_cpp_fused.png" alt="">&lt;/a>&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
Click to zoom&lt;/div>
&lt;/div>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
Autogenerated OpenAI Triton code for Mandelbrot function with &lt;code>torch.compile()&lt;/code>
&lt;/div>
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;a href="mandelbrot_gpu_fused.png">&lt;img src="mandelbrot_gpu_fused.png" alt="">&lt;/a>&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
Click to zoom&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="baseline-benchmarking-pythonnumpy">Baseline: Benchmarking Python/Numpy&lt;/h2>
&lt;p>Let&amp;rsquo;s start with a baseline. Here&amp;rsquo;s the Python/Numpy implementation of Mandelbrot function. I&amp;rsquo;ve also included a function to plot the Mandelbrot set using matplotlib. We&amp;rsquo;ll later modify this function to use PyTorch.&lt;/p>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">numpy&lt;/span> &lt;span style="color:#204a87;font-weight:bold">as&lt;/span> &lt;span style="color:#000">np&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">matplotlib.pyplot&lt;/span> &lt;span style="color:#204a87;font-weight:bold">as&lt;/span> &lt;span style="color:#000">plt&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">matplotlib.colors&lt;/span> &lt;span style="color:#204a87;font-weight:bold">as&lt;/span> &lt;span style="color:#000">colors&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">time&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">warnings&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">def&lt;/span> &lt;span style="color:#000">mandelbrot_numpy&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">max_iter&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">200&lt;/span>&lt;span style="color:#000;font-weight:bold">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic"># Define the boundaries of the complex plane&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">xn&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">450&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">yn&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">375&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">xmin&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">-&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">2.25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">xmax&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0.75&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">ymin&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">-&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1.25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">ymax&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">1.25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic"># Create the grid of complex numbers&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">x_values&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">np&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">linspace&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">xmin&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">xmax&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">xn&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">dtype&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">np&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">float64&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">y_values&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">np&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">linspace&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">ymin&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">ymax&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">yn&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">dtype&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">np&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">float64&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">rx&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">iy&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">np&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">meshgrid&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">x_values&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">y_values&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">indexing&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;xy&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">x&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">rx&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">copy&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">y&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">iy&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">copy&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">mask&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">np&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">zeros_like&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">x&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> &lt;span style="color:#000">i&lt;/span> &lt;span style="color:#204a87;font-weight:bold">in&lt;/span> &lt;span style="color:#204a87">range&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">max_iter&lt;/span>&lt;span style="color:#000;font-weight:bold">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">x_prev&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">x&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">y_prev&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">y&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">x&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">x_prev&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">**&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">2&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">-&lt;/span> &lt;span style="color:#000">y_prev&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">**&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">2&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">+&lt;/span> &lt;span style="color:#000">rx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">y&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span>&lt;span style="color:#000">x_prev&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span>&lt;span style="color:#000">y_prev&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">+&lt;/span> &lt;span style="color:#000">iy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">inside&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">np&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">sqrt&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">x&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">**&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">2&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">+&lt;/span> &lt;span style="color:#000">y&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">**&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;lt;=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">mask&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">+=&lt;/span>&lt;span style="color:#000">inside&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">return&lt;/span> &lt;span style="color:#000">mask&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">def&lt;/span> &lt;span style="color:#000">make_plot_python&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">m&lt;/span>&lt;span style="color:#000;font-weight:bold">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">xn&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">450&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">yn&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">375&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">dpi&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">32&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">width&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">5&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">height&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">5&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span> &lt;span style="color:#000">yn&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">//&lt;/span> &lt;span style="color:#000">xn&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">fig&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">plt&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">figure&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#000">width&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">height&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#000">dpi&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">dpi&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">ax&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">fig&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">add_axes&lt;/span>&lt;span style="color:#000;font-weight:bold">([&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0.0&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0.0&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">1.0&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">1.0&lt;/span>&lt;span style="color:#000;font-weight:bold">],&lt;/span> &lt;span style="color:#000">frame_on&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#204a87;font-weight:bold">False&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">aspect&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">light&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">colors&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">LightSource&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">315&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">10&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">image&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">light&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">shade&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">m&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">plt&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">cm&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">hot&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">colors&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">PowerNorm&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0.3&lt;/span>&lt;span style="color:#000;font-weight:bold">),&lt;/span> &lt;span style="color:#000">blend_mode&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;hsv&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">vert_exag&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1.5&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">plt&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">imshow&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">image&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">plt&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">axis&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;off&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">plt&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">show&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>Output:&lt;/strong>
With a baseline established let&amp;rsquo;s compare the performance of PyTorch and Mojo.
&lt;img src="mandel_numpy.png" alt="">&lt;/p>
&lt;h2 id="benchmarking-mojo-on-the-mojo-playground">Benchmarking Mojo on the Mojo playground&lt;/h2>
&lt;p>This comparision is a bit unfair because Mojo🔥 playground has a 32 core Xeon CPU, but I only have a modest 4 year old 8 core desktop CPU. The clock frequency, memory bandwidth and cache sizes and number of cores are all different, but I can&amp;rsquo;t install PyTorch on Mojo playground so this is the best I can do for comparision for now.&lt;/p>
&lt;p>At the bottom of this notebook, I add these few lines of code to measure the execution time of the mandelbrot function.&lt;/p>
&lt;p>&lt;img src="mojo_playground.png" alt="">&lt;/p>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">from&lt;/span> &lt;span style="color:#000">Time&lt;/span> &lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">now&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">let&lt;/span> &lt;span style="color:#000">eval_begin&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">now&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">let&lt;/span> &lt;span style="color:#000">mandelbrot_set&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">compute_mandelbrot_simd&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">let&lt;/span> &lt;span style="color:#000">eval_end&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">now&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">let&lt;/span> &lt;span style="color:#000">execution_time&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">eval_end&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">-&lt;/span> &lt;span style="color:#000">eval_begin&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">print&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;execution_time:&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">print&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">F64&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">execution_time&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">/&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">1000000&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>Output&lt;/strong>
I found it difficult to benchmark Mojo, because Python benchmarking tools don&amp;rsquo;t work very readily. I ran the above code several times to ensure that the results are consistent.
&lt;img src="benchmark_mojo.png" alt="">&lt;/p>
&lt;h2 id="benchmarking-pytorch-cpu">Benchmarking PyTorch CPU&lt;/h2>
&lt;p>To update our mandelbrot function from numpy implementation to PyTorch implementation I made the following small changes&lt;/p>
&lt;ul>
&lt;li>replace &lt;code>np&lt;/code> with &lt;code>torch&lt;/code>&lt;/li>
&lt;li>add &lt;code>device=device&lt;/code> to the tensor creation calls, this allows us to pass the appropriate CPU, GPU or Apple MPS device to PyTorch.&lt;/li>
&lt;/ul>
&lt;p>Updated mandelbrot function:&lt;/p>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">def&lt;/span> &lt;span style="color:#000">mandelbrot_pytorch&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">device&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;cpu&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">max_iter&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">200&lt;/span>&lt;span style="color:#000;font-weight:bold">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic"># Define the boundaries of the complex plane&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">xn&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">450&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">yn&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">375&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">xmin&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">-&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">2.25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">xmax&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">0.75&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">ymin&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">-&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1.25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">ymax&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">1.25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8f5902;font-style:italic"># Create the grid of complex numbers&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">x_values&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">torch&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">linspace&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">xmin&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">xmax&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">xn&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">device&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">device&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">y_values&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">torch&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">linspace&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">ymin&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">ymax&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">yn&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">device&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">device&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">rx&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">iy&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">torch&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">meshgrid&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">x_values&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">y_values&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">indexing&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;xy&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">x&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">rx&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">clone&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">y&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">iy&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">clone&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">mask&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">torch&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">zeros_like&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">x&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span> &lt;span style="color:#000">device&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">device&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> &lt;span style="color:#000">i&lt;/span> &lt;span style="color:#204a87;font-weight:bold">in&lt;/span> &lt;span style="color:#204a87">range&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">max_iter&lt;/span>&lt;span style="color:#000;font-weight:bold">):&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">x_prev&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">x&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">y_prev&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">y&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">x&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">x_prev&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">**&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">2&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">-&lt;/span> &lt;span style="color:#000">y_prev&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">**&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">2&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">+&lt;/span> &lt;span style="color:#000">rx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">y&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span>&lt;span style="color:#000">x_prev&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">*&lt;/span>&lt;span style="color:#000">y_prev&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">+&lt;/span> &lt;span style="color:#000">iy&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">inside&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">torch&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">sqrt&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">x&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">**&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">2&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">+&lt;/span> &lt;span style="color:#000">y&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">**&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">&amp;lt;=&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">mask&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">+=&lt;/span>&lt;span style="color:#000">inside&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">return&lt;/span> &lt;span style="color:#000">mask&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Now let&amp;rsquo;s call our &lt;code>mandelbrot_pytorch&lt;/code> function with &lt;code>device='cpu'&lt;/code>
&lt;img src="mandel_pytorch_cpu.png" alt="">&lt;/p>
&lt;h2 id="benchmarking-pytorch-cpu-with-torchcompile">Benchmarking PyTorch CPU with &lt;code>torch.compile()&lt;/code>&lt;/h2>
&lt;p>You can compile PyTorch functions using &lt;code>torch.compile()&lt;/code> and TorchInductor will optimize and generate C++ code with OpenMP pragmas for parallization. This will significantly improve the performance of the function. All of this happens under the hood when you run the code below, but you can pass an additional argument &lt;code>options={'trace.enabled':True}&lt;/code> to see the generated code. I discuss this is further detail in my PyTorch 2.0 blog post:
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26">How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation&lt;/a>&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;/div>
&lt;/p>
&lt;p>Let&amp;rsquo;s compile our mandelbrot function for CPU backend:&lt;/p>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">device&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;cpu&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">mandelbrot_compiled&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">torch&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">compile&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">mandelbrot_pytorch&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">mandelbrot_set&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">mandelbrot_compiled&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">device&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">make_plot_python&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">mandelbrot_set&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">numpy&lt;/span>&lt;span style="color:#000;font-weight:bold">())&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>Output&lt;/strong>
&lt;img src="mandel_pytorch_cpu_compile.png" alt="">&lt;/p>
&lt;p>The generated C++ code looks like this&lt;/p>
&lt;figure class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 773px">
&lt;img class="card-img-top" src="/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/mandelbrot_cpp_fused_hu7e430cbf97eb3e8735ed525bf4b9a7bc_268273_800x400_fit_catmullrom_3.png" width="763" height="400">
&lt;figcaption class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="benchmarking-pytorch-gpu">Benchmarking PyTorch GPU&lt;/h2>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">device&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;cuda&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">mandelbrot_set&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">mandelbrot_pytorch&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">device&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">make_plot_python&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">mandelbrot_set&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">cpu&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">numpy&lt;/span>&lt;span style="color:#000;font-weight:bold">())&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>Output&lt;/strong>
&lt;img src="mandel_pytorch_gpu.png" alt="">&lt;/p>
&lt;h2 id="benchmarking-pytorch-gpu-with-torchcompile">Benchmarking PyTorch GPU with &lt;code>torch.compile()&lt;/code>&lt;/h2>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">device&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;cuda&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">mandelbrot_compiled&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">torch&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">compile&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">mandelbrot_pytorch&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">mandelbrot_set&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">mandelbrot_compiled&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">device&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">make_plot_python&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">mandelbrot_set&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">cpu&lt;/span>&lt;span style="color:#000;font-weight:bold">()&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">numpy&lt;/span>&lt;span style="color:#000;font-weight:bold">())&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="mandel_pytorch_gpu_compile.png" alt="">&lt;/p>
&lt;p>The generated OpenAI Triton code which gets compiled to NVPTX for GPUs, looks like this&lt;/p>
&lt;figure class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 728px">
&lt;img class="card-img-top" src="/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/mandelbrot_gpu_fused_huc397d05ec5af180dad415181b17e01e2_296767_800x400_fit_catmullrom_3.png" width="718" height="400">
&lt;figcaption class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="benchmarking-pytorch-apple-m1m2-silicon-with-mps-support">Benchmarking PyTorch Apple M1/M2 Silicon with MPS support&lt;/h2>
&lt;p>&lt;img src="mandel_pytorch_apple_m2.png" alt="">&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>That&amp;rsquo;s it folks! I hope you enjoyed this quick comparision of PyTorch and Mojo🔥.
Mojo is fast, but doesn&amp;rsquo;t have the same level of usability of PyTorch, but that may just be just a matter of time and community support. PyTorch&amp;rsquo;s one-two punch combo of eager mode with high-level Tensor API and compilation with &lt;code>torch.compile()&lt;/code> is a powerful combination today. However, the PyTorch ecosystem is also quite fragmented with multiple code paths for different accelerators: TorchInductor (GPUs, CPUs), XLA (TPU, AWS Tranium/Inferentia), custom bindings/bridges (Intel Habana, MPS), and some accelerators like GPUs implement all paths. This leaves the end user and hardware vendors in a dilemma.&lt;/p>
&lt;p>One of the main benefit of Mojo that I see is the ability to write OpenAI Triton style kernel code in the Python language with fast or faster then C++ performance. This would make supporting custom ops for inference easier. We&amp;rsquo;re certainly living in an exciting times for AI Infra, AI accelerators and AI frameworks. Maybe we&amp;rsquo;re at the cusp of another LLVM moment but for AI.&lt;/p>
&lt;p>If you enjoyed reading this, check out my other blog posts on &lt;a href="https://medium.com/@shashankprasanna">Medium&lt;/a> or reach out to me on social media, links are on the homepage. Cheers!&lt;/p></description></item><item><title>Blog: How Pytorch 2.0 accelerates deep learning with operator fusion and CPU/GPU code-generation</title><link>/how-pytorch-2.0-accelerates-deep-learning-with-operator-fusion-and-cpu/gpu-code-generation/</link><pubDate>Sun, 30 Apr 2023 00:00:00 +0000</pubDate><guid>/how-pytorch-2.0-accelerates-deep-learning-with-operator-fusion-and-cpu/gpu-code-generation/</guid><description>
&lt;img src="/how-pytorch-2.0-accelerates-deep-learning-with-operator-fusion-and-cpu/gpu-code-generation/featured_hu0fb5b15bba115eaa0a2341edd31b99b2_23922_640x0_resize_catmullrom_3.png" width="640" height="367"/>
&lt;p>Computer programming is magical. We write code in human readable languages, and as though by magic, it gets translated into electric currents through silicon transistors making them behave like switches and allowing them to implement complex logic — just so we can enjoy cat videos on the internet. Between the programming language and hardware processors that run it, is an important piece of technology — the compiler. A compiler’s job is to translate and simplify our human readable language code into instructions that a processor understands.&lt;/p>
&lt;p>Compilers play a very important role in deep learning to improve training and inference performance, improve energy efficiency, and target diverse AI accelerator hardware. In this blog post I’m going to discuss deep learning compiler technologies that powers PyTorch 2.0. I’ll walk you through the different phases of the compilation process and discuss various underlying technologies with code examples and visualizations.&lt;/p>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
Read the full blog post here: &lt;br>&lt;br> &lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26">How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation&lt;/a>&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
&lt;img src="featured.png" alt="">&lt;/div>
&lt;/div></description></item><item><title>Blog: What is an AI accelerator?</title><link>/what-is-an-ai-accelerator/</link><pubDate>Wed, 11 Jan 2023 00:00:00 +0000</pubDate><guid>/what-is-an-ai-accelerator/</guid><description>
&lt;img src="/what-is-an-ai-accelerator/ai-hardware-spectrum-featured_hu153888cb09e42ad7fc9ab1679dae3388_33262_640x0_resize_catmullrom_3.png" width="640" height="361"/>
&lt;!-- ![](ai-hardware-spectrum-featured.png) -->
&lt;img src="ai-hardware-spectrum-featured.png" width="800"/>
&lt;p>An AI accelerator is a dedicated processor designed to accelerate machine learning computations. Machine learning, and particularly its subset, deep learning is primarily composed of a large number of linear algebra computations, (i.e. matrix-matrix, matrix-vector operations) and these operations can be easily parallelized. AI accelerators are specialized hardware designed to accelerate these basic machine learning computations and improve performance, reduce latency and reduce cost of deploying machine learning based applications.&lt;/p>
&lt;h3 id="why-do-we-need-specialized-ai-accelerators">Why do we need specialized AI accelerators?&lt;/h3>
&lt;p>The two most important reasons for building dedicated processors for machine learning are:&lt;/p>
&lt;ol>
&lt;li>Energy efficiency&lt;/li>
&lt;li>Faster performance&lt;/li>
&lt;/ol>
&lt;p>Recent trends to improve model accuracy, have been to introduce larger models with more parameters and train them on larger data sets. As model sizes get larger, and current processors won’t be able to deliver the processing power needed to train or run inference on these models under tight time-to-train and inference latency requirements.&lt;/p>
&lt;div class="alert alert-primary" role="alert">
&lt;em>General purpose processors like CPUs trade-off energy efficiency for versatility and special purpose processors (AI accelerators) trade off versatility for energy efficiency.&lt;/em>
&lt;/div>
&lt;p>AI accelerators on the other hand can be designed with features to minimize memory access, offer larger on-chip cache and include dedicated hardware features to accelerate matrix-matrix computations. Since AI accelerators are purpose built devices it is “aware” of the algorithms that it runs on and its dedicated features will run it more efficiently than a general purpose processor.&lt;/p>
&lt;p>List of popular AI accelerators for &lt;strong>training&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>NVIDIA GPUs&lt;/strong>: Available on AWS, GCP, Azure and at your local computer store (See my recommendation list on the left menu)&lt;/li>
&lt;li>&lt;strong>AWS Tranium&lt;/strong>: Available on AWS&lt;/li>
&lt;li>&lt;strong>Intel Habana Gaudi&lt;/strong>: Available on AWS (v1) and Intel DevCloud (v1 and v2)&lt;/li>
&lt;li>&lt;strong>Google Cloud TPUs&lt;/strong>: Available on GCP and via Colab (v1-v4)&lt;/li>
&lt;/ul>
&lt;p>List of popular AI accelerators for &lt;strong>inference&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>NVIDIA GPUs&lt;/strong>: Available on AWS, GCP, Azure (See my recommendation list on the left menu)&lt;/li>
&lt;li>&lt;strong>AWS Inferentia&lt;/strong>: Available on AWS (See my recommend blog post below)&lt;/li>
&lt;li>&lt;strong>Intel Habana Gaudi&lt;/strong>: Available on AWS and Intel DevCloud (v1 and v2)&lt;/li>
&lt;li>&lt;strong>Google Cloud TPUs&lt;/strong>: Available on GCP and via Colab (v1-v4)&lt;/li>
&lt;/ul>
&lt;div class="alert alert-primary" role="alert">
Note: Modern GPUs have dedicated silicon (TensorCores) and precision types (TF32, BF16) designed for deep learning bringing them closer to dedicated AI accelerators vs. general purpose parallel processors
&lt;/div>
&lt;h3 id="recommended-blog-posts">Recommended blog posts&lt;/h3>
&lt;div class="td-card-group card-group p-0 mb-4">
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;img src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*SAgQSIEprO1looCxAdf52w.png" alt="">&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/ai-accelerators-machine-learning-algorithms-and-their-co-design-and-evolution-2676efd47179">AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution&lt;/a>&lt;/div>
&lt;/div>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*AGpm_2l-32AfXUAfOxwUKA.png" alt="">&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c">A complete guide to AI accelerators for deep learning inference — GPUs, AWS Inferentia and Amazon Elastic Inference&lt;/a>&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Blog: AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution</title><link>/ai-accelerators-and-machine-learning-algorithms-co-design-and-evolution/</link><pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate><guid>/ai-accelerators-and-machine-learning-algorithms-co-design-and-evolution/</guid><description>
&lt;img src="/ai-accelerators-and-machine-learning-algorithms-co-design-and-evolution/featured_hu153888cb09e42ad7fc9ab1679dae3388_33262_640x0_resize_catmullrom_3.png" width="640" height="361"/>
&lt;p>If you told me a few years ago that data scientists would be using Docker containers in their day to day work, I wouldn’t have believed you. As a member of the broader machine learning (ML) community I always considered Docker, Kubernetes, Swarm (remember that?) exotic infrastructure tools for IT/Ops experts. Today it’s a different story, rarely a day goes by when I don’t use a Docker container for training or hosting a model.
An attribute of machine learning development that makes it different from traditional software development is that it relies on specialized hardware such as GPUs, Habana Gaudi, AWS Inferentia to accelerate training and inference. This makes it challenging to have containerized deployments that are hardware-agnostic, which is one of the key benefits of containers. In this blog post I’ll discuss how Docker and container technologies have evolved to address this challenge. We’ll discuss:&lt;/p>
&lt;ul>
&lt;li>Why Docker has become an essential tool for machine learning today and how it addresses machine learning specific challenges&lt;/li>
&lt;li>How Docker accesses specialized hardware resources on heterogeneous systems that have more than one type of processor (CPU + AI accelerators).&lt;/li>
&lt;li>How different AI accelerators extend Docker for hardware access with examples of 1/ NVIDIA GPUs and NVIDIA Container Toolkit and 2/ AWS Inferentia and Neuron SDK support for containers&lt;/li>
&lt;li>How to scale Docker containers on Kubernetes with hardware accelerated nodes&lt;/li>
&lt;/ul>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
Read the full blog post here: &lt;br>&lt;br> &lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators-e076c6eb7802">How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators&lt;/a>&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
&lt;img src="featured.png" alt="">&lt;/div>
&lt;/div></description></item><item><title>Blog: How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators</title><link>/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators/</link><pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate><guid>/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators/</guid><description>
&lt;img src="/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators/featured_hu40258b408388e57d94159bf91c739a7c_102531_640x0_resize_catmullrom_3.png" width="640" height="285"/>
&lt;p>If you told me a few years ago that data scientists would be using Docker containers in their day to day work, I wouldn’t have believed you. As a member of the broader machine learning (ML) community I always considered Docker, Kubernetes, Swarm (remember that?) exotic infrastructure tools for IT/Ops experts. Today it’s a different story, rarely a day goes by when I don’t use a Docker container for training or hosting a model.
An attribute of machine learning development that makes it different from traditional software development is that it relies on specialized hardware such as GPUs, Habana Gaudi, AWS Inferentia to accelerate training and inference. This makes it challenging to have containerized deployments that are hardware-agnostic, which is one of the key benefits of containers. In this blog post I’ll discuss how Docker and container technologies have evolved to address this challenge. We’ll discuss:&lt;/p>
&lt;ul>
&lt;li>Why Docker has become an essential tool for machine learning today and how it addresses machine learning specific challenges&lt;/li>
&lt;li>How Docker accesses specialized hardware resources on heterogeneous systems that have more than one type of processor (CPU + AI accelerators).&lt;/li>
&lt;li>How different AI accelerators extend Docker for hardware access with examples of 1/ NVIDIA GPUs and NVIDIA Container Toolkit and 2/ AWS Inferentia and Neuron SDK support for containers&lt;/li>
&lt;li>How to scale Docker containers on Kubernetes with hardware accelerated nodes&lt;/li>
&lt;/ul>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
Read the full blog post here:&lt;br>&lt;br> &lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/ai-accelerators-machine-learning-algorithms-and-their-co-design-and-evolution-2676efd47179">How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators&lt;/a>&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
&lt;img src="featured.png" alt="">&lt;/div>
&lt;/div></description></item><item><title>Blog: Automatically generate code and documentation using OpenAI CODEX</title><link>/automatically-generate-code-and-documentation-using-openai-codex/</link><pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate><guid>/automatically-generate-code-and-documentation-using-openai-codex/</guid><description>
&lt;img src="/automatically-generate-code-and-documentation-using-openai-codex/screenshot8-featured_huc7ef144ea84a250195ff19b8f35ca83f_78825_640x0_resize_catmullrom_3.png" width="640" height="271"/>
&lt;p>We&amp;rsquo;ve all dealt with the frustration of poor or incomplete documentation in software projects. In their &lt;a href="https://octoverse.github.com/creating-documentation/">2021 state of the Octoverse survey&lt;/a>, GitHub found that easy to use documentation, boosted developer productivity by 50% and improved contribution quality, yet it continues to be an under-invested area across open-source projects.&lt;/p>
&lt;p>Using OpenAI Codex you can use to automatically edit existing code to add documentation using only natural language instructions. This new feature will let you spend more time developing new features and reviewing generated documentation instead of writing documentation from scratch. In addition to generating documentation, Codex&amp;rsquo;s edit feature can refactor code, update logic, translate between programming languages and change coding styles.&lt;/p>
&lt;p>In this blog post, we&amp;rsquo;ll take a look at how Codex edit feature works through a code example. One of my favorite areas of machine learning (ML) research is automated machine learning (AutoML) or low-code ML. I&amp;rsquo;ll use Codex to implement a simple data science workflow only using natural language instructions and without writing any ML code. We&amp;rsquo;ll perform the following steps only using Codex&amp;rsquo;s Edit API and discuss its features as we implement the code example:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Generate python code to load a dataset and visualize it&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Generate python code to train a machine learning classifier on the dataset&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Edit generated code to customize it for the specific problem dataset&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Edit updated code to add detailed documentation&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Sounds exciting? Let&amp;rsquo;s get started!&lt;/p>
&lt;h2 id="using-openai-codex-edit-feature-to-generate-fully-documented-code-to-train-a-classifier-on-a-tabular-dataset">Using OpenAI Codex edit feature to generate fully documented code to train a classifier on a tabular dataset&lt;/h2>
&lt;p>Let&amp;rsquo;s use OpenAI Codex to solve the &lt;a href="https://www.kaggle.com/c/titanic/data">Titanic - Machine Learning from Disaster&lt;/a> Kaggle competition. The titanic dataset consists of passenger information like name, age, gender, etc. and if they survived the disaster or not. We&amp;rsquo;ll use natural language prompts to iteratively build code and documentation. To download the dataset, you&amp;rsquo;ll need to create a free Kaggle account and download it either via the web page or by following the instructions on the &lt;a href="https://github.com/Kaggle/kaggle-api">Kaggle API GitHub page&lt;/a>.&lt;/p>
&lt;p>Next, you&amp;rsquo;ll need an Open AI account. If you&amp;rsquo;re new to OpenAI, head over to &lt;a href="https://openai.com/api/login/">https://openai.com/api/login/&lt;/a> and create your free trial account. This will give you access to $18 in free credit that you can use for upto 3 months.&lt;/p>
&lt;p>After you&amp;rsquo;ve created your account, you&amp;rsquo;ll have access to your unique API keys that you can use to invoke the OpenAI API using the official &lt;a href="https://beta.openai.com/docs/libraries/python-bindings">Python&lt;/a>, Node.js or any of the &lt;a href="https://beta.openai.com/docs/libraries/community-libraries">community supported libraries&lt;/a>. Since our example generates Python code, we&amp;rsquo;ll use the Python bindings.&lt;/p>
&lt;h3 id="step-1-setting-up-openai-python-package">Step 1: Setting up OpenAI Python package&lt;/h3>
&lt;p>Open your favorite IDE to get started, I prefer Jupyter Lab as it&amp;rsquo;s the most popular IDE used by data scientists. Import openai Python package and specify your organization and API keys. You can find these on your Open AI account.&lt;/p>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">os&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">openai&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">openai&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">organization&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;org-XXXXXXXXXXXXXXX&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">openai&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">api_key&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;sk-XXXXXXXXXXXXXXXX&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="step-2-generate-python-code-to-load-an-analyze-the-dataset">Step 2: Generate python code to load an analyze the dataset&lt;/h3>
&lt;p>We can use the Codex edit feature to edit existing code or generate new code. Let&amp;rsquo;s start by generating some fresh code to load our Titanic CSV dataset and analyze it.&lt;/p>
&lt;p>Here&amp;rsquo;s what the arguments to the create() function mean:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Input&lt;/strong>: Provide a prompt as a starting point. We leave it empty since we&amp;rsquo;re going to generate new code and not edit existing code.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Instructions&lt;/strong>: Provide instructions to Codex on what code to generate. Here we describe that we want to generate code to load and analyze our CSV dataset.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Engine&lt;/strong>: Provide the engine for Codex edit feature, the latest name and version is &lt;code>Code-davinci-edit-001&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Temperature&lt;/strong> and &lt;strong>top_p&lt;/strong>: used to control how deterministic the model is in generating a response. I choose a lower value for top_p (=0.2) to get a consistent response.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>You can find more information about the API in the &lt;a href="https://beta.openai.com/docs/api-reference/edits/create">Edits documentation&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">from&lt;/span> &lt;span style="color:#000">IPython.display&lt;/span> &lt;span style="color:#204a87;font-weight:bold">import&lt;/span> &lt;span style="color:#000">display&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">input_prompt&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&amp;#39;&amp;#39;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">instruction&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">Write python function to load a CSV file called titanic.csv into a dataframe and use display function&amp;#39;&amp;#39;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">response&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">openai&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Edit&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">create&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#204a87">input&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">input_prompt&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">instruction&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">instruction&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">engine&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;code-davinci-edit-001&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">temperature&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">top_p&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0.2&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>Output:&lt;/strong>&lt;/p>
&lt;p>When you run the code, you should see an output that looks like the screenshot below. From the output we can see that the dataset has both numeric and categorical variables. This information will come in handy later when we ask Codex to edit our function. For now, let&amp;rsquo;s continue to build on this program to classify the dataset.&lt;/p>
&lt;p>&lt;img src="screenshot1.png" alt="">&lt;/p>
&lt;h3 id="step-3-generate-a-python-program-to-classify-the-dataset">Step 3: Generate a Python program to classify the dataset&lt;/h3>
&lt;p>In this step, rather than leave the input prompt empty, let&amp;rsquo;s provide the function signature we expect so Codex can fill out the rest of the program. Under instructions we provide:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;Write python function to load a CSV file and perform binary XGBoost classification on a target column:&lt;/li>
&lt;/ul>
&lt;p>Extending our previous example, we&amp;rsquo;ll ask Codex to generate a program to use XGBoost to classify our dataset based on a specified target variable. If you&amp;rsquo;re new to machine learning, XGBoost is a fantastic works-out-of-the-box classifier for tabular datasets that often requires little to no fine tuning to get acceptable results.&lt;/p>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">input_prompt&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">def csv_classification(csv_file, target_column):
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&amp;#39;&amp;#39;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">instruction&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">Write python function to load a CSV file and perform binary XGBoost classification on a target column&amp;#39;&amp;#39;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">response&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">openai&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Edit&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">create&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#204a87">input&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">input_prompt&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">instruction&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">instruction&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">engine&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;code-davinci-edit-001&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">temperature&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">top_p&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0.2&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">generated_code&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">response&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;choices&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">][&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>&lt;span style="color:#000;font-weight:bold">][&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;text&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">print&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">generated_code&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>Output:&lt;/strong>&lt;/p>
&lt;p>Codex generates the following code to classify our dataset using XGBoost. If you&amp;rsquo;re new to machine learning, let&amp;rsquo;s just take a moment to appreciate how much time we saved by having Codex generate this vs. writing this from scratch. We still need to test if this code works, so let&amp;rsquo;s go ahead and do that in the next step.&lt;/p>
&lt;p>&lt;img src="screenshot2.png" alt="">&lt;/p>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">exec&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">generated_code&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">csv_classification&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;titanic.csv&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;Survived&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>Output:&lt;/strong>&lt;/p>
&lt;p>Oh no! the generated code throws an error! Not what we want to see, but let&amp;rsquo;s take a closer look at the error message. The error message says:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>We have categorical variables that need to be specified as categorical variables. If we go back to the output of Step 1, we can verify from that the Name, Sex, Ticket, Cabin and Embarked are indeed categorical.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Since we have categorical variables, we need to instruct XGBoost to enable categorical support using `enable_categorical` argument.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>To remedy this, let&amp;rsquo;s make Codex do all the hard work by simply copying the error message and including it in the instructions of the next step.&lt;/p>
&lt;p>&lt;img src="screenshot3.png" alt="">&lt;/p>
&lt;h3 id="step-4-fixing-errors-in-the-generated-code-by-updating-the-instructions">Step 4: Fixing errors in the generated code by updating the instructions&lt;/h3>
&lt;p>In this step, we start with the generated code from Step 3, and provide instructions to make changes specified in the error message. We provide the following instructions:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;Update code to set dataframe columns Name, Sex, Ticket, Cabin, Embarked to categorical and set DMatrix parameter 'enable_categorical' to 'True'&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">input_prompt&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">generated_code&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">instruction&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">Update code to set dataframe columns Name, Sex, Ticket, Cabin, Embarked to categorical and set DMatrix parameter &amp;#39;enable_categorical&amp;#39; to &amp;#39;True&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&amp;#39;&amp;#39;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">response&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">openai&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Edit&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">create&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#204a87">input&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">input_prompt&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">instruction&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">instruction&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">engine&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;code-davinci-edit-001&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">temperature&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">top_p&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0.2&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">generated_code&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">response&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;choices&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">][&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>&lt;span style="color:#000;font-weight:bold">][&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;text&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">print&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">generated_code&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>Output:&lt;/strong>&lt;/p>
&lt;p>This generates updated code that addresses the categorical variable errors by converting the specified column into categorical and by setting enable_categorical to True when creating the XGBoost&amp;rsquo;s native DMatrix data structure. Again let&amp;rsquo;s take a moment to appreciate how little we had to do to generate code for a generic tabular data classifier using XGBoost.&lt;/p>
&lt;p>&lt;img src="screenshot4.png" alt="">&lt;/p>
&lt;p>Let&amp;rsquo;s go ahead and execute this code to train our classifier:&lt;/p>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">exec&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">generated_code&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">csv_classification&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;titanic.csv&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>&lt;span style="color:#4e9a06">&amp;#39;Survived&amp;#39;&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>Output:&lt;/strong>&lt;/p>
&lt;p>Sure enough, our code now runs without any issues and trains the classifier to 83% accuracy on the test set. Note that this is a very simple dataset, but the generated code can now be used for a number of tabular classification problems. Need to modify the code to change features? Just repeat the instructions in this step to specify what needs to be updated.&lt;/p>
&lt;p>&lt;img src="screenshot5.png" alt="">&lt;/p>
&lt;h3 id="step-5-generate-detailed-documentation">Step 5: Generate detailed documentation&lt;/h3>
&lt;p>Finally, let&amp;rsquo;s use the generated code and add detailed documentation to it using Codex. To do that we provide natural language instructions on how and where to add comments:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;Add a detailed paragraph at the top of the code describing what the code is doing, and add detailed comments explaining every line of code&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">instruction&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#4e9a06">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">Add a detailed paragraph at the top of the code describing what the code is doing, and add detailed comments explaining every line of code
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&amp;#39;&amp;#39;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">response&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">openai&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">Edit&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">.&lt;/span>&lt;span style="color:#000">create&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#204a87">input&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">generated_code&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">instruction&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">instruction&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">engine&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;code-davinci-edit-001&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">temperature&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#000">top_p&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0.2&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">documented_code&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">response&lt;/span>&lt;span style="color:#000;font-weight:bold">[&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;choices&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">][&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>&lt;span style="color:#000;font-weight:bold">][&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;text&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">]&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">print&lt;/span>&lt;span style="color:#000;font-weight:bold">(&lt;/span>&lt;span style="color:#000">documented_code&lt;/span>&lt;span style="color:#000;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>Output:&lt;/strong>&lt;/p>
&lt;p>Here you can see the detailed description of the full program, you can also see that the additional comments were automatically added under the function definition describing each line of code.&lt;/p>
&lt;p>&lt;img src="screenshot6.png" alt="">&lt;/p>
&lt;p>If you&amp;rsquo;re new to XGBoost, you can also use the generated documentation to learn about what each of the hyperparameters mean! Now you know that eta is actually the learning rate, and max_depth is the maximum depth of each of the 20 trees XGBoost uses. How cool is that?&lt;/p>
&lt;p>&lt;img src="screenshot7.png" alt="">&lt;/p>
&lt;p>We now have a fully functional and documented code that you can use to submit your results to the Titanic Kaggle competition!&lt;/p>
&lt;h2 id="now-its-your-turn">Now it&amp;rsquo;s your turn!&lt;/h2>
&lt;p>If this piqued your interest head on over to &lt;a href="https://openai.com/api/login/">https://openai.com/api/login/&lt;/a> and create your free trial account. I&amp;rsquo;m making all the code used in this blog post is available as a Jupyter notebook on GitHub so you can run the entire example with a single click&lt;/p>
&lt;p>Now it&amp;rsquo;s your turn! How will you use Codex&amp;rsquo;s new edit feature? Let me know by reaching out to me at &lt;a href="https://twitter.com/shshnkp">@shshnkp&lt;/a>
&lt;img src="screenshot8-featured.png" alt="">&lt;/p></description></item><item><title>Blog: AWS GPU instances complete list</title><link>/complete-list-of-all-aws-gpu-instances/</link><pubDate>Thu, 02 Jun 2022 00:00:00 +0000</pubDate><guid>/complete-list-of-all-aws-gpu-instances/</guid><description>
&lt;p>Here is a complete list of all Amazon EC2 GPU instance types on AWS that I&amp;rsquo;ve painstakenly compiled, because you can&amp;rsquo;t find this information anywhere on AWS.
In the tabular format below, you&amp;rsquo;ll find more detailed information about GPU type, interconnect, Thermal design power (TDP), precision types supported etc.&lt;/p>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;img src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*1qFKgyph2CT2Dxat3tSqmQ.png" alt="">&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
From my blog post: &lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86">Choosing the right GPU for deep learning on AWS&lt;/a>&lt;/div>
&lt;/div>
&lt;h2 id="tabular-format">Tabular format&lt;/h2>
&lt;p>&lt;em>With more information than you were probably looking for&lt;/em> 😊&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Architecture&lt;/th>
&lt;th>NVIDIA GPU&lt;/th>
&lt;th>Instance type&lt;/th>
&lt;th>Instance name&lt;/th>
&lt;th>Number of GPUs&lt;/th>
&lt;th>GPU Memory (per GPU)&lt;/th>
&lt;th>GPU Interconnect (NVLink / PCIe)&lt;/th>
&lt;th>Thermal&lt;br>Design Power (TDP) from nvidia-smi&lt;/th>
&lt;th>Tensor Cores (mixed-precision)&lt;/th>
&lt;th>Precision Support&lt;/th>
&lt;th>CPU Type&lt;/th>
&lt;th>Nitro based&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A100&lt;/td>
&lt;td>P4&lt;/td>
&lt;td>p4d.24xlarge&lt;/td>
&lt;td>8&lt;/td>
&lt;td>40 GB&lt;/td>
&lt;td>NVLink gen 3 (600 GB/s)&lt;/td>
&lt;td>400W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.2xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.4xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.8xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.16xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.12xlarge&lt;/td>
&lt;td>4&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.24xlarge&lt;/td>
&lt;td>4&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ampere&lt;/td>
&lt;td>A10G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5.48xlarge&lt;/td>
&lt;td>8&lt;/td>
&lt;td>24 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 3)&lt;/td>
&lt;td>FP64, FP32, FP16, INT8, BF16, TF32&lt;/td>
&lt;td>AMD EPYC&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5g.xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>AWS Graviton2&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5g.2xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>AWS Graviton2&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5g.4xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>AWS Graviton2&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5g.8xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>AWS Graviton2&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5g.16xlarge&lt;/td>
&lt;td>2&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>AWS Graviton2&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4G&lt;/td>
&lt;td>G5&lt;/td>
&lt;td>g5g.metal&lt;/td>
&lt;td>2&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>AWS Graviton2&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.2xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.4xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.8xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.16xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.12xlarge&lt;/td>
&lt;td>4&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Turing&lt;/td>
&lt;td>T4&lt;/td>
&lt;td>G4&lt;/td>
&lt;td>g4dn.metal&lt;/td>
&lt;td>8&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>70W&lt;/td>
&lt;td>Tensor Cores (Gen 2)&lt;/td>
&lt;td>FP32, FP16, INT8&lt;/td>
&lt;td>Intel Xeon Scalable (Cascade Lake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Volta&lt;/td>
&lt;td>V100&lt;/td>
&lt;td>P3&lt;/td>
&lt;td>p3.2xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 1)&lt;/td>
&lt;td>FP64, FP32, FP16&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Volta&lt;/td>
&lt;td>V100&lt;/td>
&lt;td>P3&lt;/td>
&lt;td>p3.8xlarge&lt;/td>
&lt;td>4&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NVLink gen 2 (300 GB/s)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 1)&lt;/td>
&lt;td>FP64, FP32, FP16&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Volta&lt;/td>
&lt;td>V100&lt;/td>
&lt;td>P3&lt;/td>
&lt;td>p3.16xlarge&lt;/td>
&lt;td>8&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>NVLink gen 2 (300 GB/s)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 1)&lt;/td>
&lt;td>FP64, FP32, FP16&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Volta&lt;/td>
&lt;td>V100*&lt;/td>
&lt;td>P3&lt;/td>
&lt;td>p3dn.24xlarge&lt;/td>
&lt;td>8&lt;/td>
&lt;td>32 GB&lt;/td>
&lt;td>NVLink gen 2 (300 GB/s)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>Tensor Cores (Gen 1)&lt;/td>
&lt;td>FP64, FP32, FP16&lt;/td>
&lt;td>Intel Xeon (Skylake)&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kepler&lt;/td>
&lt;td>K80&lt;/td>
&lt;td>P2&lt;/td>
&lt;td>p2.xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>12 GB&lt;/td>
&lt;td>NA (single GPU)&lt;/td>
&lt;td>149W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP64, FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kepler&lt;/td>
&lt;td>K80&lt;/td>
&lt;td>P2&lt;/td>
&lt;td>p2.8xlarge&lt;/td>
&lt;td>8&lt;/td>
&lt;td>12 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>149W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP64, FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kepler&lt;/td>
&lt;td>K80&lt;/td>
&lt;td>P2&lt;/td>
&lt;td>p2.16xlarge&lt;/td>
&lt;td>16&lt;/td>
&lt;td>12 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>149W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP64, FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maxwell&lt;/td>
&lt;td>M60&lt;/td>
&lt;td>G3&lt;/td>
&lt;td>g3s.xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>8 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>150W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maxwell&lt;/td>
&lt;td>M60&lt;/td>
&lt;td>G3&lt;/td>
&lt;td>g3.4xlarge&lt;/td>
&lt;td>1&lt;/td>
&lt;td>8 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>150W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maxwell&lt;/td>
&lt;td>M60&lt;/td>
&lt;td>G3&lt;/td>
&lt;td>g3.8xlarge&lt;/td>
&lt;td>2&lt;/td>
&lt;td>8 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>150W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maxwell&lt;/td>
&lt;td>M60&lt;/td>
&lt;td>G3&lt;/td>
&lt;td>g3.16xlarge&lt;/td>
&lt;td>4&lt;/td>
&lt;td>8 GB&lt;/td>
&lt;td>PCIe&lt;/td>
&lt;td>150W&lt;/td>
&lt;td>No&lt;/td>
&lt;td>FP32&lt;/td>
&lt;td>Intel Xeon (Broadwell)&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Blog: Best GPUs on AWS for Deep Learning</title><link>/best-gpus-on-aws-for-deep-learning/</link><pubDate>Wed, 11 May 2022 00:00:00 +0000</pubDate><guid>/best-gpus-on-aws-for-deep-learning/</guid><description>
&lt;img src="/best-gpus-on-aws-for-deep-learning/gpu4-featured_hu93531d3f19a1a68a0fc2d3bfd6de5052_4220_640x0_resize_catmullrom_3.png" width="640" height="142"/>
&lt;p>Here are 5 GPU instance recommendations on AWS that should serve majority of deep learning use-cases. For a complete deep dive into choosing the right GPU for deep learning on AWS, read my blog post:&lt;/p>
&lt;div class="alert alert-primary" role="alert">
&lt;h3 id="i-classfa-brands-fa-mediumi-choosing-the-right-gpu-for-deep-learning-on-awshttpsmediumcomtowards-data-sciencechoosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86">&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86">Choosing the right GPU for deep learning on AWS&lt;/a>&lt;/h3>
&lt;/div>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
&lt;strong>Highest performing multi-GPU instance on AWS&lt;/strong>
&lt;/div>
&lt;div class="card-body">
&lt;p class="card-text">
&lt;p>&lt;img src="gpu1.png" alt="GPU">&lt;/p>
&lt;p>&lt;strong>Instance: p4d.24xlarge&lt;/strong>&lt;/p>
&lt;p>&lt;strong>When to use it:&lt;/strong> When you need all the performance you can get. Use it for distributed training on large models and datasets.&lt;/p>
&lt;p>&lt;strong>What you get:&lt;/strong> 8 x NVIDIA A100 GPUs with 40 GB GPU memory per GPU. Based on the latest NVIDIA Ampere architecture. Includes 3rd generation NVLink for fast multi-GPU training.&lt;/p>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;br/>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
&lt;strong>Highest performing single-GPU instance on AWS&lt;/strong>
&lt;/div>
&lt;div class="card-body">
&lt;p class="card-text">
&lt;p>&lt;img src="gpu2.png" alt="GPU">&lt;/p>
&lt;p>&lt;strong>Instance: p3.2xlarge&lt;/strong>&lt;/p>
&lt;p>&lt;strong>When to use it:&lt;/strong> When you want the highest performance Single GPU and you’re fine with 16 GB of GPU memory.&lt;/p>
&lt;p>&lt;strong>What you get:&lt;/strong> 1 x NVIDIA V100 GPU with 16 GB of GPU memory. Based on the older NVIDIA Volta architecture. The best performing single-GPU is still the NVIDIA A100 on P4 instance, but you can only get 8 x NVIDIA A100 GPUs on P4. This GPU has a slight performance edge over NVIDIA A10G on G5 instance discussed next, but G5 is far more cost-effective and has more GPU memory.&lt;/p>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;br/>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
&lt;strong>Best performance/cost, single-GPU instance on AWS&lt;/strong>
&lt;/div>
&lt;div class="card-body">
&lt;p class="card-text">
&lt;p>&lt;img src="gpu3.png" alt="GPU">&lt;/p>
&lt;p>&lt;strong>Instance: g5.xlarge&lt;/strong>&lt;/p>
&lt;p>&lt;strong>When to use it:&lt;/strong> When you want high-performance, more GPU memory at lower cost than P3 instance&lt;/p>
&lt;p>&lt;strong>What you get:&lt;/strong> 1 x NVIDIA A10G GPU with 24 GB of GPU memory, based on the latest Ampere architecture. NVIDIA A10G can be seen as a lower powered cousin of the A100 on the p4d.24xlarge so it’s easy to migrate and scale when you need more compute. Consider larger sizes withg5.(2/4/8/16)xlarge for the same single-GPU with more vCPUs and higher system memory if you have more pre or post processing steps.&lt;/p>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;br/>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
&lt;strong>Best performance/cost, multi-GPU instance on AWS&lt;/strong>
&lt;/div>
&lt;div class="card-body">
&lt;p class="card-text">
&lt;p>&lt;img src="gpu4-featured.png" alt="GPU">&lt;/p>
&lt;p>&lt;strong>Instance: p3.(8/16)xlarge&lt;/strong>&lt;/p>
&lt;p>&lt;strong>When to use it:&lt;/strong> Cost-effective multi-GPU model development and training.&lt;/p>
&lt;p>&lt;strong>What you get:&lt;/strong> p3.8xlarge has 4 x NVIDIA V100 GPUs and p3.16xlarge has 8 x NVIDIA V100 GPUs with 16 GB of GPU memory on each GPU, based on the older NVIDIA Volta architecture. For larger models, datasets and faster performance consider P4 instances.&lt;/p>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;br/>
&lt;div class="td-card card border me-4">
&lt;div class="card-header">
&lt;strong>High-performance GPU instance at a budget on AWS&lt;/strong>
&lt;/div>
&lt;div class="card-body">
&lt;p class="card-text">
&lt;p>&lt;img src="gpu5.png" alt="GPU">&lt;/p>
&lt;p>&lt;strong>Instance: g4dn.xlarge&lt;/strong>&lt;/p>
&lt;p>&lt;strong>When to use it:&lt;/strong> Lower performance than other options at lower cost for model development and training. Cost effective model inference deployment.&lt;/p>
&lt;p>&lt;strong>What you get:&lt;/strong> 1 x NVIDIA T4 GPU with 16 GB of GPU memory. Based on the previous generation NVIDIA Turing architecture. Consider g4dn.(2/4/8/16)xlarge for more vCPUs and higher system memory if you have more pre or post processing.&lt;/p>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;br/>
## Related blog posts
&lt;div class="td-card-group card-group p-0 mb-4">
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;img src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*SAgQSIEprO1looCxAdf52w.png" alt="">&lt;/h5>
&lt;/div>
&lt;div class="card-footer">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/ai-accelerators-machine-learning-algorithms-and-their-co-design-and-evolution-2676efd47179">AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution&lt;/a>&lt;/div>
&lt;/div>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;img src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*qAx5CnFbebRR_bOyQbSGYw.png" alt="">&lt;/h5>
&lt;/div>
&lt;div class="card-footer">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86">Choosing the right GPU for deep learning on AWS&lt;/a>&lt;/div>
&lt;/div>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
&lt;img src="https://miro.medium.com/v2/resize:fit:4800/0*Po5qTIX3N1ZJoJHd" alt="">&lt;/h5>
&lt;/div>
&lt;div class="card-footer">
&lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators-e076c6eb7802">How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators&lt;/a>&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Blog: A complete guide to AI accelerators for deep learning inference — GPUs, AWS Inferentia and Amazon Elastic Inference</title><link>/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/</guid><description>
&lt;img src="/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/featured_hu153888cb09e42ad7fc9ab1679dae3388_33262_640x0_resize_catmullrom_3.png" width="640" height="361"/>
&lt;h3 id="lets-start-by-answering-the-question-what-is-an-ai-accelerator">Let’s start by answering the question “What is an AI accelerator?”&lt;/h3>
&lt;p>An AI accelerator is a dedicated processor designed to accelerate machine learning computations. Machine learning, and particularly its subset, deep learning is primarily composed of a large number of linear algebra computations, (i.e. matrix-matrix, matrix-vector operations) and these operations can be easily parallelized. AI accelerators are specialized hardware designed to accelerate these basic machine learning computations and improve performance, reduce latency and reduce cost of deploying machine learning based applications.&lt;/p>
&lt;h3 id="do-i-need-an-ai-accelerator-for-machine-learning-ml-inference">Do I need an AI accelerator for machine learning (ML) inference?&lt;/h3>
&lt;p>Let’s say you have an ML model as part of your software application. The prediction step (or inference) is often the most time consuming part of your application that directly affects user experience. A model that takes several hundreds of milliseconds to generate text translations or apply filters to images or generate product recommendations, can drive users away from your “sluggish”, “slow”, “frustrating to use” app.
By speeding up inference, you can reduce the overall application latency and deliver an app experience that can be described as “smooth”, “snappy”, and “delightful to use”. And you can speed up inference by offloading ML model prediction computation to an AI accelerator.
With great market needs comes great many product alternatives, so naturally there is more than one way to accelerate your ML models in the cloud.
In this blog post, I’ll explore three popular options:&lt;/p>
&lt;ol>
&lt;li>GPUs: Particularly, the high-performance NVIDIA T4 and NVIDIA V100 GPUs&lt;/li>
&lt;li>AWS Inferentia: A custom designed machine learning inference chip by AWS&lt;/li>
&lt;li>Amazon Elastic Inference (EI): An accelerator that saves cost by giving you access to variable-size GPU acceleration, for models that don’t need a dedicated GPU&lt;/li>
&lt;/ol>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
Read the full blog post here: &lt;br>&lt;br> &lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c">A complete guide to AI accelerators for deep learning inference — GPUs, AWS Inferentia and Amazon Elastic Inference&lt;/a>&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
&lt;img src="featured.png" alt="">&lt;/div>
&lt;/div></description></item><item><title>Blog: Choosing the right GPU for deep learning on AWS</title><link>/choosing-the-right-gpu-for-deep-learning-on-aws/</link><pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate><guid>/choosing-the-right-gpu-for-deep-learning-on-aws/</guid><description>
&lt;img src="/choosing-the-right-gpu-for-deep-learning-on-aws/featured_hu3cf755c2b3f91847dc184a8c7feac088_100672_640x0_resize_catmullrom_3.png" width="640" height="453"/>
&lt;p>On AWS, you can launch GPU instances with different GPU memory sizes (8 GB, 16 GB, 24 GB, 32 GB, 40 GB), NVIDIA GPU generations (Ampere, Turing, Volta, Maxwell, Kepler) different capabilities (FP64, FP32, FP16, INT8, Sparsity, TensorCores, NVLink), different number of GPUs per instance (1, 2, 4, 8, 16), and paired with different CPUs (Intel, AMD, Graviton2). You can also select instances with different vCPUs (core thread count), system memory and network bandwidth and add a range of storage options (object storage, network file systems, block storage, etc.) — in summary, you have options.&lt;/p>
&lt;p>My goal with this blog post is to provide you with guidance on how you can choose the right GPU instance on AWS for your deep learning projects. I’ll discuss key features and benefits of various EC2 GPU instances, and workloads that are best suited for each instance type and size. If you’re new to AWS, or new to GPUs, or new to deep learning, my hope is that you’ll find the information you need to make the right choice for your projects.&lt;/p>
&lt;h4 id="topics-covered-in-this-blog-post">Topics covered in this blog post:&lt;/h4>
&lt;ol>
&lt;li>Key recommendations for the busy data scientist/ML practitioner&lt;/li>
&lt;li>Why you should choose the right GPU instance not just the right GPU&lt;/li>
&lt;li>Deep dive on GPU instance types: P4, P3, G5 (G5g), G4, P2 and G3&lt;/li>
&lt;li>Other machine learning accelerators and instances on AWS&lt;/li>
&lt;li>Cost optimization tips when using GPU instances for ML&lt;/li>
&lt;li>What software and frameworks to use on AWS?&lt;/li>
&lt;li>Which GPUs to consider for HPC use-cases?&lt;/li>
&lt;li>A complete and unapologetically detailed spreadsheet of all AWS GPU instances and their features&lt;/li>
&lt;/ol>
&lt;div class="td-card card border me-4">
&lt;div class="card-body">
&lt;h5 class="card-title">
Read the full blog post here: &lt;br>&lt;br> &lt;i class='fa-brands fa-medium'>&lt;/i> &lt;a href="https://medium.com/towards-data-science/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86">Choosing the right GPU for deep learning on AWS&lt;/a>&lt;/h5>
&lt;p class="card-text">
&lt;/p>
&lt;/div>
&lt;div class="card-footer">
&lt;img src="featured.png" alt="">&lt;/div>
&lt;/div></description></item></channel></rss>