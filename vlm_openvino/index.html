<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=alternate type=application/rss+xml href=/vlm_openvino/index.xml><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Building a local multimodal Chatbot with OpenVINO | Shashank Prasanna</title><meta name=description content="Guide to building and deploying a CPU-only local multimodal chatbot, accessible through: **(1) Gradio chat UI, (2) Jupyter notebook, (3) Open WebUI and (4) iOS app.**"><meta property="og:url" content="/vlm_openvino/"><meta property="og:site_name" content="Shashank Prasanna"><meta property="og:title" content="Building a local multimodal Chatbot with OpenVINO"><meta property="og:description" content="Guide to building and deploying a CPU-only local multimodal chatbot, accessible through: **(1) Gradio chat UI, (2) Jupyter notebook, (3) Open WebUI and (4) iOS app.**"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="Building a local multimodal Chatbot with OpenVINO"><meta itemprop=description content="Guide to building and deploying a CPU-only local multimodal chatbot, accessible through: **(1) Gradio chat UI, (2) Jupyter notebook, (3) Open WebUI and (4) iOS app.**"><meta itemprop=wordCount content="3619"><meta name=twitter:card content="summary"><meta name=twitter:title content="Building a local multimodal Chatbot with OpenVINO"><meta name=twitter:description content="Guide to building and deploying a CPU-only local multimodal chatbot, accessible through: **(1) Gradio chat UI, (2) Jupyter notebook, (3) Open WebUI and (4) iOS app.**"><link rel=preload href=/scss/main.min.aa71f034e2f6cfca3f5ece19e844af7a0e1d304dece013e319adf5d84e3265a1.css as=style integrity="sha256-qnHwNOL2z8o/Xs4Z6ESveg4dME3s4BPjGa312E4yZaE=" crossorigin=anonymous><link href=/scss/main.min.aa71f034e2f6cfca3f5ece19e844af7a0e1d304dece013e319adf5d84e3265a1.css rel=stylesheet integrity="sha256-qnHwNOL2z8o/Xs4Z6ESveg4dME3s4BPjGa312E4yZaE=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-20Q10LSLZ7"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-20Q10LSLZ7")}</script></head><body class="td-section td-blog"><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"></span><span class=navbar-brand__name>Shashank Prasanna</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class="nav-link active" href=/><span>Home</span></a></li><li class=nav-item><a class=nav-link href=https://medium.com/@shashankprasanna target=_blank rel=noopener><i class='fa-brands fa-medium'></i><span>Medium Blog </span><sup><i class='ps-1 fa-solid fa-up-right-from-square fa-xs' aria-hidden=true></i></sup></a></li></ul></div><div class="d-none d-lg-block"></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><a href=/><img style=vertical-align:middle src=/featured-background.png alt="shashank prasanna" width=200></a><div class="td-page-meta ml-2 pb-1 pt-2 mb-0"><div class=column><div class=column><a href=https://twitter.com/shshnkp target=_blank><font color=gray size=3><i class="fa fab fa-twitter"></i>&nbsp;&nbsp;@shshnkp</font></a></div><div class=column><a href=https://www.youtube.com/@shashank.prasanna target=_blank><font color=gray size=3><i class="fa fab fa-youtube"></i>&nbsp;&nbsp;@shashank.prasanna</font></a></div><div class=column><a href=https://medium.com/@shashankprasanna target=_blank><font color=gray size=3><i class="fa fab fa-medium"></i>&nbsp;@shashankprasanna</font></a></div><div class=column><a href=https://www.linkedin.com/in/shashankprasanna/ target=_blank><font color=gray size=3><i class="fa fab fa-linkedin"></i>&nbsp;&nbsp;shashankprasanna</font></a></div><div class=column><a href=https://github.com/shashankprasanna/ target=_blank><font color=gray size=3><i class="fa fab fa-github"></i>&nbsp;&nbsp;shashankprasanna</font></a></div><div class=column><a href=/shashank_prasanna_resume_2026.pdf target=_blank><font color=gray size=3><i class="fa-solid fa-file"></i>&nbsp;&nbsp;&nbsp;resume</font></a></div></div></div></aside><aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none"><div class=td-toc><font color=gray size=6>Sections</font><nav id=TableOfContents><ul><li><ul><li><a href=#guide-to-building-and-deploying-a-cpu-only-local-multimodal-chatbot-accessible-through--1-gradio-ui-2-jupyter-notebook-3-open-webui-and-4-ios-app>Guide to building and deploying a CPU-only local multimodal chatbot, accessible through<br><strong>(1) Gradio UI, (2) Jupyter notebook, (3) Open WebUI and (4) iOS app.</strong></a></li></ul></li><li><a href=#running-the-project>Running the project</a><ul><li></li></ul></li><li><a href=#project-overview>Project overview</a><ul><li></li></ul></li><li><a href=#choosing-the-right-model-qwen2-vl-2b-instruct>Choosing the right model: <code>Qwen2-VL-2B-Instruct</code></a></li><li><a href=#model-preparation>Model preparation</a><ul><li><a href=#quantize-to-int4>Quantize to INT4</a></li><li><a href=#testing-inference>Testing inference</a></li></ul></li><li><a href=#model-inference-engine>Model inference engine</a><ul><li><a href=#image-preprocessing-for-performance>Image preprocessing for performance</a></li><li><a href=#loading-the-model>Loading the model</a></li><li><a href=#chat-history>Chat history</a></li><li><a href=#streaming-with-thread--queue>Streaming with Thread + Queue</a></li></ul></li><li><a href=#implementing-the-gradio-ui>Implementing the Gradio UI</a><ul><li><a href=#gradio-ui-embedded-in-jupyter-notebook>Gradio UI embedded in Jupyter notebook</a></li><li><a href=#gradio-web-ui-with-a-stand-alone-app>Gradio web UI with a stand-alone app</a></li><li><a href=#hosting-the-gradio-server>Hosting the Gradio server</a></li></ul></li><li><a href=#deploying-openvino-model-server-ovms-using-docker>Deploying OpenVINO model server (OVMS) using Docker</a><ul><li><a href=#exporting-ovms-model>Exporting OVMS model</a></li><li><a href=#docker-compose-configuration>Docker compose configuration</a></li><li><a href=#testing-the-api-with-curl>Testing the API with curl</a></li></ul></li><li><a href=#open-webui-client>Open WebUI client</a></li><li><a href=#building-an-ios-vlm-chatbot-client-with-ovms-backend>Building an iOS VLM chatbot client with OVMS backend</a><ul><li><a href=#client-configuration>Client configuration</a></li><li><a href=#client-side-image-preprocessing>Client-side image preprocessing</a></li><li><a href=#streaming-responses>Streaming responses</a></li></ul></li><li><a href=#wrapping-up>Wrapping up</a><ul><li><a href=#developer-friction-log>Developer friction log</a></li></ul></li></ul></nav></div></aside><main class="col-12 col-md-9 col-xl-8 ps-md-5 pe-md-4" role=main><font color=black size=5></font><div class=td-content><h1>Building a local multimodal Chatbot with OpenVINO</h1><h3 id=guide-to-building-and-deploying-a-cpu-only-local-multimodal-chatbot-accessible-through--1-gradio-ui-2-jupyter-notebook-3-open-webui-and-4-ios-app>Guide to building and deploying a CPU-only local multimodal chatbot, accessible through<br><strong>(1) Gradio UI, (2) Jupyter notebook, (3) Open WebUI and (4) iOS app.</strong></h3><div class="alert alert-info" role=alert><i class="fa-solid fa-laptop-code"></i> <a href=/llm-chat-app.zip>Click here</a> to download the project</div><br><style>.hugo-square-grid{display:grid;grid-template-columns:repeat(2,minmax(0,1fr));gap:70px 1px;align-items:stretch;width:100%}.hugo-square-grid>div{height:100%;margin:0;width:100%!important}@media(max-width:768px){.hugo-square-grid{grid-template-columns:1fr}}</style><div class=hugo-square-grid-wrapper><div class=hugo-square-grid><div class="td-card card" style=width:auto;padding:8px><div class=card-header><strong><center>Gradio UI</center></strong></div><div style=display:flex;flex-direction:column;justify-content:center;align-items:center;gap:1px><a href=images/gradio.gif target=_blank rel="noopener noreferrer" style=width:100%><img src=images/gradio.gif alt="Gradio UI" style="border-radius:8px;display:block;margin:0 auto;width:100%;height:350px;object-fit:contain;background-color:#f8f9fa"></a></div></div><div class="td-card card" style=width:auto;padding:8px><div class=card-header><strong><center>Jupyter Notebook</center></strong></div><div style=display:flex;flex-direction:column;justify-content:center;align-items:center;gap:1px><a href=images/jupyter.gif target=_blank rel="noopener noreferrer" style=width:100%><img src=images/jupyter.gif alt="Jupyter Notebook" style="border-radius:8px;display:block;margin:0 auto;width:100%;height:350px;object-fit:contain;background-color:#f8f9fa"></a></div></div><div class="td-card card" style=width:auto;padding:8px><div class=card-header><strong><center>Open WebUI</center></strong></div><div style=display:flex;flex-direction:column;justify-content:center;align-items:center;gap:1px><a href=images/openwebui.gif target=_blank rel="noopener noreferrer" style=width:100%><img src=images/openwebui.gif alt="Open WebUI" style="border-radius:8px;display:block;margin:0 auto;width:100%;height:350px;object-fit:contain;background-color:#f8f9fa"></a></div></div><div class="td-card card" style=width:auto;padding:8px><div class=card-header><strong><center>iOS App</center></strong></div><div style=display:flex;flex-direction:column;justify-content:center;align-items:center;gap:1px><a href=images/ios.gif target=_blank rel="noopener noreferrer" style=width:100%><img src=images/ios.gif alt="iOS App" style="border-radius:8px;display:block;margin:0 auto;width:100%;height:350px;object-fit:contain;background-color:#f8f9fa"></a></div></div></div></div><br>In this guide I'll walk you through building a multimodal chatbot that supports text and image input, streaming responses, and runs entirely on a local CPU with no internet access at runtime.<p>Here&rsquo;s what we&rsquo;ll build:</p><ol><li>A <strong>model preparation pipeline</strong> — download, export, and quantize a vision-language model for CPU inference</li><li>An <strong>inference engine</strong> with streaming — image preprocessing, chat history, and token-by-token output</li><li>A <strong>Gradio web UI</strong> and <strong>Jupyter notebook</strong> for local experimentation</li><li>A <strong>Docker Compose stack</strong> with OVMS (OpenVINO Model Server) serving the model over an OpenAI-compatible API</li><li>Two production-ready clients<ul><li><strong>Open WebUI</strong> (zero-code ChatGPT-like frontend) and</li><li><strong>Native iOS app</strong> built with SwiftUI</li></ul></li></ol><blockquote><p>I&rsquo;ll share the architectural decisions, trade-offs, and rough edges along the way. I&rsquo;ve also included a friction log at the end based on my experience building this project.</p></blockquote><h2 id=running-the-project>Running the project</h2><p>Before we dive into the implementation details, here are few quick ways to run the project and see the results shown at the top.</p><h4 id=option-1-local-gradio-or-jupyter>Option 1: Local (Gradio or Jupyter)</h4><p>Install dependencies with <code>uv</code>, then launch the Gradio app or the notebook:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>uv sync
</span></span><span style=display:flex><span><span style=color:#204a87>source</span> .venv/bin/activate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Launch the Gradio chat UI at http://localhost:7860</span>
</span></span><span style=display:flex><span>python -m app.gradio_app
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Or open the interactive notebook</span>
</span></span><span style=display:flex><span>jupyter lab notebooks/gradio_demo.ipynb
</span></span></code></pre></div><p>This requires running the model preparation notebook first (<code>notebooks/model_preparation.ipynb</code>) to download and quantize the model.</p><h4 id=option-2-docker-compose-ovms--all-clients>Option 2: Docker Compose (OVMS + all clients)</h4><p>To skip the local Python setup entirely:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># One-time: export the model for OVMS (requires internet)</span>
</span></span><span style=display:flex><span>bash export_ovms_model.sh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Start all services</span>
</span></span><span style=display:flex><span>docker compose up --build
</span></span></code></pre></div><h4 id=option-3-ios-client>Option 3: iOS client</h4><p><div class="td-card card border-0" style=width:auto;padding:8px><div style=display:flex;flex-direction:row;justify-content:center;align-items:center;gap:20px><div class="td-card card me-4" style=width:auto><div class=card-body><div class=card-text><a href=images/xcode.png target=_blank><img src=images/xcode.png style=width:100%;border-radius:8px;overflow:hidden;display:block></a></div><div class=card-body-extra></div></div></div><br></div></div>Open <code>LLMChatApp/LLMChatApp.xcodeproj</code> in Xcode, build and run in the simulator. The app connects to OVMS at <code>localhost:8000</code>, so the Docker stack needs to be running.</p><h2 id=project-overview>Project overview</h2><p>This project has three components (1) Local development environment (2) Docker based deployment stack and (3) Open WebUI and iOS clients.</p><h4 id=1-local-setup-for-development-and-testing-environment>1. Local setup for development and testing environment</h4><div class="td-card card border-0" style=width:auto;padding:8px><div style=display:flex;flex-direction:row;justify-content:center;align-items:center;gap:20px><div class="td-card card me-4" style=width:auto><div class=card-body><div class=card-text><a href=images/host_install.png target=_blank><img src=images/host_install.png style=width:100%;border-radius:8px;overflow:hidden;display:block></a></div><div class=card-body-extra></div></div></div><br></div></div><p>This includes Python files and Jupyter notebook for model preparation and chatbot UI using Gradio. You can use <code>uv</code> and the accompanying <code>pyproject.toml</code> to quickly setup your development environment. Alternatively, I&rsquo;ve also provided a <code>Dockerfile</code> with all the dependencies and Jupyter notebooks included if you don&rsquo;t want to do a local environment setup.</p><h4 id=2-docker-compose-stack-for-a-more-portable-setup-and-reproducibility>2. Docker Compose stack for a more portable setup and reproducibility</h4><div class="td-card card border-0" style=width:auto;padding:8px><div style=display:flex;flex-direction:row;justify-content:center;align-items:center;gap:20px><div class="td-card card me-4" style=width:auto><div class=card-body><div class=card-text><a href=images/docker_all.png target=_blank><img src=images/docker_all.png style=width:100%;border-radius:8px;overflow:hidden;display:block></a></div><div class=card-body-extra></div></div></div><br></div></div><p>This includes <code>docker-compose.yml</code> file with specifications for:</p><ol><li>Containerized local development environment that gives you the same environment discussed above.</li><li>OVMS (OpenVINO Model Server) hosting the models generated from step (1), making it accessible to clients like Open WebUI and iOS apps.</li></ol><h4 id=3-chatbot-clients-used-in-production>3. Chatbot clients used in production</h4><p>This includes two clients:</p><ol><li>Open WebUI client service that can be started from the same Docker Compose stack as above. It auto-discovers OVMS service and our hosted models.</li><li>A full-fledged iOS chat app that accesses OVMS over the network to deliver a multimodal chat experience, allowing users to upload their photos or take camera photos and share it in the chatbot.</li></ol><p>The complete project structure is as follows:</p><pre tabindex=0><code>llm-chat-app/
├── app/
│   ├── __init__.py
│   ├── model_engine.py             # OpenVINO VLMPipeline + streaming engine
│   └── gradio_app.py               # Gradio multimodal chat UI
├── notebooks/
│   ├── model_preparation.ipynb     # Download, export, quantize, test
│   └── gradio_demo.ipynb           # Interactive Gradio demo in Jupyter
├── LLMChatApp/                     # Native iOS client (SwiftUI + OpenAI SDK)
│   └── LLMChatApp/
│       ├── LLMChatAppApp.swift
│       └── ChatView.swift
├── Dockerfile                      # Distributable Gradio + Jupyter image
├── entrypoint.sh                   # Launches gradio and jupyter servers
├── docker-compose.yml              # OVMS + Open WebUI + Gradio app
├── export_ovms_model.sh            # One-time OVMS model export
├── pyproject.toml                  # Dependencies (managed by uv)
└── models/                         # Created by notebook (gitignored)
    ├── qwen2-vl-2b-instruct-fp16/  # Created by notebook (gitignored)
    └── qwen2-vl-2b-instruct-int4/  # Created by notebook (gitignored)
</code></pre><p>Now, let&rsquo;s walk through building this project. I&rsquo;ll share architectural considerations and choices along the way.</p><h2 id=choosing-the-right-model-qwen2-vl-2b-instruct>Choosing the right model: <code>Qwen2-VL-2B-Instruct</code></h2><p>There are a lot of VLM models on HuggingFace to choose from, but our first step is to check what models are actually supported by OpenVINO GenAI&rsquo;s <code>VLMPipeline</code>. <code>VLMPipeline</code> is the high-level API for VLM inference — it handles chat history, image input, streaming, and generation config. But it only works with specific architectures or similar, listed on the <a href=https://docs.openvino.ai/2025/learn-openvino/llm_inference_guide/genai-guide-vlm.html>OpenVINO GenAI docs</a>.</p><p>This ruled out my first choice which was HuggingFace&rsquo;s <code>SmolVLM-256M</code> and <code>SmolVLM-500M</code> as these are not supported with <code>VLMPipeline</code>. On further research, I found out that you can run these using <code>OVModelForCausalLM</code> API but it comes with some downsides, namely the entire bulk of HuggingFace transformers is now part of your deployment package. Therefore this was not really an option.</p><p>From the list of models supported by <code>VLMPipeline</code>, I narrowed down to these.</p><table><thead><tr><th>Model</th><th>Parameters</th><th>Why we passed</th></tr></thead><tbody><tr><td>LLaVA-1.5</td><td>7B</td><td>Too large. Older.</td></tr><tr><td>MiniCPM-V-2.6</td><td>~3B</td><td>Slower prefill.</td></tr><tr><td>Phi-4-Multimodal</td><td>14B</td><td>Too large.</td></tr><tr><td><strong>Qwen2-VL-2B-Instruct</strong></td><td><strong>2B</strong></td><td><strong>Sweet spot, with native <code>VLMPipeline</code> support.</strong></td></tr></tbody></table><p>Qwen2-VL-2B-Instruct is the smallest vision-language model in the supported list that delivers usable quality. At INT4, it fits under 2 GB on disk.</p><h2 id=model-preparation>Model preparation</h2><p>We&rsquo;re now ready to prepare the model for deployment with OpenVINO. The code excerpts discussed below live in our Jupyter notebook (<code>notebooks/model_preparation.ipynb</code>).</p><p>First we use <code>optimum-cli</code> to download the model from HuggingFace and export it as half-precision model represented in OpenVINO IR:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>optimum-cli <span style=color:#204a87>export</span> openvino <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --model Qwen/Qwen2-VL-2B-Instruct <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --task image-text-to-text <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --weight-format fp16 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --trust-remote-code <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    models/qwen2-vl-2b-instruct-fp16
</span></span></code></pre></div><h3 id=quantize-to-int4>Quantize to INT4</h3><p>To get this running fast on a CPU, we have to further quantize it. For INT4, we use symmetric quantization with group size 64:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>optimum-cli <span style=color:#204a87>export</span> openvino <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --model Qwen/Qwen2-VL-2B-Instruct <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --task image-text-to-text <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --weight-format int4 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --group-size <span style=color:#0000cf;font-weight:700>64</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --sym <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --trust-remote-code <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    models/qwen2-vl-2b-instruct-int4
</span></span></code></pre></div><h4 id=size-comparison>Size comparison</h4><table><thead><tr><th>Variant</th><th>Size</th><th>Compression vs FP16</th></tr></thead><tbody><tr><td>FP16</td><td>4,685 MB</td><td>Baseline</td></tr><tr><td>INT4</td><td>1,773 MB</td><td>2.6x smaller</td></tr></tbody></table><h3 id=testing-inference>Testing inference</h3><p>Before building any UI, we verify the model loads and generates output:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>import</span> <span style=color:#000>openvino_genai</span> <span style=color:#204a87;font-weight:700>as</span> <span style=color:#000>ov_genai</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000>pipe</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>ov_genai</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>VLMPipeline</span><span style=color:#000;font-weight:700>(</span><span style=color:#204a87>str</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>INT4_DIR</span><span style=color:#000;font-weight:700>),</span> <span style=color:#4e9a06>&#34;CPU&#34;</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000>config</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>ov_genai</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>GenerationConfig</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span><span style=color:#000>config</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>max_new_tokens</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#0000cf;font-weight:700>100</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000>history</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>ov_genai</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>ChatHistory</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span><span style=color:#000>history</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>append</span><span style=color:#000;font-weight:700>({</span><span style=color:#4e9a06>&#34;role&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;user&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#4e9a06>&#34;content&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;What is the capital of France?&#34;</span><span style=color:#000;font-weight:700>})</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000>result</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>pipe</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>generate</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>history</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>generation_config</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>config</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#204a87>print</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>result</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>texts</span><span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>])</span>
</span></span></code></pre></div><p>Output</p><pre tabindex=0><code>The capital of France is Paris.
</code></pre><h2 id=model-inference-engine>Model inference engine</h2><p>With the model quantized, we can now implement additional components we need to serve the model. The inference engine lives in <code>app/model_engine.py</code>. It handles image preprocessing, model loading, and token streaming.</p><h3 id=image-preprocessing-for-performance>Image preprocessing for performance</h3><p>Even with INT4 quantization, I found that the prefill step was taking several seconds to run, making the chat experience unsatisfying. The issue wasn&rsquo;t the text but the image input tokenization. Unfortunately, CPUs are not the best brute-force throughput engines to crunch through image processing. So, I did the next best thing, reduce the size of the image before sending it to the model. Later you&rsquo;ll see that I&rsquo;ve implemented the same technique for SwiftUI iOS app and Open WebUI on the client side.</p><p>Qwen2-VL tiles images into 28x28 pixel patches. This means a large photo produces a large number of patches, and the prefill time grows accordingly. What we can do is scale down the image but keep it as a multiple of 28 so there are no partial patches. This ensures that the image tokenization can be speedy.</p><p>Later you&rsquo;ll see that we predefine small, medium and large sizes in <code>app/gradio_app.py</code>, used in both the Gradio UI and mirrored as an <code>ImageQualityOption</code> enum in the iOS app:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000>IMAGE_QUALITY_MAP</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000;font-weight:700>{</span><span style=color:#4e9a06>&#34;Small (336px)&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>336</span><span style=color:#000;font-weight:700>,</span> <span style=color:#4e9a06>&#34;Medium (448px)&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>448</span><span style=color:#000;font-weight:700>,</span> <span style=color:#4e9a06>&#34;Large (672px)&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>672</span><span style=color:#000;font-weight:700>}</span>
</span></span></code></pre></div><p>On the Python side of things, this is our simple preprocessor:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000>MAX_IMAGE_DIM</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#0000cf;font-weight:700>672</span>  <span style=color:#8f5902;font-style:italic># 28*24</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>_preprocess_image</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>image</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>max_dim</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>MAX_IMAGE_DIM</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>-&gt;</span> <span style=color:#000>Image</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>Image</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>    <span style=color:#4e9a06>&#34;&#34;&#34;Resize image so longest side &lt;= max_dim, dimensions rounded to 28px.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#000>image</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>image</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>convert</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;RGB&#34;</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#000>w</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>h</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>image</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>size</span>
</span></span><span style=display:flex><span>    <span style=color:#000>scale</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>max_dim</span> <span style=color:#ce5c00;font-weight:700>/</span> <span style=color:#204a87>max</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>w</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>h</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#000>new_w</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#204a87>max</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>28</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000;font-weight:700>(</span><span style=color:#204a87>int</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>w</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000>scale</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>//</span> <span style=color:#0000cf;font-weight:700>28</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#0000cf;font-weight:700>28</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#000>new_h</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#204a87>max</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>28</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000;font-weight:700>(</span><span style=color:#204a87>int</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>h</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000>scale</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>//</span> <span style=color:#0000cf;font-weight:700>28</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#0000cf;font-weight:700>28</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>return</span> <span style=color:#000>image</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>resize</span><span style=color:#000;font-weight:700>((</span><span style=color:#000>new_w</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>new_h</span><span style=color:#000;font-weight:700>),</span> <span style=color:#000>Image</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>LANCZOS</span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><p>Here <code>max(28, ...)</code> prevents zero-dimension images. <code>// 28 * 28</code> rounds down to the nearest multiple of 28. We implement <a href=/vlm_openvino/#client-side-image-preprocessing>similar preprocessor in Swift</a> for the iOS app.</p><p>To pass a preprocessed image into <code>VLMPipeline</code>, we convert it to an OpenVINO tensor:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>_pil_to_ov_tensor</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>image</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>max_dim</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>MAX_IMAGE_DIM</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>-&gt;</span> <span style=color:#000>ov</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>Tensor</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>return</span> <span style=color:#000>ov</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>Tensor</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>np</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>array</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>_preprocess_image</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>image</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>max_dim</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>max_dim</span><span style=color:#000;font-weight:700>)))</span>
</span></span></code></pre></div><h3 id=loading-the-model>Loading the model</h3><p><code>load_model()</code> wraps <code>VLMPipeline</code> with <code>PERFORMANCE_HINT=LATENCY</code>, which tells OpenVINO to optimize for single-request latency rather than throughput. Simplified excerpt below (see <code>app/model_engine.py</code> for the full implementation):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>load_model</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>model_dir</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>None</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>device</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;CPU&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#ce5c00;font-weight:700>**</span><span style=color:#000>kwargs</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>-&gt;</span> <span style=color:#000>ov_genai</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>VLMPipeline</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>    <span style=color:#000>model_dir</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>Path</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>model_dir</span><span style=color:#000;font-weight:700>)</span> <span style=color:#204a87;font-weight:700>if</span> <span style=color:#000>model_dir</span> <span style=color:#204a87;font-weight:700>else</span> <span style=color:#000>DEFAULT_MODEL_DIR</span>
</span></span><span style=display:flex><span>    <span style=color:#000>config</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000;font-weight:700>{</span><span style=color:#4e9a06>&#34;PERFORMANCE_HINT&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;LATENCY&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#ce5c00;font-weight:700>**</span><span style=color:#000>kwargs</span><span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>return</span> <span style=color:#000>ov_genai</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>VLMPipeline</span><span style=color:#000;font-weight:700>(</span><span style=color:#204a87>str</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>model_dir</span><span style=color:#000;font-weight:700>),</span> <span style=color:#000>device</span><span style=color:#000;font-weight:700>,</span> <span style=color:#ce5c00;font-weight:700>**</span><span style=color:#000>config</span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><h3 id=chat-history>Chat history</h3><p>The <a href=https://openvinotoolkit.github.io/openvino.genai/docs/guides/chat-scenario>OpenVINO GenAI documentation</a> recommends using the newer <code>ChatHistory</code> object shared between the caller and the model. This new API replaces the older <code>start_chat()</code> / <code>finish_chat()</code> APIs. The <code>new_history()</code> helper in <code>model_engine.py</code> creates a <code>ChatHistory</code> and optionally seeds it with messages:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>new_history</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>messages</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>None</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>-&gt;</span> <span style=color:#000>ov_genai</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>ChatHistory</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>    <span style=color:#000>history</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>ov_genai</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>ChatHistory</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>if</span> <span style=color:#000>messages</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>for</span> <span style=color:#000>msg</span> <span style=color:#204a87;font-weight:700>in</span> <span style=color:#000>messages</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>            <span style=color:#000>history</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>append</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>msg</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>return</span> <span style=color:#000>history</span>
</span></span></code></pre></div><p>The caller owns the history object and passes it into every <code>stream()</code> call — the engine appends user and assistant messages as it generates.</p><blockquote><p><strong>Note:</strong> After a lot of trial and error, I discovered that <code>ChatHistory</code> is <em>only</em> supported for <code>LLMPipeline</code> and support for <code>VLMPipeline</code> is only available on the pre-release builds. Therefore I had to specifically pin the environment to the pre-release to make multi-turn conversations work with <code>ChatHistory</code>.</p></blockquote><h3 id=streaming-with-thread--queue>Streaming with Thread + Queue</h3><p>The current implementation of <code>VLMPipeline.generate()</code> is a blocking call. To stream tokens to the UI as they&rsquo;re produced, we can run the generation on a background thread using <code>threading.Thread</code> and use <code>queue.Queue</code> to pass tokens to the main thread, which yields them as a Python generator.</p><p>Here&rsquo;s our <code>OpenVINOStreamer</code> class:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>class</span> <span style=color:#000>OpenVINOStreamer</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>    <span style=color:#4e9a06>&#34;&#34;&#34;Streams tokens from a VLMPipeline; exposes .metrics after iteration.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>__init__</span><span style=color:#000;font-weight:700>(</span><span style=color:#3465a4>self</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>pipe</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>ov_genai</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>VLMPipeline</span><span style=color:#000;font-weight:700>):</span>
</span></span><span style=display:flex><span>        <span style=color:#3465a4>self</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>pipe</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>pipe</span>
</span></span><span style=display:flex><span>        <span style=color:#3465a4>self</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>metrics</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>ov_genai</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>PerfMetrics</span> <span style=color:#ce5c00;font-weight:700>|</span> <span style=color:#204a87;font-weight:700>None</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#204a87;font-weight:700>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>stream</span><span style=color:#000;font-weight:700>(</span><span style=color:#3465a4>self</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>history</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>prompt</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>image</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>None</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>               <span style=color:#000>max_new_tokens</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>DEFAULT_MAX_NEW_TOKENS</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>               <span style=color:#000>temperature</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>0.0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>top_p</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>1.0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>top_k</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>50</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>               <span style=color:#000>max_dim</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>MAX_IMAGE_DIM</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>-&gt;</span> <span style=color:#000>Iterator</span><span style=color:#000;font-weight:700>[</span><span style=color:#204a87>str</span><span style=color:#000;font-weight:700>]:</span>
</span></span><span style=display:flex><span>        <span style=color:#4e9a06>&#34;&#34;&#34;Stream a response token-by-token.
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        Appends a user message before generation and an assistant message after.
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#3465a4>self</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>metrics</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#204a87;font-weight:700>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#000>history</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>append</span><span style=color:#000;font-weight:700>({</span><span style=color:#4e9a06>&#34;role&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;user&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#4e9a06>&#34;content&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>prompt</span><span style=color:#000;font-weight:700>})</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#000>config</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>ov_genai</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>GenerationConfig</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>        <span style=color:#000>config</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>max_new_tokens</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>max_new_tokens</span>
</span></span><span style=display:flex><span>        <span style=color:#000>config</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>repetition_penalty</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#0000cf;font-weight:700>1.1</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>if</span> <span style=color:#000>temperature</span> <span style=color:#ce5c00;font-weight:700>&gt;</span> <span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>            <span style=color:#000>config</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>do_sample</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#204a87;font-weight:700>True</span>
</span></span><span style=display:flex><span>            <span style=color:#000>config</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>temperature</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>temperature</span>
</span></span><span style=display:flex><span>            <span style=color:#000>config</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>top_p</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>top_p</span>
</span></span><span style=display:flex><span>            <span style=color:#000>config</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>top_k</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>top_k</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>else</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>            <span style=color:#000>config</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>do_sample</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#204a87;font-weight:700>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#000>token_queue</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>queue</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>Queue</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>        <span style=color:#000>response_parts</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000;font-weight:700>[]</span>
</span></span><span style=display:flex><span>        <span style=color:#000>error_holder</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000;font-weight:700>[]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>streamer_callback</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>token</span><span style=color:#000;font-weight:700>):</span>
</span></span><span style=display:flex><span>            <span style=color:#000>response_parts</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>append</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>token</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>            <span style=color:#000>token_queue</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>put</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>token</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>            <span style=color:#204a87;font-weight:700>return</span> <span style=color:#204a87;font-weight:700>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>run</span><span style=color:#000;font-weight:700>():</span>
</span></span><span style=display:flex><span>            <span style=color:#204a87;font-weight:700>try</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>                <span style=color:#000>gen_kwargs</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000;font-weight:700>{</span><span style=color:#4e9a06>&#34;generation_config&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>config</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>                              <span style=color:#4e9a06>&#34;streamer&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>streamer_callback</span><span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>                <span style=color:#204a87;font-weight:700>if</span> <span style=color:#000>image</span> <span style=color:#204a87;font-weight:700>is</span> <span style=color:#204a87;font-weight:700>not</span> <span style=color:#204a87;font-weight:700>None</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>                    <span style=color:#000>gen_kwargs</span><span style=color:#000;font-weight:700>[</span><span style=color:#4e9a06>&#34;images&#34;</span><span style=color:#000;font-weight:700>]</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000;font-weight:700>[</span><span style=color:#000>_pil_to_ov_tensor</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>image</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>max_dim</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>max_dim</span><span style=color:#000;font-weight:700>)]</span>
</span></span><span style=display:flex><span>                <span style=color:#000>result</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#3465a4>self</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>pipe</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>generate</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>history</span><span style=color:#000;font-weight:700>,</span> <span style=color:#ce5c00;font-weight:700>**</span><span style=color:#000>gen_kwargs</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>                <span style=color:#3465a4>self</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>metrics</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>result</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>perf_metrics</span>
</span></span><span style=display:flex><span>            <span style=color:#204a87;font-weight:700>except</span> <span style=color:#c00;font-weight:700>Exception</span> <span style=color:#204a87;font-weight:700>as</span> <span style=color:#000>e</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>                <span style=color:#000>error_holder</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>append</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>e</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>            <span style=color:#204a87;font-weight:700>finally</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>                <span style=color:#000>token_queue</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>put</span><span style=color:#000;font-weight:700>(</span><span style=color:#204a87;font-weight:700>None</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#000>threading</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>Thread</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>target</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>run</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>daemon</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>True</span><span style=color:#000;font-weight:700>)</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>start</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>while</span> <span style=color:#204a87;font-weight:700>True</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>            <span style=color:#000>token</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>token_queue</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>get</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>            <span style=color:#204a87;font-weight:700>if</span> <span style=color:#000>token</span> <span style=color:#204a87;font-weight:700>is</span> <span style=color:#204a87;font-weight:700>None</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>                <span style=color:#204a87;font-weight:700>break</span>
</span></span><span style=display:flex><span>            <span style=color:#204a87;font-weight:700>yield</span> <span style=color:#000>token</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#000>history</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>append</span><span style=color:#000;font-weight:700>({</span><span style=color:#4e9a06>&#34;role&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;assistant&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>                        <span style=color:#4e9a06>&#34;content&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;&#34;</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>join</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>response_parts</span><span style=color:#000;font-weight:700>)})</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>if</span> <span style=color:#000>error_holder</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>            <span style=color:#204a87;font-weight:700>raise</span> <span style=color:#000>error_holder</span><span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>]</span>
</span></span></code></pre></div><p>Here&rsquo;s what&rsquo;s going on:</p><ul><li>VLMPipeline calls <code>streamer_callback</code> function with each generated token. The callback puts the token into the queue and returns <code>False</code> to continue generation.</li><li>The <code>finally</code> block pushes <code>None</code> into the queue after generation completes or fails. The main thread breaks on <code>None</code>.</li></ul><h2 id=implementing-the-gradio-ui>Implementing the Gradio UI</h2><p>In this section we&rsquo;ll implement Gradio UI:</p><ol><li>Embedded within a Jupyter notebook and</li><li>As a stand-alone app</li></ol><h3 id=gradio-ui-embedded-in-jupyter-notebook>Gradio UI embedded in Jupyter notebook</h3><p>The notebook <code>notebooks/gradio_demo.ipynb</code> reuses the same <code>model_engine.py</code> module we built above — no code duplication. It&rsquo;s a three-cell workflow:</p><ol><li><strong>Setup</strong> — add the project root to <code>sys.path</code> and set the model directory.</li><li><strong>Load</strong> — import <code>load_model</code>, <code>new_history</code>, and <code>OpenVINOStreamer</code> from <code>app.model_engine</code>, then instantiate the streamer.</li><li><strong>Launch</strong> — build a <code>gr.ChatInterface</code> and call <code>demo.launch(inline=True, share=False)</code> to embed the chat UI right inside the notebook output cell.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000>chat_history</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>new_history</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>respond</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>message</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>_history</span><span style=color:#000;font-weight:700>):</span>
</span></span><span style=display:flex><span>    <span style=color:#000>user_text</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>message</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>get</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;text&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#4e9a06>&#34;&#34;</span><span style=color:#000;font-weight:700>)</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>strip</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>    <span style=color:#000>files</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>message</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>get</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;files&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000;font-weight:700>[])</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>if</span> <span style=color:#204a87;font-weight:700>not</span> <span style=color:#000>user_text</span> <span style=color:#204a87;font-weight:700>and</span> <span style=color:#204a87;font-weight:700>not</span> <span style=color:#000>files</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>yield</span> <span style=color:#4e9a06>&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#000>image</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#204a87;font-weight:700>None</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>if</span> <span style=color:#000>files</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>        <span style=color:#000>path</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>files</span><span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>]</span> <span style=color:#204a87;font-weight:700>if</span> <span style=color:#204a87>isinstance</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>files</span><span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>],</span> <span style=color:#204a87>str</span><span style=color:#000;font-weight:700>)</span> <span style=color:#204a87;font-weight:700>else</span> <span style=color:#000>files</span><span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>]</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>get</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;path&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#4e9a06>&#34;&#34;</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>if</span> <span style=color:#000>path</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>            <span style=color:#000>image</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>Image</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>open</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>path</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#000>prompt</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>user_text</span> <span style=color:#204a87;font-weight:700>or</span> <span style=color:#4e9a06>&#34;Describe this image.&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#000>response</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#4e9a06>&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>for</span> <span style=color:#000>token</span> <span style=color:#204a87;font-weight:700>in</span> <span style=color:#000>streamer</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>stream</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>chat_history</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>prompt</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>image</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>image</span><span style=color:#000;font-weight:700>):</span>
</span></span><span style=display:flex><span>        <span style=color:#000>response</span> <span style=color:#ce5c00;font-weight:700>+=</span> <span style=color:#000>token</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>yield</span> <span style=color:#000>response</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000>demo</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>gr</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>ChatInterface</span><span style=color:#000;font-weight:700>(</span>
</span></span><span style=display:flex><span>    <span style=color:#000>fn</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>respond</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#000>multimodal</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>True</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#000>title</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;Qwen2-VL Chat (OpenVINO)&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#000>textbox</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>gr</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>MultimodalTextbox</span><span style=color:#000;font-weight:700>(</span>
</span></span><span style=display:flex><span>        <span style=color:#000>file_types</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000;font-weight:700>[</span><span style=color:#4e9a06>&#34;image&#34;</span><span style=color:#000;font-weight:700>],</span> <span style=color:#000>file_count</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;single&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#000>placeholder</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;Type a message or upload an image...&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>),</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#000>demo</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>launch</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>inline</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>True</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>share</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>False</span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><h3 id=gradio-web-ui-with-a-stand-alone-app>Gradio web UI with a stand-alone app</h3><p>The UI in <code>app/gradio_app.py</code> uses Gradio&rsquo;s <code>ChatInterface</code> for layout. We define a <code>respond()</code> function which is a generator that yields the concatenated response string on each token. Gradio streams this to the chat bubble in real time:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>respond</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>message</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>_history</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>temperature</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>max_tokens</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>top_p</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>img_quality</span><span style=color:#000;font-weight:700>):</span>
</span></span><span style=display:flex><span>    <span style=color:#000>user_text</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>message</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>get</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;text&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#4e9a06>&#34;&#34;</span><span style=color:#000;font-weight:700>)</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>strip</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>    <span style=color:#000>files</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>message</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>get</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;files&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000;font-weight:700>[])</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>if</span> <span style=color:#204a87;font-weight:700>not</span> <span style=color:#000>user_text</span> <span style=color:#204a87;font-weight:700>and</span> <span style=color:#204a87;font-weight:700>not</span> <span style=color:#000>files</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>yield</span> <span style=color:#4e9a06>&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#000>image</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#204a87;font-weight:700>None</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>if</span> <span style=color:#000>files</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>        <span style=color:#000>path</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>files</span><span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>]</span> <span style=color:#204a87;font-weight:700>if</span> <span style=color:#204a87>isinstance</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>files</span><span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>],</span> <span style=color:#204a87>str</span><span style=color:#000;font-weight:700>)</span> <span style=color:#204a87;font-weight:700>else</span> <span style=color:#000>files</span><span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>]</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>get</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;path&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#4e9a06>&#34;&#34;</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>if</span> <span style=color:#000>path</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>            <span style=color:#000>image</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>Image</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>open</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>path</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#000>prompt</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>user_text</span> <span style=color:#204a87;font-weight:700>or</span> <span style=color:#4e9a06>&#34;Describe this image.&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#000>max_dim</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>IMAGE_QUALITY_MAP</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>get</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>img_quality</span><span style=color:#000;font-weight:700>,</span> <span style=color:#0000cf;font-weight:700>448</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#000>response</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#4e9a06>&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>for</span> <span style=color:#000>token</span> <span style=color:#204a87;font-weight:700>in</span> <span style=color:#000>streamer</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>stream</span><span style=color:#000;font-weight:700>(</span>
</span></span><span style=display:flex><span>        <span style=color:#000>chat_history</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>prompt</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>image</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>image</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#000>max_new_tokens</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87>int</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>max_tokens</span><span style=color:#000;font-weight:700>),</span> <span style=color:#000>temperature</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>temperature</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#000>top_p</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>top_p</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>max_dim</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>max_dim</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>):</span>
</span></span><span style=display:flex><span>        <span style=color:#000>response</span> <span style=color:#ce5c00;font-weight:700>+=</span> <span style=color:#000>token</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>yield</span> <span style=color:#000>response</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>if</span> <span style=color:#000>streamer</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>metrics</span><span style=color:#000;font-weight:700>:</span>
</span></span><span style=display:flex><span>        <span style=color:#000>metrics</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>streamer</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>metrics</span>
</span></span><span style=display:flex><span>        <span style=color:#000>ttft</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>metrics</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>get_ttft</span><span style=color:#000;font-weight:700>()</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>mean</span>
</span></span><span style=display:flex><span>        <span style=color:#000>tok_s</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>metrics</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>get_throughput</span><span style=color:#000;font-weight:700>()</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>mean</span>
</span></span><span style=display:flex><span>        <span style=color:#000>n_tok</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>metrics</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>get_num_generated_tokens</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>        <span style=color:#000>stats</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#4e9a06>f</span><span style=color:#4e9a06>&#34;</span><span style=color:#4e9a06>\n\n</span><span style=color:#4e9a06>*TTFT: </span><span style=color:#4e9a06>{</span><span style=color:#000>ttft</span><span style=color:#4e9a06>:</span><span style=color:#4e9a06>.0f</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>ms · </span><span style=color:#4e9a06>{</span><span style=color:#000>tok_s</span><span style=color:#4e9a06>:</span><span style=color:#4e9a06>.1f</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06> tok/s · </span><span style=color:#4e9a06>{</span><span style=color:#000>n_tok</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06> tokens*&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>yield</span> <span style=color:#000>response</span> <span style=color:#ce5c00;font-weight:700>+</span> <span style=color:#000>stats</span>
</span></span></code></pre></div><p>After each response, we display performance metrics — time to first token, throughput, and total tokens — so you can see how the model performs on your hardware. Clearing the chat resets the <code>ChatHistory</code> via a <code>clear_chat()</code> callback wired to Gradio&rsquo;s clear button.</p><h4 id=chatinterface-setup><code>ChatInterface</code> setup</h4><div class="td-card card me-4 border-0" style=width:auto><div class=card-body><div class=card-text><a href=images/gradio.gif target=_blank><img src=images/gradio.gif style=width:90%;border-radius:8px;overflow:hidden;display:block></a></div><div class=card-body-extra></div></div></div><br><p>For the chat interface we use Gradio&rsquo;s UI out of the box, we also introduced a generation settings picker to control the generation quality.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000>demo</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>gr</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>ChatInterface</span><span style=color:#000;font-weight:700>(</span>
</span></span><span style=display:flex><span>    <span style=color:#000>fn</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>respond</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#000>multimodal</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>True</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#000>title</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;Qwen2-VL Chat (OpenVINO)&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#000>description</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;Multimodal chatbot powered by Qwen2-VL-2B-Instruct with OpenVINO.&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#000>chatbot</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>gr</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>Chatbot</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>height</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>700</span><span style=color:#000;font-weight:700>),</span>
</span></span><span style=display:flex><span>    <span style=color:#000>textbox</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>gr</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>MultimodalTextbox</span><span style=color:#000;font-weight:700>(</span>
</span></span><span style=display:flex><span>        <span style=color:#000>file_types</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000;font-weight:700>[</span><span style=color:#4e9a06>&#34;image&#34;</span><span style=color:#000;font-weight:700>],</span> <span style=color:#000>file_count</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;single&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#000>placeholder</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;Type a message or upload an image...&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>),</span>
</span></span><span style=display:flex><span>    <span style=color:#000>additional_inputs</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000;font-weight:700>[</span>
</span></span><span style=display:flex><span>        <span style=color:#000>gr</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>Slider</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>0.0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#0000cf;font-weight:700>2.0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>value</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>0.0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>step</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>0.1</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>label</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;Temperature&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>                  <span style=color:#000>info</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;0 = greedy / deterministic&#34;</span><span style=color:#000;font-weight:700>),</span>
</span></span><span style=display:flex><span>        <span style=color:#000>gr</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>Slider</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>64</span><span style=color:#000;font-weight:700>,</span> <span style=color:#0000cf;font-weight:700>2048</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>value</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>512</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>step</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>64</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>label</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;Max Tokens&#34;</span><span style=color:#000;font-weight:700>),</span>
</span></span><span style=display:flex><span>        <span style=color:#000>gr</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>Slider</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>0.0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#0000cf;font-weight:700>1.0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>value</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>1.0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>step</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>0.05</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>label</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;Top-P&#34;</span><span style=color:#000;font-weight:700>),</span>
</span></span><span style=display:flex><span>        <span style=color:#000>gr</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>Radio</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>choices</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87>list</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>IMAGE_QUALITY_MAP</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>keys</span><span style=color:#000;font-weight:700>()),</span>
</span></span><span style=display:flex><span>                 <span style=color:#000>value</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;Medium (448px)&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>label</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;Image Quality&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>                 <span style=color:#000>info</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;Max resolution sent to model&#34;</span><span style=color:#000;font-weight:700>),</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>],</span>
</span></span><span style=display:flex><span>    <span style=color:#000>additional_inputs_accordion</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>gr</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>Accordion</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;Generation Settings&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#204a87>open</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>False</span><span style=color:#000;font-weight:700>),</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><div class="td-card card me-4" style=width:auto><div class=card-body><div class=card-text><a href=images/gradio_settings.png target=_blank><img src=images/gradio_settings.png style=width:100%;border-radius:8px;overflow:hidden;display:block></a></div><div class=card-body-extra></div></div></div><br><h3 id=hosting-the-gradio-server>Hosting the Gradio server</h3><p>There are two ways to run the Gradio UI:</p><h4 id=local><strong>Local</strong></h4><p>To run the app module directly use <code>uv</code> or <code>python</code></p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>uv run llm-chat
</span></span></code></pre></div><p>or</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>python -m app.gradio_app
</span></span></code></pre></div><p>output</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>Loading model from &lt;PATH&gt;/llm-chat-app/models/qwen2-vl-2b-instruct-int4...
</span></span><span style=display:flex><span>Model loaded.
</span></span><span style=display:flex><span>* Running on <span style=color:#204a87>local</span> URL:  http://0.0.0.0:7860
</span></span><span style=display:flex><span>* To create a public link, <span style=color:#204a87>set</span> <span style=color:#4e9a06>`</span><span style=color:#000>share</span><span style=color:#ce5c00;font-weight:700>=</span>True<span style=color:#4e9a06>`</span> in <span style=color:#4e9a06>`</span>launch<span style=color:#ce5c00;font-weight:700>()</span><span style=color:#4e9a06>`</span>.
</span></span></code></pre></div><h4 id=docker><strong>Docker</strong></h4><p>The <code>Dockerfile</code> uses <code>python:3.12-slim</code> as its base image, installs <a href=https://github.com/astral-sh/uv><code>uv</code></a> for fast dependency resolution, copies <code>pyproject.toml</code> and <code>uv.lock</code> to install the pinned environment, then copies in the <code>app/</code> and <code>notebooks/</code> directories. The <code>app</code> service in <code>docker-compose.yml</code> builds this image and starts the Gradio server automatically:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>app</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>build</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>.</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>network_mode</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>host</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>volumes</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>./models:/app/models:ro</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>environment</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>MODE=gradio</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>depends_on</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>model-server</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>restart</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>unless-stopped</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>It uses <code>network_mode: host</code> so the container shares the host&rsquo;s network stack, making Gradio available at <code>http://localhost:7860</code>. The model directory is mounted read-only. In the default Gradio mode, the entrypoint also starts Jupyter Lab in the background on port 8888, so you get notebook access alongside the chat UI.</p><h4 id=entrypointsh>entrypoint.sh</h4><p>The Docker image launches both servers, Gradio and Jupyter simultaneously. Jupyter runs in the background and Gradio runs in the foreground.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#204a87>set</span> -e
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>if</span> <span style=color:#ce5c00;font-weight:700>[</span> <span style=color:#4e9a06>&#34;</span><span style=color:#000>$MODE</span><span style=color:#4e9a06>&#34;</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#4e9a06>&#34;jupyter&#34;</span> <span style=color:#ce5c00;font-weight:700>]</span><span style=color:#000;font-weight:700>;</span> <span style=color:#204a87;font-weight:700>then</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87>exec</span> jupyter lab <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>        --ip<span style=color:#ce5c00;font-weight:700>=</span>0.0.0.0 --port<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>8888</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>        --no-browser --allow-root <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>        --NotebookApp.token<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#39;&#39;</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>        --notebook-dir<span style=color:#ce5c00;font-weight:700>=</span>/app/notebooks
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>else</span>
</span></span><span style=display:flex><span>    <span style=color:#8f5902;font-style:italic># Start Jupyter in the background for notebook access</span>
</span></span><span style=display:flex><span>    jupyter lab <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>        --ip<span style=color:#ce5c00;font-weight:700>=</span>0.0.0.0 --port<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>8888</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>        --no-browser --allow-root <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>        --NotebookApp.token<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#39;&#39;</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>        --notebook-dir<span style=color:#ce5c00;font-weight:700>=</span>/app/notebooks <span style=color:#000;font-weight:700>&amp;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#8f5902;font-style:italic># Start Gradio in the foreground</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87>exec</span> python -m app.gradio_app <span style=color:#4e9a06>&#34;</span><span style=color:#000>$@</span><span style=color:#4e9a06>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>fi</span>
</span></span></code></pre></div><h2 id=deploying-openvino-model-server-ovms-using-docker>Deploying OpenVINO model server (OVMS) using Docker</h2><p>With our local setup working, next we&rsquo;ll explore hosting our models as a service using OVMS. OVMS provides an OpenAI-compatible REST API out of the box — <code>POST /v3/chat/completions</code> which means you can use it with a host of clients, i.e. any client that speaks the OpenAI protocol can connect: Open WebUI, the iOS app, <code>curl</code>.</p><h3 id=exporting-ovms-model>Exporting OVMS model</h3><p>OVMS is capable of pulling OpenVINO models and hosting them easily. But this assumes that the model you need is available in HuggingFace or other repository. In our case, the official OpenVINO repository doesn&rsquo;t include our model.</p><p>There is a handy script in the OpenVINO repository that lets you start with the OpenVINO-optimized model and organize it in a specific model directory layout OVMS expects. Our script <code>export_ovms_model.sh</code> does that automatically.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Clone the repository to access the helper script export_model.py</span>
</span></span><span style=display:flex><span>git clone https://github.com/openvinotoolkit/model_server
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create ovms model directory</span>
</span></span><span style=display:flex><span>mkdir -p models-ovms
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Run helper script with a temporary isolated python env</span>
</span></span><span style=display:flex><span>uv run --isolated --no-project --python 3.12 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --pre <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --extra-index-url https://download.pytorch.org/whl/cpu <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --with-requirements model_server/demos/common/export_models/requirements.txt <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    model_server/demos/common/export_models/export_model.py text_generation <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --source_model Qwen/Qwen2-VL-2B-Instruct <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --weight-format int4 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --config_file_path models-ovms/config_all.json <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>    --model_repository_path models-ovms
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Change ownership of the model directory to container user UID 5000</span>
</span></span><span style=display:flex><span>sudo chown -R 5000:5000 models-ovms
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Delete the repository we cloned for the helper script export_model.py</span>
</span></span><span style=display:flex><span>rm -rf model_server
</span></span></code></pre></div><h3 id=docker-compose-configuration>Docker compose configuration</h3><p>With the model ready, we can use this Docker Compose specification:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>services</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>model-server</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>image</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>openvino/model_server:weekly</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>ports</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#4e9a06>&#34;8000:8000&#34;</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>volumes</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>./models-ovms:/models:rw</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>command</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000;font-weight:700>&gt;</span><span style=color:#8f5902;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>      --rest_port 8000
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>      --config_path /models/config_all.json</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>restart</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>unless-stopped</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>To start using the service run</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>docker compose up
</span></span></code></pre></div><h3 id=testing-the-api-with-curl>Testing the API with curl</h3><p>Before building client integrations, we tested the OVMS API with <code>curl</code></p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>curl -X POST http://localhost:8000/v3/chat/completions <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>  -H <span style=color:#4e9a06>&#34;Content-Type: application/json&#34;</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span>  -d <span style=color:#4e9a06>&#39;{
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    &#34;model&#34;: &#34;Qwen/Qwen2-VL-2B-Instruct&#34;,
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is OpenVINO?&#34;}],
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    &#34;max_tokens&#34;: 200
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  }&#39;</span>
</span></span></code></pre></div><p>Output:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;choices&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>[</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;finish_reason&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;stop&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;index&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;logprobs&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#204a87;font-weight:700>null</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>      <span style=color:#204a87;font-weight:700>&#34;message&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>&#34;content&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;OpenVINO is a software framework that allows developers to build machine learning models using the TensorFlow and PyTorch APIs. It provides a common interface for different programming languages and platforms, making it easier to integrate machine learning models into existing applications.&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>&#34;role&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;assistant&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>&#34;tool_calls&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>[]</span>
</span></span><span style=display:flex><span>      <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>  <span style=color:#000;font-weight:700>],</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;created&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>1771917514</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;model&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;Qwen/Qwen2-VL-2B-Instruct&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;object&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;chat.completion&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>  <span style=color:#204a87;font-weight:700>&#34;usage&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;prompt_tokens&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>25</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;completion_tokens&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>49</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;total_tokens&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>74</span>
</span></span><span style=display:flex><span>  <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>}</span>
</span></span></code></pre></div><h2 id=open-webui-client>Open WebUI client</h2><div class="td-card card me-4 border-0" style=width:auto><div class=card-body><div class=card-text><a href=images/openwebui.gif target=_blank><img src=images/openwebui.gif style=width:90%;border-radius:8px;overflow:hidden;display:block></a></div><div class=card-body-extra></div></div></div><br><p>Open WebUI is a ChatGPT-like frontend that can front model servers hosting LLMs. Since OVMS already exposes Open AI compatible <code>/v3/chat/completions</code>, Open WebUI connects to it with zero custom code. Our <code>docker-compose.yml</code> includes OVMS and Open WebUI services together making it easy for Open WebUI to discover the available models in OVMS automatically.</p><p>Here&rsquo;s the Open WebUI section from <code>docker-compose.yml</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f8f8f8>  </span><span style=color:#204a87;font-weight:700>open-webui</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>image</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>ghcr.io/open-webui/open-webui:main</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>ports</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#4e9a06>&#34;3000:8080&#34;</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>environment</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>OPENAI_API_BASE_URL=http://model-server:8000/v3</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>OPENAI_API_KEY=not-needed</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>WEBUI_AUTH=false</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>FILE_IMAGE_COMPRESSION_WIDTH=336</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>FILE_IMAGE_COMPRESSION_HEIGHT=336</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>ENABLE_RAG=False</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>HF_HUB_OFFLINE=1</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>depends_on</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>      </span>- <span style=color:#000>model-server</span><span style=color:#f8f8f8>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8>    </span><span style=color:#204a87;font-weight:700>restart</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8> </span><span style=color:#000>unless-stopped</span><span style=color:#f8f8f8>
</span></span></span></code></pre></div><p>A few things to note:</p><ul><li><code>OPENAI_API_BASE_URL=http://model-server:8000/v3</code> — this uses Docker&rsquo;s internal service discovery to route requests to OVMS.</li><li><code>HF_HUB_OFFLINE=1</code> — prevents Open WebUI from making any internet calls at startup.</li><li><code>FILE_IMAGE_COMPRESSION_WIDTH/HEIGHT=336</code> — we ask Open WebUI to preprocess images, and it compresses uploaded images to 336px on the client side before sending them to the API, which keeps prefill times fast.</li></ul><p>After running <code>docker compose up</code>, Open WebUI is accessible at <code>http://localhost:3000</code>. Our model will already be selected as the default and is ready to run!</p><h2 id=building-an-ios-vlm-chatbot-client-with-ovms-backend>Building an iOS VLM chatbot client with OVMS backend</h2><div class="td-card card border-0" style=width:auto;padding:8px><div style=display:flex;flex-direction:row;justify-content:center;align-items:center;gap:20px><div class="td-card card me-4 border-0" style=width:auto><div class=card-body><div class=card-text><a href=images/ios_full.gif target=_blank><img src=images/ios_full.gif style=width:150%;border-radius:8px;overflow:hidden;display:block></a></div><div class=card-body-extra></div></div></div><br></div></div><p>This is the benefit of hosting the model as an OVMS micro service. Because OVMS exposes an OpenAI-compatible API, the iOS client can use the <a href=https://github.com/MacPaw/OpenAI>OpenAI Swift SDK</a> to send requests to OVMS hosted models.</p><h3 id=client-configuration>Client configuration</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-swift data-lang=swift><span style=display:flex><span><span style=color:#204a87;font-weight:700>private</span> <span style=color:#204a87;font-weight:700>let</span> <span style=color:#000>client</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#000>OpenAI</span><span style=color:#000;font-weight:700>(</span>
</span></span><span style=display:flex><span>    <span style=color:#000>configuration</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>.</span><span style=color:#204a87;font-weight:700>init</span><span style=color:#000;font-weight:700>(</span>
</span></span><span style=display:flex><span>        <span style=color:#000>token</span><span style=color:#000;font-weight:700>:</span> <span style=color:#204a87;font-weight:700>nil</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#000>host</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;localhost&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#000>port</span><span style=color:#000;font-weight:700>:</span> <span style=color:#0000cf;font-weight:700>8000</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#000>scheme</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;http&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#000>basePath</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;/v3&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#000>parsingOptions</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>.</span><span style=color:#000>relaxed</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><h3 id=client-side-image-preprocessing>Client-side image preprocessing</h3><p>We perform client-side image processing here as well. The iOS app has its own <code>ImageQualityOption</code> enum with the same small/medium/large tiers (336, 448, 672 px) as the Python <code>IMAGE_QUALITY_MAP</code>, so both clients use the same preprocessing strategy. Here&rsquo;s the Swift translation of the resize logic:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-swift data-lang=swift><span style=display:flex><span><span style=color:#204a87;font-weight:700>extension</span> <span style=color:#000>UIImage</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>func</span> <span style=color:#000>preprocessedJPEG</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>maxDimension</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>CGFloat</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>compressionQuality</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>CGFloat</span><span style=color:#000;font-weight:700>)</span> <span style=color:#000;font-weight:700>-&gt;</span> <span style=color:#000>Data</span><span style=color:#000;font-weight:700>?</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>let</span> <span style=color:#000>maxSide</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#3465a4>max</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>size</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>width</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>size</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>height</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>guard</span> <span style=color:#000>maxSide</span> <span style=color:#ce5c00;font-weight:700>&gt;</span> <span style=color:#0000cf;font-weight:700>0</span> <span style=color:#204a87;font-weight:700>else</span> <span style=color:#000;font-weight:700>{</span> <span style=color:#204a87;font-weight:700>return</span> <span style=color:#204a87;font-weight:700>nil</span> <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>let</span> <span style=color:#000>scale</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#3465a4>min</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>maxDimension</span> <span style=color:#ce5c00;font-weight:700>/</span> <span style=color:#000>maxSide</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>let</span> <span style=color:#000>targetWidth</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#3465a4>max</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>28</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000;font-weight:700>(</span><span style=color:#204a87>Int</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>size</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>width</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000>scale</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>/</span> <span style=color:#0000cf;font-weight:700>28</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#0000cf;font-weight:700>28</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>let</span> <span style=color:#000>targetHeight</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#3465a4>max</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>28</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000;font-weight:700>(</span><span style=color:#204a87>Int</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>size</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>height</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000>scale</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>/</span> <span style=color:#0000cf;font-weight:700>28</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#0000cf;font-weight:700>28</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>let</span> <span style=color:#000>newSize</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#000>CGSize</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>width</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>targetWidth</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>height</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>targetHeight</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>let</span> <span style=color:#000>format</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#000>UIGraphicsImageRendererFormat</span><span style=color:#000;font-weight:700>.</span><span style=color:#204a87;font-weight:700>default</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>        <span style=color:#000>format</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>scale</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#0000cf;font-weight:700>1</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>let</span> <span style=color:#000>renderer</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#000>UIGraphicsImageRenderer</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>size</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>newSize</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>format</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>format</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>let</span> <span style=color:#000>resized</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#000>renderer</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>image</span> <span style=color:#000;font-weight:700>{</span> <span style=color:#204a87;font-weight:700>_</span> <span style=color:#204a87;font-weight:700>in</span>
</span></span><span style=display:flex><span>            <span style=color:#000>draw</span><span style=color:#000;font-weight:700>(</span><span style=color:#204a87;font-weight:700>in</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>CGRect</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>origin</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>.</span><span style=color:#000>zero</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>size</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>newSize</span><span style=color:#000;font-weight:700>))</span>
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>return</span> <span style=color:#000>resized</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>jpegData</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>compressionQuality</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>compressionQuality</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>}</span>
</span></span></code></pre></div><h3 id=streaming-responses>Streaming responses</h3><p>The OpenAI Swift SDK provides <code>AsyncThrowingStream</code> for streaming, it&rsquo;s an Async iterator and we can await each token and stream it to the view:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-swift data-lang=swift><span style=display:flex><span><span style=color:#204a87;font-weight:700>private</span> <span style=color:#204a87;font-weight:700>func</span> <span style=color:#000>fetchResponse</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>intoMessageAt</span> <span style=color:#000>index</span><span style=color:#000;font-weight:700>:</span> <span style=color:#204a87>Int</span><span style=color:#000;font-weight:700>)</span> <span style=color:#000>async</span> <span style=color:#204a87;font-weight:700>throws</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>let</span> <span style=color:#000>query</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#000>buildQuery</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>let</span> <span style=color:#000>stream</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>AsyncThrowingStream</span><span style=color:#000;font-weight:700>&lt;</span><span style=color:#000>ChatStreamResult</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>Error</span><span style=color:#000;font-weight:700>&gt;</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#000>client</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>chatsStream</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>query</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>query</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>for</span> <span style=color:#204a87;font-weight:700>try</span> <span style=color:#000>await</span> <span style=color:#000>result</span> <span style=color:#204a87;font-weight:700>in</span> <span style=color:#000>stream</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>if</span> <span style=color:#204a87;font-weight:700>let</span> <span style=color:#000>delta</span> <span style=color:#000;font-weight:700>=</span> <span style=color:#000>result</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>choices</span><span style=color:#000;font-weight:700>.</span><span style=color:#3465a4>first</span><span style=color:#000;font-weight:700>?.</span><span style=color:#000>delta</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>content</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>            <span style=color:#000>messages</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>index</span><span style=color:#000;font-weight:700>].</span><span style=color:#000>text</span> <span style=color:#ce5c00;font-weight:700>+=</span> <span style=color:#000>delta</span>
</span></span><span style=display:flex><span>        <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>}</span>
</span></span></code></pre></div><h2 id=wrapping-up>Wrapping up</h2><p>Here&rsquo;s what we built:</p><ol><li><strong>Model preparation notebook</strong> (<code>notebooks/model_preparation.ipynb</code>) — download Qwen2-VL-2B-Instruct, export to OpenVINO FP16, quantize to INT4, and testing inference.</li><li><strong>Inference engine</strong> (<code>app/model_engine.py</code>) — image preprocessing, VLMPipeline wrapper with ChatHistory API, and thread-based streaming via a callback queue.</li><li><strong>Gradio web UI</strong> (<code>app/gradio_app.py</code>) — standalone multimodal chat interface with adjustable generation settings.</li><li><strong>Jupyter notebook demo</strong> (<code>notebooks/gradio_demo.ipynb</code>) — embedded Gradio chat for testing.</li><li><strong>Docker Compose stack</strong> (<code>docker-compose.yml</code>) — OVMS model server plus the containerized Gradio/Jupyter app, one command to start everything.</li><li><strong>Open WebUI integration</strong> — zero-code ChatGPT-like frontend connected to OVMS via the OpenAI-compatible API.</li><li><strong>Native iOS client</strong> — SwiftUI app using the OpenAI Swift SDK against the OVMS backend.</li></ol><h3 id=developer-friction-log>Developer friction log</h3><p>Here are the rough edges I ran into while building this:</p><ul><li><strong>Documentation Fragmentation:</strong> I found OpenVINO&rsquo;s documentation split across two separate sites (<a href=https://docs.openvino.ai>openvino.ai</a> and <a href=https://openvinotoolkit.github.io/openvino.genai>openvinotoolkit.github.io</a>), with overlapping but inconsistent versions and guidance.</li><li><strong>Pre-Release Dependency:</strong> I discovered that <code>ChatHistory</code> support for <code>VLMPipeline</code> only exists in pre-release builds of <code>openvino-genai</code>. The stable docs didn&rsquo;t surface this gap clearly, so pinning the right nightly version took trial and error.</li><li><strong>Model Selection Constraints:</strong> I couldn&rsquo;t run just any HuggingFace VLM through <code>VLMPipeline</code>, but worked via Optimum. <code>VLMPipeline</code> only supports a specific set of architectures. I had to cross-reference the supported list to land on Qwen2-VL-2B-Instruct.</li><li><strong>OVMS Model Export:</strong> I had to clone the full <code>model_server</code> repository just to access a helper script buried in the demos directory. There&rsquo;s no standalone CLI or documented export path for custom models.</li><li><strong>Client-Side Preprocessing Trade-off:</strong> I had to duplicate the 28-pixel image resize logic across Python, Swift, and Open WebUI config. OVMS&rsquo;s DAG pipeline can handle server-side preprocessing, but I found it needlessly complicated.</li></ul></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Twitter aria-label=Twitter><a target=_blank rel=noopener href=https://twitter.com/shshnkp aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=LinkedIn aria-label=LinkedIn><a target=_blank rel=noopener href=https://www.linkedin.com/in/shashankprasanna/ aria-label=LinkedIn><i class="fab fa-linkedin"></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/shashankprasanna/ aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=YouTube aria-label=YouTube><a target=_blank rel=noopener href=https://www.youtube.com/@shashank.prasanna aria-label=YouTube><i class="fab fa-youtube"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Medium aria-label=Medium><a target=_blank rel=noopener href=https://medium.com/@shashankprasanna aria-label=Medium><i class="fab fa-medium"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2026
<span class=td-footer__authors>Shashank Prasanna</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/js/main.min.b953d3b9ad10abbdb4b7e640819c02d8a1ebc4876dc56f3f655b64485dd773f1.js integrity="sha256-uVPTua0Qq720t+ZAgZwC2KHrxIdtxW8/ZVtkSF3Xc/E=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>