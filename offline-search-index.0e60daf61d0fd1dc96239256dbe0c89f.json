[{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"Personal Blog"},{"body":"Here are few of my popular blogposts. Read these and more on my medium blog: https://medium.com/@shashankprasanna How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation Choosing the right GPU for deep learning on AWS AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators A complete guide to AI accelerators for deep learning inference ‚Äî GPUs, AWS Inferentia and Amazon Elastic Inference ","categories":"","description":"","excerpt":"Here are few of my popular blogposts. Read these and more on my medium ‚Ä¶","ref":"/popular_blog_posts/","tags":"","title":"Popular blog posts"},{"body":"Here are 5 GPU instance recommendations on AWS that should serve majority of deep learning use-cases. For a complete deep dive into choosing the right GPU for deep learning on AWS, read my blog post:\nChoosing the right GPU for deep learning on AWS Highest performing multi-GPU instance on AWS Instance: p4d.24xlarge\nWhen to use it: When you need all the performance you can get. Use it for distributed training on large models and datasets.\nWhat you get: 8 x NVIDIA A100 GPUs with 40 GB GPU memory per GPU. Based on the latest NVIDIA Ampere architecture. Includes 3rd generation NVLink for fast multi-GPU training.\nHighest performing single-GPU instance on AWS Instance: p3.2xlarge\nWhen to use it: When you want the highest performance Single GPU and you‚Äôre fine with 16 GB of GPU memory.\nWhat you get: 1 x NVIDIA V100 GPU with 16 GB of GPU memory. Based on the older NVIDIA Volta architecture. The best performing single-GPU is still the NVIDIA A100 on P4 instance, but you can only get 8 x NVIDIA A100 GPUs on P4. This GPU has a slight performance edge over NVIDIA A10G on G5 instance discussed next, but G5 is far more cost-effective and has more GPU memory.\nBest performance/cost, single-GPU instance on AWS Instance: g5.xlarge\nWhen to use it: When you want high-performance, more GPU memory at lower cost than P3 instance\nWhat you get: 1 x NVIDIA A10G GPU with 24 GB of GPU memory, based on the latest Ampere architecture. NVIDIA A10G can be seen as a lower powered cousin of the A100 on the p4d.24xlarge so it‚Äôs easy to migrate and scale when you need more compute. Consider larger sizes withg5.(2/4/8/16)xlarge for the same single-GPU with more vCPUs and higher system memory if you have more pre or post processing steps.\nBest performance/cost, multi-GPU instance on AWS Instance: p3.(8/16)xlarge\nWhen to use it: Cost-effective multi-GPU model development and training.\nWhat you get: p3.8xlarge has 4 x NVIDIA V100 GPUs and p3.16xlarge has 8 x NVIDIA V100 GPUs with 16 GB of GPU memory on each GPU, based on the older NVIDIA Volta architecture. For larger models, datasets and faster performance consider P4 instances.\nHigh-performance GPU instance at a budget on AWS Instance: g4dn.xlarge\nWhen to use it: Lower performance than other options at lower cost for model development and training. Cost effective model inference deployment.\nWhat you get: 1 x NVIDIA T4 GPU with 16 GB of GPU memory. Based on the previous generation NVIDIA Turing architecture. Consider g4dn.(2/4/8/16)xlarge for more vCPUs and higher system memory if you have more pre or post processing.\nRelated blog posts AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution Choosing the right GPU for deep learning on AWS How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators ","categories":"","description":"In a hurry? Here are the best GPUs for Deep Learining on AWS","excerpt":"In a hurry? Here are the best GPUs for Deep Learining on AWS","ref":"/best-gpus-on-aws-for-deep-learning/","tags":"","title":"Best GPUs on AWS for Deep Learning"},{"body":"\nAn AI accelerator is a dedicated processor designed to accelerate machine learning computations. Machine learning, and particularly its subset, deep learning is primarily composed of a large number of linear algebra computations, (i.e. matrix-matrix, matrix-vector operations) and these operations can be easily parallelized. AI accelerators are specialized hardware designed to accelerate these basic machine learning computations and improve performance, reduce latency and reduce cost of deploying machine learning based applications.\nWhy do we need specialized AI accelerators? The two most important reasons for building dedicated processors for machine learning are:\nEnergy efficiency Faster performance Recent trends to improve model accuracy, have been to introduce larger models with more parameters and train them on larger data sets. As model sizes get larger, and current processors won‚Äôt be able to deliver the processing power needed to train or run inference on these models under tight time-to-train and inference latency requirements.\nGeneral purpose processors like CPUs trade-off energy efficiency for versatility and special purpose processors (AI accelerators) trade off versatility for energy efficiency. AI accelerators on the other hand can be designed with features to minimize memory access, offer larger on-chip cache and include dedicated hardware features to accelerate matrix-matrix computations. Since AI accelerators are purpose built devices it is ‚Äúaware‚Äù of the algorithms that it runs on and its dedicated features will run it more efficiently than a general purpose processor.\nList of popular AI accelerators for training\nNVIDIA GPUs: Available on AWS, GCP, Azure and at your local computer store (See my recommendation list on the left menu) AWS Tranium: Available on AWS Intel Habana Gaudi: Available on AWS (v1) and Intel DevCloud (v1 and v2) Google Cloud TPUs: Available on GCP and via Colab (v1-v4) List of popular AI accelerators for inference\nNVIDIA GPUs: Available on AWS, GCP, Azure (See my recommendation list on the left menu) AWS Inferentia: Available on AWS (See my recommend blog post below) Intel Habana Gaudi: Available on AWS and Intel DevCloud (v1 and v2) Google Cloud TPUs: Available on GCP and via Colab (v1-v4) Note: Modern GPUs have dedicated silicon (TensorCores) and precision types (TF32, BF16) designed for deep learning bringing them closer to dedicated AI accelerators vs. general purpose parallel processors Recommended blog posts AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution A complete guide to AI accelerators for deep learning inference ‚Äî GPUs, AWS Inferentia and Amazon Elastic Inference ","categories":"","description":"An AI accelerator is a dedicated processor designed to accelerate machine learning computations.","excerpt":"An AI accelerator is a dedicated processor designed to accelerate ‚Ä¶","ref":"/what-is-an-ai-accelerator/","tags":"","title":"What is an AI accelerator?"},{"body":"","categories":"","description":"","excerpt":"","ref":"/quick-guides/","tags":"","title":"üìö Quick guides"},{"body":"Here is a complete list of all Amazon EC2 GPU instance types on AWS that I‚Äôve painstakenly compiled, because you can‚Äôt find this information anywhere on AWS. In the tabular format below, you‚Äôll find more detailed information about GPU type, interconnect, Thermal design power (TDP), precision types supported etc.\nFrom my blog post: Choosing the right GPU for deep learning on AWS Tabular format With more information than you were probably looking for üòä\nArchitecture NVIDIA GPU Instance type Instance name Number of GPUs GPU Memory (per GPU) GPU Interconnect (NVLink / PCIe) Thermal\nDesign Power (TDP) from nvidia-smi Tensor Cores (mixed-precision) Precision Support CPU Type Nitro based Ampere A100 P4 p4d.24xlarge 8 40 GB NVLink gen 3 (600 GB/s) 400W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 Intel Xeon Scalable (Cascade Lake) Yes Ampere A10G G5 g5.xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.2xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.4xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.8xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.16xlarge 1 24 GB NA (single GPU) 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.12xlarge 4 24 GB PCIe 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.24xlarge 4 24 GB PCIe 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Ampere A10G G5 g5.48xlarge 8 24 GB PCIe 300W Tensor Cores (Gen 3) FP64, FP32, FP16, INT8, BF16, TF32 AMD EPYC Yes Turing T4G G5 g5g.xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.2xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.4xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.8xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.16xlarge 2 16 GB PCIe 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4G G5 g5g.metal 2 16 GB PCIe 70W Tensor Cores (Gen 2) FP32, FP16, INT8 AWS Graviton2 Yes Turing T4 G4 g4dn.xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.2xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.4xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.8xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.16xlarge 1 16 GB NA (single GPU) 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.12xlarge 4 16 GB PCIe 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Turing T4 G4 g4dn.metal 8 16 GB PCIe 70W Tensor Cores (Gen 2) FP32, FP16, INT8 Intel Xeon Scalable (Cascade Lake) Yes Volta V100 P3 p3.2xlarge 1 16 GB NA (single GPU) 300W Tensor Cores (Gen 1) FP64, FP32, FP16 Intel Xeon (Broadwell) No Volta V100 P3 p3.8xlarge 4 16 GB NVLink gen 2 (300 GB/s) 300W Tensor Cores (Gen 1) FP64, FP32, FP16 Intel Xeon (Broadwell) No Volta V100 P3 p3.16xlarge 8 16 GB NVLink gen 2 (300 GB/s) 300W Tensor Cores (Gen 1) FP64, FP32, FP16 Intel Xeon (Broadwell) No Volta V100* P3 p3dn.24xlarge 8 32 GB NVLink gen 2 (300 GB/s) 300W Tensor Cores (Gen 1) FP64, FP32, FP16 Intel Xeon (Skylake) Yes Kepler K80 P2 p2.xlarge 1 12 GB NA (single GPU) 149W No FP64, FP32 Intel Xeon (Broadwell) No Kepler K80 P2 p2.8xlarge 8 12 GB PCIe 149W No FP64, FP32 Intel Xeon (Broadwell) No Kepler K80 P2 p2.16xlarge 16 12 GB PCIe 149W No FP64, FP32 Intel Xeon (Broadwell) No Maxwell M60 G3 g3s.xlarge 1 8 GB PCIe 150W No FP32 Intel Xeon (Broadwell) No Maxwell M60 G3 g3.4xlarge 1 8 GB PCIe 150W No FP32 Intel Xeon (Broadwell) No Maxwell M60 G3 g3.8xlarge 2 8 GB PCIe 150W No FP32 Intel Xeon (Broadwell) No Maxwell M60 G3 g3.16xlarge 4 8 GB PCIe 150W No FP32 Intel Xeon (Broadwell) No ","categories":"","description":"A complete list of all Amazon EC2 GPU instance types on AWS that I've painstakenly compiled, because you can't find this information anywhere in AWS docs","excerpt":"A complete list of all Amazon EC2 GPU instance types on AWS that I've ‚Ä¶","ref":"/complete-list-of-all-aws-gpu-instances/","tags":"","title":"AWS GPU instances complete list"},{"body":"Interviews with PyTorch 2.0 engineers\nVisit the PyTorch YouTube channel https://www.youtube.com/@PyTorch/ ","categories":"","description":"","excerpt":"Interviews with PyTorch 2.0 engineers\nVisit the PyTorch YouTube ‚Ä¶","ref":"/pytorch2-livestreams/","tags":"","title":"Interviews/Livestreams"},{"body":" Reach out if you‚Äôd like an instructor led hands-on workshop ‚ö°Ô∏è More coming soon! ‚ö°Ô∏è üõ†Ô∏è Workshop: A Tour of PyTorch 2.0 Learn about PyTorch 2.0 with a deep dive into the technology stack that powers the new torch.compile() API: TorchDynamo, AITAutograd, PrimTorch and TorchInductor. The new compiler stack reduces training times across a wide range of workloads while being fully backwards compatible. Bring your laptops, or connect to remote GPU powered systems to run the examples. üõ†Ô∏è Workshop: PyTorch Distributed Training on AWS Learn how to efficiently scale your training workloads to multiple instances with Amazon SageMaker. SageMaker manages your compute, storage and networking infrastructure, simply bring in your PyTorch code and learn how to distribute training across large number of CPUs and GPUs. Relevant blog posts AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution Choosing the right GPU for deep learning on AWS How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators A complete guide to AI accelerators for deep learning inference ‚Äî GPUs, AWS Inferentia and Amazon Elastic Inference ","categories":"","description":"","excerpt":" Reach out if you‚Äôd like an instructor led hands-on workshop ‚ö°Ô∏è More ‚Ä¶","ref":"/workshops/","tags":"","title":"Hosted Workshops"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/compiler/","tags":"","title":"compiler"},{"body":" Updates Latest blog post: April 20th 2023: How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation Shashank Prasanna Hi! üëãüèΩ I‚Äôm a multi-disciplinary engineer, technology communicator and doodler. I ‚ù§Ô∏è machine learning, statistics, linear algebra, numerical optimization, control theory, non-linear dynamical systems (and chaos), high-performance computing (HPC) and specialized machine learning hardware (AI Accelerators). üë®üèΩ‚Äçüíª Work I‚Äôm an AI/ML Open-Source and PyTorch Developer Advocate at Meta. I‚Äôve worked at Amazon Web Services (AWS), MathWorks (MATLAB \u0026 Simulink) and NVIDIA in various roles including software development and product marketing. I find most joy in education and I put a lot of my energy in content creation and story telling.\nüèÉüèΩNot Work: I‚Äôm a recreational runner and a ‚òïÔ∏è coffee nut.\nüéì Education I‚Äôm an Electrical Engineering by training, with a specialization in Control Theory and Non-linear Dynamics. My graduate research was advised by Dr. Leon Iasemidis.\nIn my graduate thesis Directional Information Flow and Applications I discussed the application of transfer entropy (a model-free, information theoretic measure) to the detection of epileptogenic focus in the brain (origin of epileptic seizures).\n","categories":"","description":"","excerpt":" Updates Latest blog post: April 20th 2023: How Pytorch 2.0 ‚Ä¶","ref":"/","tags":"","title":""},{"body":"Computer programming is magical. We write code in human readable languages, and as though by magic, it gets translated into electric currents through silicon transistors making them behave like switches and allowing them to implement complex logic ‚Äî just so we can enjoy cat videos on the internet. Between the programming language and hardware processors that run it, is an important piece of technology ‚Äî the compiler. A compiler‚Äôs job is to translate and simplify our human readable language code into instructions that a processor understands.\nCompilers play a very important role in deep learning to improve training and inference performance, improve energy efficiency, and target diverse AI accelerator hardware. In this blog post I‚Äôm going to discuss deep learning compiler technologies that powers PyTorch 2.0. I‚Äôll walk you through the different phases of the compilation process and discuss various underlying technologies with code examples and visualizations.\nRead the full blog post here: How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation ","categories":"","description":"A primer on deep learning compiler technologies in PyTorch for graph capture, intermediate representations, operator fusion, and more","excerpt":"A primer on deep learning compiler technologies in PyTorch for graph ‚Ä¶","ref":"/how-pytorch-2.0-accelerates-deep-learning-with-operator-fusion-and-cpu/gpu-code-generation/","tags":["pytorch","compiler","IR"],"title":"How Pytorch 2.0 accelerates deep learning with operator fusion and CPU/GPU code-generation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ir/","tags":"","title":"IR"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pytorch/","tags":"","title":"pytorch"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ai-accelerators/","tags":"","title":"ai accelerators"},{"body":"If you told me a few years ago that data scientists would be using Docker containers in their day to day work, I wouldn‚Äôt have believed you. As a member of the broader machine learning (ML) community I always considered Docker, Kubernetes, Swarm (remember that?) exotic infrastructure tools for IT/Ops experts. Today it‚Äôs a different story, rarely a day goes by when I don‚Äôt use a Docker container for training or hosting a model. An attribute of machine learning development that makes it different from traditional software development is that it relies on specialized hardware such as GPUs, Habana Gaudi, AWS Inferentia to accelerate training and inference. This makes it challenging to have containerized deployments that are hardware-agnostic, which is one of the key benefits of containers. In this blog post I‚Äôll discuss how Docker and container technologies have evolved to address this challenge. We‚Äôll discuss:\nWhy Docker has become an essential tool for machine learning today and how it addresses machine learning specific challenges How Docker accesses specialized hardware resources on heterogeneous systems that have more than one type of processor (CPU + AI accelerators). How different AI accelerators extend Docker for hardware access with examples of 1/ NVIDIA GPUs and NVIDIA Container Toolkit and 2/ AWS Inferentia and Neuron SDK support for containers How to scale Docker containers on Kubernetes with hardware accelerated nodes Read the full blog post here: How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators ","categories":"","description":"Efficient algorithms and methods in machine learning for AI accelerators ‚Äî NVIDIA GPUs, Intel Habana Gaudi and AWS Trainium and Inferentia","excerpt":"Efficient algorithms and methods in machine learning for AI ‚Ä¶","ref":"/ai-accelerators-and-machine-learning-algorithms-co-design-and-evolution/","tags":["ai accelerators","gpu"],"title":"AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/gpu/","tags":"","title":"gpu"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/aws/","tags":"","title":"aws"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/docker/","tags":"","title":"docker"},{"body":"If you told me a few years ago that data scientists would be using Docker containers in their day to day work, I wouldn‚Äôt have believed you. As a member of the broader machine learning (ML) community I always considered Docker, Kubernetes, Swarm (remember that?) exotic infrastructure tools for IT/Ops experts. Today it‚Äôs a different story, rarely a day goes by when I don‚Äôt use a Docker container for training or hosting a model. An attribute of machine learning development that makes it different from traditional software development is that it relies on specialized hardware such as GPUs, Habana Gaudi, AWS Inferentia to accelerate training and inference. This makes it challenging to have containerized deployments that are hardware-agnostic, which is one of the key benefits of containers. In this blog post I‚Äôll discuss how Docker and container technologies have evolved to address this challenge. We‚Äôll discuss:\nWhy Docker has become an essential tool for machine learning today and how it addresses machine learning specific challenges How Docker accesses specialized hardware resources on heterogeneous systems that have more than one type of processor (CPU + AI accelerators). How different AI accelerators extend Docker for hardware access with examples of 1/ NVIDIA GPUs and NVIDIA Container Toolkit and 2/ AWS Inferentia and Neuron SDK support for containers How to scale Docker containers on Kubernetes with hardware accelerated nodes Read the full blog post here:\nHow Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators ","categories":"","description":"Learn about how Docker simplifies access to NVIDIA GPUs, AWS Inferentia and scaling ML containers on Kubernetes","excerpt":"Learn about how Docker simplifies access to NVIDIA GPUs, AWS ‚Ä¶","ref":"/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators/","tags":["docker","gpu","aws"],"title":"How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators"},{"body":"Let‚Äôs start by answering the question ‚ÄúWhat is an AI accelerator?‚Äù An AI accelerator is a dedicated processor designed to accelerate machine learning computations. Machine learning, and particularly its subset, deep learning is primarily composed of a large number of linear algebra computations, (i.e. matrix-matrix, matrix-vector operations) and these operations can be easily parallelized. AI accelerators are specialized hardware designed to accelerate these basic machine learning computations and improve performance, reduce latency and reduce cost of deploying machine learning based applications.\nDo I need an AI accelerator for machine learning (ML) inference? Let‚Äôs say you have an ML model as part of your software application. The prediction step (or inference) is often the most time consuming part of your application that directly affects user experience. A model that takes several hundreds of milliseconds to generate text translations or apply filters to images or generate product recommendations, can drive users away from your ‚Äúsluggish‚Äù, ‚Äúslow‚Äù, ‚Äúfrustrating to use‚Äù app. By speeding up inference, you can reduce the overall application latency and deliver an app experience that can be described as ‚Äúsmooth‚Äù, ‚Äúsnappy‚Äù, and ‚Äúdelightful to use‚Äù. And you can speed up inference by offloading ML model prediction computation to an AI accelerator. With great market needs comes great many product alternatives, so naturally there is more than one way to accelerate your ML models in the cloud. In this blog post, I‚Äôll explore three popular options:\nGPUs: Particularly, the high-performance NVIDIA T4 and NVIDIA V100 GPUs AWS Inferentia: A custom designed machine learning inference chip by AWS Amazon Elastic Inference (EI): An accelerator that saves cost by giving you access to variable-size GPU acceleration, for models that don‚Äôt need a dedicated GPU Read the full blog post here: A complete guide to AI accelerators for deep learning inference ‚Äî GPUs, AWS Inferentia and Amazon Elastic Inference ","categories":"","description":"Learn about CPUs, GPUs, AWS Inferentia, and Amazon Elastic Inference and how to choose the right AI accelerator for inference deployment","excerpt":"Learn about CPUs, GPUs, AWS Inferentia, and Amazon Elastic Inference ‚Ä¶","ref":"/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/","tags":["ai accelerators","gpu"],"title":"A complete guide to AI accelerators for deep learning inference ‚Äî GPUs, AWS Inferentia and Amazon Elastic Inference"},{"body":"On AWS, you can launch GPU instances with different GPU memory sizes (8 GB, 16 GB, 24 GB, 32 GB, 40 GB), NVIDIA GPU generations (Ampere, Turing, Volta, Maxwell, Kepler) different capabilities (FP64, FP32, FP16, INT8, Sparsity, TensorCores, NVLink), different number of GPUs per instance (1, 2, 4, 8, 16), and paired with different CPUs (Intel, AMD, Graviton2). You can also select instances with different vCPUs (core thread count), system memory and network bandwidth and add a range of storage options (object storage, network file systems, block storage, etc.) ‚Äî in summary, you have options.\nMy goal with this blog post is to provide you with guidance on how you can choose the right GPU instance on AWS for your deep learning projects. I‚Äôll discuss key features and benefits of various EC2 GPU instances, and workloads that are best suited for each instance type and size. If you‚Äôre new to AWS, or new to GPUs, or new to deep learning, my hope is that you‚Äôll find the information you need to make the right choice for your projects.\nTopics covered in this blog post: Key recommendations for the busy data scientist/ML practitioner Why you should choose the right GPU instance not just the right GPU Deep dive on GPU instance types: P4, P3, G5 (G5g), G4, P2 and G3 Other machine learning accelerators and instances on AWS Cost optimization tips when using GPU instances for ML What software and frameworks to use on AWS? Which GPUs to consider for HPC use-cases? A complete and unapologetically detailed spreadsheet of all AWS GPU instances and their features Read the full blog post here: Choosing the right GPU for deep learning on AWS ","categories":"","description":"How to choose the right Amazon EC2 GPU instance for deep learning training and inference ‚Äî from best performance to the most cost-effective and everything in-between","excerpt":"How to choose the right Amazon EC2 GPU instance for deep learning ‚Ä¶","ref":"/choosing-the-right-gpu-for-deep-learning-on-aws/","tags":["aws","gpu"],"title":"Choosing the right GPU for deep learning on AWS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"}]