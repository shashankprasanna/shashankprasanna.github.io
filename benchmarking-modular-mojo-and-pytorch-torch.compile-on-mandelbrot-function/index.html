<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.118.2"><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Benchmarking Modular Mojo🔥 and PyTorch torch.compile() on Mandelbrot function | Shashank Prasanna</title><meta name=description content="Quick comparision of Mandelbrot function acceleration using PyTorch 2.0's torch.compile() and Modular's Mojo🔥"><meta property="og:title" content="Benchmarking Modular Mojo🔥 and PyTorch torch.compile() on Mandelbrot function"><meta property="og:description" content="Quick comparision of Mandelbrot function acceleration using PyTorch 2.0's torch.compile() and Modular's Mojo🔥"><meta property="og:type" content="article"><meta property="og:url" content="https://shashankprasanna.com/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/"><meta property="og:image" content="https://shashankprasanna.com/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/featured-background.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2023-05-08T00:00:00+00:00"><meta property="article:modified_time" content="2023-05-08T00:00:00+00:00"><meta itemprop=name content="Benchmarking Modular Mojo🔥 and PyTorch torch.compile() on Mandelbrot function"><meta itemprop=description content="Quick comparision of Mandelbrot function acceleration using PyTorch 2.0's torch.compile() and Modular's Mojo🔥"><meta itemprop=datePublished content="2023-05-08T00:00:00+00:00"><meta itemprop=dateModified content="2023-05-08T00:00:00+00:00"><meta itemprop=wordCount content="1819"><meta itemprop=image content="https://shashankprasanna.com/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/featured-background.png"><meta itemprop=keywords content="pytorch,modular,mojo,mandelbrot,"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://shashankprasanna.com/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/featured-background.png"><meta name=twitter:title content="Benchmarking Modular Mojo🔥 and PyTorch torch.compile() on Mandelbrot function"><meta name=twitter:description content="Quick comparision of Mandelbrot function acceleration using PyTorch 2.0's torch.compile() and Modular's Mojo🔥"><link rel=preload href=/scss/main.min.69537fa29b588a0489895f73e23bb15ef971d18488cb8464157b4024198e733a.css as=style><link href=/scss/main.min.69537fa29b588a0489895f73e23bb15ef971d18488cb8464157b4024198e733a.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-20Q10LSLZ7"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-20Q10LSLZ7")}</script></head><body class="td-page td-blog"><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"></span><span class=navbar-brand__name>Shashank Prasanna</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/><span class=active>Home</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://medium.com/@shashankprasanna target=_blank><i class='fa-brands fa-medium'></i><span>Medium+TDS</span><sup><i class='ps-1 fa-solid fa-up-right-from-square fa-xs' aria-hidden=true></i></sup></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/blog/><span class=active>Blog</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.8dec93ffbb0c56474b088af8fbb8214c.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><form class="td-sidebar__search d-flex align-items-center"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.8dec93ffbb0c56474b088af8fbb8214c.json data-offline-search-base-href=/ data-offline-search-max-results=10></div><button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type=button data-toggle=collapse data-target=#td-section-nav aria-controls=td-section-nav aria-expanded=false aria-label="Toggle section navigation"></button></form><nav class="collapse td-sidebar-nav" id=td-section-nav><ul class="td-sidebar-nav__section pr-md-3 ul-0"><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-blog-li><a href=/blog/ class="align-left pl-0 td-sidebar-link td-sidebar-link__section tree-root" id=m-blog><span>Blog</span></a><ul class=ul-1><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-why-i-joined-modular-ai-li><a href=/why-i-joined-modular-ai/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-why-i-joined-modular-ai><span>Why I joined Modular AI?</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child active-path" id=m-benchmarking-modular-mojo-and-pytorch-torchcompile-on-mandelbrot-function-li><a href=/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/ title="Benchmarking Modular Mojo🔥 and PyTorch torch.compile() on Mandelbrot function" class="align-left pl-0 active td-sidebar-link td-sidebar-link__page" id=m-benchmarking-modular-mojo-and-pytorch-torchcompile-on-mandelbrot-function><span class=td-sidebar-nav-active-item>Benchmarking Modular Mojo and PyTorch torch.compile() on Mandelbrot function</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-how-pytorch-20-accelerates-deep-learning-with-operator-fusion-and-cpugpu-code-generation-li><a href=/how-pytorch-2.0-accelerates-deep-learning-with-operator-fusion-and-cpu/gpu-code-generation/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-how-pytorch-20-accelerates-deep-learning-with-operator-fusion-and-cpugpu-code-generation><span>How Pytorch 2.0 accelerates deep learning with operator fusion and CPU/GPU code-generation</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-ai-accelerators-and-machine-learning-algorithms-co-design-and-evolution-li><a href=/ai-accelerators-and-machine-learning-algorithms-co-design-and-evolution/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-ai-accelerators-and-machine-learning-algorithms-co-design-and-evolution><span>AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators-li><a href=/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators><span>How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-automatically-generate-code-and-documentation-using-openai-codex-li><a href=/automatically-generate-code-and-documentation-using-openai-codex/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-automatically-generate-code-and-documentation-using-openai-codex><span>Automatically generate code and documentation using OpenAI CODEX</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference-li><a href=/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-elastic-inference><span>A complete guide to AI accelerators for deep learning inference — GPUs, AWS Inferentia and Amazon Elastic Inference</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-choosing-the-right-gpu-for-deep-learning-on-aws-li><a href=/choosing-the-right-gpu-for-deep-learning-on-aws/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-choosing-the-right-gpu-for-deep-learning-on-aws><span>Choosing the right GPU for deep learning on AWS</span></a></li></ul></li></ul></nav></div></aside><aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none"><div class="td-page-meta ml-2 pb-1 pt-2 mb-0"><div class=box><img style=vertical-align:middle src=https://shashankprasanna.com/featured-background.png alt="shashank prasanna" width=200></div><a href=https://github.com/shashankprasanna/shashankprasanna.com/tree/main/content/en/blog/Comparing%20Modular%20Mojo%20and%20PyTorch%20on%20Mandelbrot%20function/index.md class=td-page-meta--view target=_blank rel=noopener><i class="fa fa-file-alt fa-fw"></i> View page source</a>
<a href=https://github.com/shashankprasanna/shashankprasanna.com/edit/main/content/en/blog/Comparing%20Modular%20Mojo%20and%20PyTorch%20on%20Mandelbrot%20function/index.md class=td-page-meta--edit target=_blank rel=noopener><i class="fa fa-edit fa-fw"></i> Edit this page</a>
<a href="https://github.com/shashankprasanna/shashankprasanna.com/new/main/content/en/blog/Comparing%20Modular%20Mojo%20and%20PyTorch%20on%20Mandelbrot%20function/index.md?filename=change-me.md&amp;value=---%0Atitle%3A+%22Long+Page+Title%22%0AlinkTitle%3A+%22Short+Nav+Title%22%0Aweight%3A+100%0Adescription%3A+%3E-%0A+++++Page+description+for+heading+and+indexes.%0A---%0A%0A%23%23+Heading%0A%0AEdit+this+template+to+create+your+new+page.%0A%0A%2A+Give+it+a+good+name%2C+ending+in+%60.md%60+-+e.g.+%60getting-started.md%60%0A%2A+Edit+the+%22front+matter%22+section+at+the+top+of+the+page+%28weight+controls+how+its+ordered+amongst+other+pages+in+the+same+directory%3B+lowest+number+first%29.%0A%2A+Add+a+good+commit+message+at+the+bottom+of+the+page+%28%3C80+characters%3B+use+the+extended+description+field+for+more+detail%29.%0A%2A+Create+a+new+branch+so+you+can+preview+your+new+file+and+request+a+review+via+Pull+Request.%0A" class=td-page-meta--child target=_blank rel=noopener><i class="fa fa-edit fa-fw"></i> Create child page</a>
<a href="https://github.com/shashankprasanna/shashankprasanna.com/issues/new?title=Benchmarking%20Modular%20Mojo%f0%9f%94%a5%20and%20PyTorch%20torch.compile%28%29%20on%20Mandelbrot%20function" class=td-page-meta--issue target=_blank rel=noopener><i class="fas fa-tasks fa-fw"></i> Create documentation issue</a>
<a href=https://github.com/shashankprasanna/shashankprasanna.com/issues/new class=td-page-meta--project-issue target=_blank rel=noopener><i class="fas fa-tasks fa-fw"></i> Create project issue</a></div><div class=td-toc><nav id=TableOfContents><ul><li><ul><li></li></ul></li><li><a href=#benchmarks-and-caveats>Benchmarks and caveats</a><ul><li></li></ul></li><li><a href=#why-is-pytorch-so-much-faster-than-pythonnumpy-on-cpu>Why is PyTorch so much faster than Python/Numpy on CPU?</a></li><li><a href=#baseline-benchmarking-pythonnumpy>Baseline: Benchmarking Python/Numpy</a></li><li><a href=#benchmarking-mojo-on-the-mojo-playground>Benchmarking Mojo on the Mojo playground</a></li><li><a href=#benchmarking-pytorch-cpu>Benchmarking PyTorch CPU</a></li><li><a href=#benchmarking-pytorch-cpu-with-torchcompile>Benchmarking PyTorch CPU with <code>torch.compile()</code></a></li><li><a href=#benchmarking-pytorch-gpu>Benchmarking PyTorch GPU</a></li><li><a href=#benchmarking-pytorch-gpu-with-torchcompile>Benchmarking PyTorch GPU with <code>torch.compile()</code></a></li><li><a href=#benchmarking-pytorch-apple-m1m2-silicon-with-mps-support>Benchmarking PyTorch Apple M1/M2 Silicon with MPS support</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></aside><main class="col-12 col-md-9 col-xl-8 ps-md-5 pe-md-4" role=main><font color=black size=5></font>
<a class=td-rss-button title=RSS href=https://shashankprasanna.com/blog/index.xml target=_blank rel=noopener><i class="fa-solid fa-rss" aria-hidden=true></i></a><div class=td-content><h1>Benchmarking Modular Mojo🔥 and PyTorch torch.compile() on Mandelbrot function</h1><div class=lead>Quick comparision of Mandelbrot function acceleration using PyTorch 2.0&rsquo;s torch.compile() and Modular&rsquo;s Mojo🔥</div><div class="td-byline mb-4">By <b>Shashank Prasanna</b> |
<time datetime=2023-05-08 class=text-muted>Monday, May 08, 2023</time></div><header class=article-meta><div class="taxonomy taxonomy-terms-article taxo-tags"><h5 class=taxonomy-title>Tags:</h5><ul class=taxonomy-terms><li><a class=taxonomy-term href=https://shashankprasanna.com/tags/pytorch/ data-taxonomy-term=pytorch><span class=taxonomy-label>pytorch</span></a></li><li><a class=taxonomy-term href=https://shashankprasanna.com/tags/modular/ data-taxonomy-term=modular><span class=taxonomy-label>modular</span></a></li><li><a class=taxonomy-term href=https://shashankprasanna.com/tags/mojo/ data-taxonomy-term=mojo><span class=taxonomy-label>mojo</span></a></li><li><a class=taxonomy-term href=https://shashankprasanna.com/tags/mandelbrot/ data-taxonomy-term=mandelbrot><span class=taxonomy-label>mandelbrot</span></a></li></ul></div></header><p>Last week, Modular - an startup co-founded by Chris Lattner (of LLVM, Swift, MLIR fame), announced a brand new high-performance language called Mojo🔥. Mojo🔥 looks and reads like Python but that&rsquo;s only on the surface, underneath the familiar Python syntax Mojo uses it&rsquo;s own JIT and AOT compilation process to accelerate Python code. Although Mojo doesn&rsquo;t fully support all of Python today, according to Mojo docs, over time Mojo is expected to become a superset of Python.<figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:410px><img class=card-img-top src=/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/featured-background_hu52d0dfd7da44399cff6626242a002e0e_3253184_400x300_fit_catmullrom_3.png width=400 height=285><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text>PyTorch's got some Mojo🔥</p></figcaption></figure>Mojo isn&rsquo;t open-source yet and can only be accessed from the Mojo playground. So I promptly applied and got access to Mojo Playground a couple of days after the accouncement on May 3rd. First thing I wanted to do was to compare Mojo&rsquo;s performance to PyTorch. While PyTorch is a popular framework for deep learning, its also a capable replacement for numpy as a high-performance scientific computing library.</p><p>And my favorite feature of PyTorch is the new <code>torch.compile()</code> API introduced in PyTorch 2.0 that can accelerate arbitrary functions (with limitations) written using the PyTorch API. It takes PyTorch highlevel API, optimizes it and generates C++ or GPU code to improve it&rsquo;s performance. I&rsquo;ve discussed <code>torch.compile()</code> in great detail in my blog post, and I highly recommend reading it if you want to learn about how PyTorch compiler does operator fusion and CPU/GPU code-generation:</p><div class="td-card card mb-4"><div class=card-body><h5 class=card-title><i class='fa-brands fa-medium'></i> <a href=https://medium.com/towards-data-science/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26>How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation</a></h5><p class=card-text></p></div></div><p>In this blog post I want to discuss relative performance between Mojo🔥 and PyTorch, and I picked the Mandelbrot example that Jeremy Howard (or FastAI fame) demoed during the Modular keynote. I reimplemnted Mojo&rsquo;s Mandelbrot example in PyTorch to compared it&rsquo;s performance with Mojo. Before I get into the code, here are the results.</p><p><strong>Summary: Mojo is fast! and PyTorch is no slouch either!</strong></p><table><thead><tr><th>Language/Framework</th><th>Mandelbrot execution<br>(200 iterations)</th><th>System</th></tr></thead><tbody><tr><td>PyTorch GPU <code>torch.compile()</code></td><td>~165 μs (micro seconds)</td><td>Intel Core i7-9700K CPU @ 3.60GHz + NVIDIA Titan V</td></tr><tr><td>Mojo CPU</td><td>~2.6 ms</td><td>Mojo Playground: Intel Xeon Platinum 8375C CPU @ 2.90GHz</td></tr><tr><td>PyTorch CPU <code>torch.compile()</code></td><td>~9 ms</td><td>Intel Core i7-9700K CPU @ 3.60GHz</td></tr><tr><td>PyTorch GPU</td><td>~15 ms</td><td>Intel Core i7-9700K CPU @ 3.60GHz + NVIDIA Titan V</td></tr><tr><td>PyTorch CPU</td><td>~50 ms</td><td>Intel Core i7-9700K CPU @ 3.60GHz</td></tr><tr><td>PyTorch Apple M2 MPS</td><td>~70 ms</td><td>Macbook Pro M2 Apple Silicon + 30-Core GPU + 16-Core Neural Engine</td></tr><tr><td>Python/numpy</td><td>~152 ms</td><td>Intel Core i7-9700K CPU @ 3.60GHz</td></tr></tbody></table><h4 id=interesting-observations>Interesting observations</h4><ul><li>Mojo is the fastest CPU implementation</li><li>PyTorch GPU with <code>torch.compile()</code> generates a fused cuda kernel making it the fastest on GPU</li><li>PyTorch CPU with <code>torch.compile()</code> which generates fused C++ code is still faster than PyTorch GPU without compilation</li></ul><p>It should come as no surprise that PyTorch generated custom fused kernel for Mandelbrot function running on GPU is indeed faster than Mojo CPU, it&rsquo;s not even a fair comparision. PyTorch CPU is only slightly slower, but makes up for performance with better usability. Mojo is harder to use and I&rsquo;m positive the UX will improve over time.</p><h2 id=benchmarks-and-caveats>Benchmarks and caveats</h2><p><strong>This is not a scientific benchmark test.</strong> This is a rather crude, and hacked-together-in-a-day example that should illustrate the performance differences and coding approaches, so take it with a grain of Sodium Chloride.</p><h4 id=my-naive-testing-methodology>My naive testing methodology:</h4><ul><li>I use Jupyter&rsquo;s native <code>timeit</code> with <code>10</code> repeats to benchmark and report mean and variance.</li><li>For GPU, I call <code>torch.cuda.synchronize()</code> before measurement to ensure that the kernel is fully executed.</li><li>This code example also benchmarks tensors/arrays creation which is in the body of the function, which migh not be a feature that arises in real-world scenarios.</li><li>I don&rsquo;t measure the compilation time for PyTorch 2.0 and that does take a lot of time to generate loop-unrolled C++ code for CPU and NVPTX for GPU.</li></ul><h4 id=hardware-differences>Hardware differences</h4><p>Mojo is not open-source (yet), and the only way to run it is on the early access Mojo Playground which has a different configuration compared to my desktop running PyTorch. For the Apple M2 benchmark I also use a MacBook Pro laptop for M2 testing. Suffice to say, this is not a fair comparison. Hardware details:</p><ul><li>PyTorch on Desktop<ul><li>CPU: Intel Core i7-9700K CPU @ 3.60GHz with 8 cores</li><li>GPU: NVIDIA Titan V</li></ul></li><li>PyTorch on Mac<ul><li>Apple MacBook Pro with M2 Apple Silicon</li></ul></li><li>Mojo🔥 on Mojo Playground<ul><li>Intel Xeon Platinum 8375C CPU @ 2.90GHz with 32 cores</li></ul></li></ul><div class="td-card card mb-4"><div class=card-header><strong>Update (05/09/23):</strong> Chris Lattner pointed out that Mojo doesn&rsquo;t use all 32 cores on the Mojo Playground. That is mindblowing performance on 4 cores! 🤯</div><div class=card-body><h5 class=card-title><img src=chris_tweet.png alt></h5><p class=card-text></p></div></div><h2 id=why-is-pytorch-so-much-faster-than-pythonnumpy-on-cpu>Why is PyTorch so much faster than Python/Numpy on CPU?</h2><p>PyTorch is faster than Python/Numpy because the higher level PyTorch API calls highly optimized C++ routines implemented in the ATen library. These routines are eagerly evaluated, which means that each PyTorch API call is executed immediately which adds some function call overhead with every API call. To address this, PyTorch 2.0 introduced a compilation API called <code>torch.compile()</code> which takes eager PyTorch code, optimizes it and generates C++ code with OpenMP pragmas for parallelization on CPU or generates GPU code using OpenAI Triton. This is similar to what Numba does for Python code, but PyTorch 2.0&rsquo;s <code>torch.compile()</code> is much more powerful because it can fuse multiple PyTorch API calls into a single kernel, which reduces function call overhead and improves performance.</p><p>I&rsquo;ve discussed this in detail in my PyTorch 2.0 blog post, but here is a screenshot of what the automatically generated fused kernels for C++ for CPU and OpenAI Triton for GPU look like for Mandelbrot function</p><div class="alert alert-primary" role=alert><i class='fa-brands fa-medium'></i>
<a href=https://medium.com/towards-data-science/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26>How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation</a></div><div class="td-card-deck card-deck mb-4"><div class="td-card card mb-4"><div class=card-header>Autogenerated C++ code for Mandelbrot function with <code>torch.compile()</code></div><div class=card-body><h5 class=card-title><a href=mandelbrot_cpp_fused.png><img src=mandelbrot_cpp_fused.png alt></a></h5><p class=card-text></p></div><div class=card-footer>Click to zoom</div></div><div class="td-card card mb-4"><div class=card-header>Autogenerated OpenAI Triton code for Mandelbrot function with <code>torch.compile()</code></div><div class=card-body><h5 class=card-title><a href=mandelbrot_gpu_fused.png><img src=mandelbrot_gpu_fused.png alt></a></h5><p class=card-text></p></div><div class=card-footer>Click to zoom</div></div></div><h2 id=baseline-benchmarking-pythonnumpy>Baseline: Benchmarking Python/Numpy</h2><p>Let&rsquo;s start with a baseline. Here&rsquo;s the Python/Numpy implementation of Mandelbrot function. I&rsquo;ve also included a function to plot the Mandelbrot set using matplotlib. We&rsquo;ll later modify this function to use PyTorch.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.colors <span style=color:#66d9ef>as</span> colors
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> warnings
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mandelbrot_numpy</span>(max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>200</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Define the boundaries of the complex plane</span>
</span></span><span style=display:flex><span>    xn <span style=color:#f92672>=</span> <span style=color:#ae81ff>450</span>
</span></span><span style=display:flex><span>    yn <span style=color:#f92672>=</span> <span style=color:#ae81ff>375</span>
</span></span><span style=display:flex><span>    xmin <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>2.25</span>
</span></span><span style=display:flex><span>    xmax <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.75</span>
</span></span><span style=display:flex><span>    ymin <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1.25</span>
</span></span><span style=display:flex><span>    ymax <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.25</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create the grid of complex numbers</span>
</span></span><span style=display:flex><span>    x_values <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(xmin, xmax, xn, dtype<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>float64)
</span></span><span style=display:flex><span>    y_values <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(ymin, ymax, yn, dtype<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>float64)
</span></span><span style=display:flex><span>    rx, iy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>meshgrid(x_values, y_values, indexing<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;xy&#39;</span>) 
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> rx<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> iy<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>    mask <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros_like(x)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(max_iter):
</span></span><span style=display:flex><span>        x_prev <span style=color:#f92672>=</span> x
</span></span><span style=display:flex><span>        y_prev <span style=color:#f92672>=</span> y
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x_prev<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>-</span> y_prev<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> rx
</span></span><span style=display:flex><span>        y <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>x_prev<span style=color:#f92672>*</span>y_prev <span style=color:#f92672>+</span> iy
</span></span><span style=display:flex><span>        inside <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sqrt(x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> y<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>) <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>        mask<span style=color:#f92672>+=</span>inside
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> mask
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>make_plot_python</span>(m):
</span></span><span style=display:flex><span>    xn <span style=color:#f92672>=</span> <span style=color:#ae81ff>450</span>
</span></span><span style=display:flex><span>    yn <span style=color:#f92672>=</span> <span style=color:#ae81ff>375</span>
</span></span><span style=display:flex><span>    dpi <span style=color:#f92672>=</span> <span style=color:#ae81ff>32</span>
</span></span><span style=display:flex><span>    width <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>    height <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span> <span style=color:#f92672>*</span> yn <span style=color:#f92672>//</span> xn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    fig <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>figure(<span style=color:#ae81ff>1</span>, [width, height], dpi<span style=color:#f92672>=</span>dpi)
</span></span><span style=display:flex><span>    ax <span style=color:#f92672>=</span> fig<span style=color:#f92672>.</span>add_axes([<span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>1.0</span>], frame_on<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, aspect<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    light <span style=color:#f92672>=</span> colors<span style=color:#f92672>.</span>LightSource(<span style=color:#ae81ff>315</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> light<span style=color:#f92672>.</span>shade(m, plt<span style=color:#f92672>.</span>cm<span style=color:#f92672>.</span>hot, colors<span style=color:#f92672>.</span>PowerNorm(<span style=color:#ae81ff>0.3</span>), blend_mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;hsv&#39;</span>, vert_exag<span style=color:#f92672>=</span><span style=color:#ae81ff>1.5</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>imshow(image)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>axis(<span style=color:#e6db74>&#34;off&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><strong>Output:</strong>
With a baseline established let&rsquo;s compare the performance of PyTorch and Mojo.
<img src=mandel_numpy.png alt></p><h2 id=benchmarking-mojo-on-the-mojo-playground>Benchmarking Mojo on the Mojo playground</h2><p>This comparision is a bit unfair because Mojo🔥 playground has a 32 core Xeon CPU, but I only have a modest 4 year old 8 core desktop CPU. The clock frequency, memory bandwidth and cache sizes and number of cores are all different, but I can&rsquo;t install PyTorch on Mojo playground so this is the best I can do for comparision for now.</p><p>At the bottom of this notebook, I add these few lines of code to measure the execution time of the mandelbrot function.</p><p><img src=mojo_playground.png alt></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> Time <span style=color:#f92672>import</span> now
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>let eval_begin <span style=color:#f92672>=</span> now()
</span></span><span style=display:flex><span>let mandelbrot_set <span style=color:#f92672>=</span> compute_mandelbrot_simd()
</span></span><span style=display:flex><span>let eval_end <span style=color:#f92672>=</span> now()
</span></span><span style=display:flex><span>let execution_time <span style=color:#f92672>=</span> (eval_end <span style=color:#f92672>-</span> eval_begin)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;execution_time:&#34;</span>)
</span></span><span style=display:flex><span>print(F64(execution_time) <span style=color:#f92672>/</span> <span style=color:#ae81ff>1000000</span>)
</span></span></code></pre></div><p><strong>Output</strong>
I found it difficult to benchmark Mojo, because Python benchmarking tools don&rsquo;t work very readily. I ran the above code several times to ensure that the results are consistent.
<img src=benchmark_mojo.png alt></p><h2 id=benchmarking-pytorch-cpu>Benchmarking PyTorch CPU</h2><p>To update our mandelbrot function from numpy implementation to PyTorch implementation I made the following small changes</p><ul><li>replace <code>np</code> with <code>torch</code></li><li>add <code>device=device</code> to the tensor creation calls, this allows us to pass the appropriate CPU, GPU or Apple MPS device to PyTorch.</li></ul><p>Updated mandelbrot function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mandelbrot_pytorch</span>(device<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;cpu&#39;</span>, max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>200</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Define the boundaries of the complex plane</span>
</span></span><span style=display:flex><span>    xn <span style=color:#f92672>=</span> <span style=color:#ae81ff>450</span>
</span></span><span style=display:flex><span>    yn <span style=color:#f92672>=</span> <span style=color:#ae81ff>375</span>
</span></span><span style=display:flex><span>    xmin <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>2.25</span>
</span></span><span style=display:flex><span>    xmax <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.75</span>
</span></span><span style=display:flex><span>    ymin <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1.25</span>
</span></span><span style=display:flex><span>    ymax <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.25</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create the grid of complex numbers</span>
</span></span><span style=display:flex><span>    x_values <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>linspace(xmin, xmax, xn, device<span style=color:#f92672>=</span>device)
</span></span><span style=display:flex><span>    y_values <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>linspace(ymin, ymax, yn, device<span style=color:#f92672>=</span>device)
</span></span><span style=display:flex><span>    rx, iy <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>meshgrid(x_values, y_values, indexing<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;xy&#39;</span>) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> rx<span style=color:#f92672>.</span>clone()
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> iy<span style=color:#f92672>.</span>clone()
</span></span><span style=display:flex><span>    mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros_like(x, device<span style=color:#f92672>=</span>device)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(max_iter):
</span></span><span style=display:flex><span>        x_prev <span style=color:#f92672>=</span> x
</span></span><span style=display:flex><span>        y_prev <span style=color:#f92672>=</span> y
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x_prev<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>-</span> y_prev<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> rx
</span></span><span style=display:flex><span>        y <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>x_prev<span style=color:#f92672>*</span>y_prev <span style=color:#f92672>+</span> iy
</span></span><span style=display:flex><span>        inside <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sqrt(x<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> y<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>) <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>        mask<span style=color:#f92672>+=</span>inside
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> mask
</span></span></code></pre></div><p>Now let&rsquo;s call our <code>mandelbrot_pytorch</code> function with <code>device='cpu'</code>
<img src=mandel_pytorch_cpu.png alt></p><h2 id=benchmarking-pytorch-cpu-with-torchcompile>Benchmarking PyTorch CPU with <code>torch.compile()</code></h2><p>You can compile PyTorch functions using <code>torch.compile()</code> and TorchInductor will optimize and generate C++ code with OpenMP pragmas for parallization. This will significantly improve the performance of the function. All of this happens under the hood when you run the code below, but you can pass an additional argument <code>options={'trace.enabled':True}</code> to see the generated code. I discuss this is further detail in my PyTorch 2.0 blog post:<div class="td-card card mb-4"><div class=card-body><h5 class=card-title><i class='fa-brands fa-medium'></i> <a href=https://medium.com/towards-data-science/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26>How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation</a></h5><p class=card-text></p></div></div></p><p>Let&rsquo;s compile our mandelbrot function for CPU backend:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>device <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;cpu&#39;</span>
</span></span><span style=display:flex><span>mandelbrot_compiled <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>compile(mandelbrot_pytorch)
</span></span><span style=display:flex><span>mandelbrot_set <span style=color:#f92672>=</span> mandelbrot_compiled(device)
</span></span><span style=display:flex><span>make_plot_python(mandelbrot_set<span style=color:#f92672>.</span>numpy())
</span></span></code></pre></div><p><strong>Output</strong>
<img src=mandel_pytorch_cpu_compile.png alt></p><p>The generated C++ code looks like this</p><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:773px><img class=card-img-top src=/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/mandelbrot_cpp_fused_hu7e430cbf97eb3e8735ed525bf4b9a7bc_268273_800x400_fit_catmullrom_3.png width=763 height=400><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text></p></figcaption></figure><h2 id=benchmarking-pytorch-gpu>Benchmarking PyTorch GPU</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>device <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;cuda&#39;</span>
</span></span><span style=display:flex><span>mandelbrot_set <span style=color:#f92672>=</span> mandelbrot_pytorch(device)
</span></span><span style=display:flex><span>make_plot_python(mandelbrot_set<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy())
</span></span></code></pre></div><p><strong>Output</strong>
<img src=mandel_pytorch_gpu.png alt></p><h2 id=benchmarking-pytorch-gpu-with-torchcompile>Benchmarking PyTorch GPU with <code>torch.compile()</code></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>device <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;cuda&#39;</span>
</span></span><span style=display:flex><span>mandelbrot_compiled <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>compile(mandelbrot_pytorch)
</span></span><span style=display:flex><span>mandelbrot_set <span style=color:#f92672>=</span> mandelbrot_compiled(device)
</span></span><span style=display:flex><span>make_plot_python(mandelbrot_set<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy())
</span></span></code></pre></div><p><img src=mandel_pytorch_gpu_compile.png alt></p><p>The generated OpenAI Triton code which gets compiled to NVPTX for GPUs, looks like this</p><figure class="card rounded p-2 td-post-card mb-4 mt-4" style=max-width:728px><img class=card-img-top src=/benchmarking-modular-mojo-and-pytorch-torch.compile-on-mandelbrot-function/mandelbrot_gpu_fused_huc397d05ec5af180dad415181b17e01e2_296767_800x400_fit_catmullrom_3.png width=718 height=400><figcaption class="card-body px-0 pt-2 pb-0"><p class=card-text></p></figcaption></figure><h2 id=benchmarking-pytorch-apple-m1m2-silicon-with-mps-support>Benchmarking PyTorch Apple M1/M2 Silicon with MPS support</h2><p><img src=mandel_pytorch_apple_m2.png alt></p><h2 id=conclusion>Conclusion</h2><p>That&rsquo;s it folks! I hope you enjoyed this quick comparision of PyTorch and Mojo🔥.
Mojo is fast, but doesn&rsquo;t have the same level of usability of PyTorch, but that may just be just a matter of time and community support. PyTorch&rsquo;s one-two punch combo of eager mode with high-level Tensor API and compilation with <code>torch.compile()</code> is a powerful combination today. However, the PyTorch ecosystem is also quite fragmented with multiple code paths for different accelerators: TorchInductor (GPUs, CPUs), XLA (TPU, AWS Tranium/Inferentia), custom bindings/bridges (Intel Habana, MPS), and some accelerators like GPUs implement all paths. This leaves the end user and hardware vendors in a dilemma.</p><p>One of the main benefit of Mojo that I see is the ability to write OpenAI Triton style kernel code in the Python language with fast or faster then C++ performance. This would make supporting custom ops for inference easier. We&rsquo;re certainly living in an exciting times for AI Infra, AI accelerators and AI frameworks. Maybe we&rsquo;re at the cusp of another LLVM moment but for AI.</p><p>If you enjoyed reading this, check out my other blog posts on <a href=https://medium.com/@shashankprasanna>Medium</a> or reach out to me on social media, links are on the homepage. Cheers!</p><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><li><a href=/how-pytorch-2.0-accelerates-deep-learning-with-operator-fusion-and-cpu/gpu-code-generation/ aria-label="Previous - How Pytorch 2.0 accelerates deep learning with operator fusion and CPU/GPU code-generation" class="btn btn-primary"><span class=mr-1>←</span>Previous</a></li><li><a href=/why-i-joined-modular-ai/ aria-label="Next - Why I joined Modular AI?" class="btn btn-primary">Next<span class=ml-1>→</span></a></li></ul></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank rel=noopener href=https://twitter.com/shshnkp aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=LinkedIn aria-label=LinkedIn><a class=text-white target=_blank rel=noopener href=https://www.linkedin.com/in/shashankprasanna/ aria-label=LinkedIn><i class="fab fa-linkedin"></i></a></li></ul></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank rel=noopener href=https://github.com/shashankprasanna/ aria-label=GitHub><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=YouTube aria-label=YouTube><a class=text-white target=_blank rel=noopener href=https://www.youtube.com/@shashank.prasanna aria-label=YouTube><i class="fab fa-youtube"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Medium aria-label=Medium><a class=text-white target=_blank rel=noopener href=https://medium.com/@shashankprasanna aria-label=Medium><i class="fab fa-medium"></i></a></li></ul></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2023 Shashank Prasanna All Rights Reserved</small></div></div></div></footer></div><script src=/js/main.min.2aff983e9d0d8ecff9ed53e14b657216e2d8d2aff059bf4a5651283020995f1b.js integrity="sha256-Kv+YPp0Njs/57VPhS2VyFuLY0q/wWb9KVlEoMCCZXxs=" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script></body></html>